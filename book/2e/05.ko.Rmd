---
suppress-bibliography: true
---

```{r console_start, include=FALSE}
console_start()
```

```{console setup_history, include=FALSE}
 export CHAPTER="05"
 export HISTFILE=/history/history_${CHAPTER}
 rm -f $HISTFILE
```

```{console install_pup_arm64, include=FALSE}
curl -sL https://github.com/ericchiang/pup/releases/download/v0.4.0/pup_v0.4.0_linux_arm64.zip -o pup.zip
unzip pup.zip
sudo mv pup /usr/bin/
rm pup.zip
```

# 데이터 정제 {#chapter-5-scrubbing-data}

두 장 전, 데이터 과학을 위한 OSEMN 모델의 첫 번째 단계에서 다양한 출처에서 데이터를 *획득*하는 방법을 살펴보았습니다.
이 장은 두 번째 단계인 데이터 *정제*에 관한 것입니다.
보시다시피 데이터를 즉시 *탐색*하거나 심지어 *모델링*할 수 있는 경우는 드뭅니다.
데이터를 먼저 정리하거나 정제해야 하는 이유는 수없이 많습니다.

우선 데이터가 원하는 형식이 아닐 수 있습니다.
예를 들어 API에서 일부 JSON 데이터를 얻었지만 시각화를 만들기 위해 CSV 형식으로 만들어야 할 수 있습니다.
다른 일반적인 형식으로는 일반 텍스트, HTML, XML이 있습니다.
대부분의 명령줄 도구는 하나 또는 두 가지 형식으로만 작동하므로 데이터를 한 형식에서 다른 형식으로 변환할 수 있다는 것이 중요합니다.

데이터가 원하는 형식이 되면 누락된 값, 불일치, 이상한 문자 또는 불필요한 부분과 같은 문제가 여전히 있을 수 있습니다.
필터를 적용하고 값을 바꾸고 여러 파일을 결합하여 이러한 문제를 해결할 수 있습니다.
명령줄은 많은 특수 도구를 사용할 수 있고 대부분이 대량의 데이터를 처리할 수 있으므로 이러한 종류의 변환에 특히 적합합니다.
이 장에서는 `grep` [@grep] 및 `awk` [@awk]와 같은 고전적인 도구와 `jq` [@jq] 및 `pup` [@pup]와 같은 최신 도구에 대해 설명합니다.

때로는 여러 작업을 수행하기 위해 동일한 명령줄 도구를 사용하거나 동일한 작업을 수행하기 위해 여러 도구를 사용할 수 있습니다.
이 장은 명령줄 도구 자체를 깊이 파고드는 것보다 문제나 레시피에 초점을 맞춘 요리책과 같이 더 구조화되어 있습니다.


## 개요

<!-- #TODO: SHOULD: Review the list below once the chapter is complete -->

이 장에서는 다음을 수행하는 방법을 배웁니다.

- 데이터를 한 형식에서 다른 형식으로 변환
- CSV에 직접 SQL 쿼리 적용
- 줄 필터링
- 값 추출 및 바꾸기
- 열 분할, 병합 및 추출
- 여러 파일 결합

이 장은 다음 파일로 시작합니다.

```{console list_files}
cd /data/ch05
l
```

이러한 파일을 가져오는 지침은 [2장](#chapter-2-getting-started)에 있습니다.
다른 모든 파일은 명령줄 도구를 사용하여 다운로드하거나 생성됩니다.

실제 변환에 대해 알아보기 전에 명령줄에서 작업할 때 이러한 변환이 얼마나 보편적인지 보여 드리고자 합니다.


## 변환, 어디에나 있는 변환

[1장](#chapter-1-introduction)에서 실제로 OSEMN 모델의 단계가 선형적으로 수행되는 경우는 드물다고 언급했습니다.
이러한 맥락에서 정제는 OSEMN 모델의 두 번째 단계이지만 정제가 필요한 것은 획득한 데이터뿐만이 아니라는 점을 알아두시기 바랍니다.
이 장에서 배울 변환은 파이프라인의 모든 부분과 OSEMN 모델의 모든 단계에서 유용할 수 있습니다.
일반적으로 한 명령줄 도구가 다음 도구에서 즉시 사용할 수 있는 출력을 생성하면 파이프 연산자(`|`)를 사용하여 두 도구를 함께 연결할 수 있습니다.
그렇지 않으면 파이프라인에 중간 도구를 삽입하여 먼저 데이터에 변환을 적용해야 합니다.

이를 더 구체적으로 설명하기 위해 예를 들어 보겠습니다.
*fizzbuzz* 시퀀스의 처음 100개 항목을 얻었고([4장](#chapter-4-creating-command-line-tools) 참조) 막대 차트를 사용하여 *fizz*, *buzz*, *fizzbuzz*라는 단어가 얼마나 자주 나타나는지 시각화하고 싶다고 가정해 보겠습니다.
이 예제에서 아직 익숙하지 않은 도구를 사용하더라도 걱정하지 마십시오. 나중에 모두 자세히 설명합니다.

먼저 시퀀스를 생성하여 데이터를 얻고 *fb.seq*에 씁니다.

```{console fb_seq, callouts=2}
seq 100 |
/data/ch04/fizzbuzz.py |
tee fb.seq | trim
```
<1> 사용자 지정 도구 `fizzbuzz.py`는 [4장](#chapter-4-creating-command-line-tools)에서 가져왔습니다.

그런 다음 `grep`을 사용하여 *fizz* 또는 *buzz* 패턴과 일치하는 줄을 유지하고 `sort`와 `uniq` [@uniq]를 사용하여 각 단어가 얼마나 자주 나타나는지 계산합니다.

```{console fb_count}
grep -E "fizz|buzz" fb.seq | # <1>
sort | uniq -c | sort -nr > fb.cnt # <2>
bat -A fb.cnt
```
<1> 이 정규 표현식은 *fizzbuzz*와도 일치합니다.
<2> 이와 같이 `sort`와 `uniq`를 사용하는 것은 줄을 계산하고 내림차순으로 정렬하는 일반적인 방법입니다. 개수를 추가하는 것은 `-c` 옵션입니다.

`sort`가 두 번 사용된다는 점에 유의하십시오. 첫 번째는 `uniq`가 입력 데이터가 정렬되어 있다고 가정하기 때문이고 두 번째는 개수를 숫자로 정렬하기 위해서입니다.
어떤 면에서 이것은 미묘하지만 중간 변환입니다.

다음 단계는 `rush` [@rush]를 사용하여 개수를 시각화하는 것입니다.
그러나 `rush`는 입력 데이터가 CSV 형식일 것으로 예상하므로 먼저 덜 미묘한 변환이 필요합니다.
`awk`는 한 번의 호출로 헤더를 추가하고 두 필드를 뒤집고 쉼표를 삽입할 수 있습니다.

```{console fb_awk}
< fb.cnt awk 'BEGIN { print "value,count" } { print $2","$1 }' > fb.csv
bat fb.csv
csvlook fb.csv
```

이제 `rush`를 사용하여 막대 차트를 만들 준비가 되었습니다.
결과는 \@ref(fig:fb-image)를 참조하십시오.
(`rush`의 이 구문은 [7장](#chapter-7-exploring-data)에서 자세히 다룹니다.)

```{console fb_rush}
rush plot -x value -y count --geom col --height 2 fb.csv > fb.png
display fb.png
```
```{r fb-image, echo=FALSE, fig.cap="fizz, buzz, fizzbuzz 세기", fig.align="center", out.width="90%"}
knitr::include_graphics("images/fb.png")
```

이 예는 다소 작위적이지만 명령줄에서 작업할 때 일반적인 패턴을 보여줍니다.
데이터를 얻거나 시각화를 만들거나 모델을 훈련하는 것과 같은 핵심 도구는 파이프라인으로 연결되기 위해 종종 중간 변환이 필요합니다.
그런 의미에서 파이프라인을 작성하는 것은 퍼즐을 푸는 것과 같으며, 핵심 조각은 종종 맞추기 위해 도우미 조각이 필요합니다.

이제 데이터 정제의 중요성을 알았으므로 몇 가지 실제 변환에 대해 배울 준비가 되었습니다.


## 일반 텍스트

공식적으로 *일반 텍스트*는 사람이 읽을 수 있는 문자 시퀀스와 선택적으로 탭 및 줄 바꿈과 같은 특정 유형의 제어 문자를 나타냅니다[@plaintext].
예로는 로그, 전자책, 이메일, 소스 코드가 있습니다.
일반 텍스트는 이진 데이터에 비해 많은 이점이 있습니다[@pragmaticprogrammer]. 다음을 포함합니다.

- 모든 텍스트 편집기를 사용하여 열고 편집하고 저장할 수 있습니다.
- 자체 설명적이며 생성한 응용 프로그램과 독립적입니다.
- 처리하는 데 추가 지식이나 응용 프로그램이 필요하지 않으므로 다른 형태의 데이터보다 오래 지속됩니다.

그러나 가장 중요한 것은 Unix 철학이 일반 텍스트를 명령줄 도구 간의 보편적인 인터페이스로 간주한다는 것입니다[@raymond2003art].
즉, 대부분의 도구는 일반 텍스트를 입력으로 사용하고 일반 텍스트를 출력으로 생성합니다.

그것만으로도 일반 텍스트부터 시작하기에 충분한 이유입니다.
이 장에서 설명하는 다른 형식인 CSV, JSON, XML, HTML도 실제로 일반 텍스트입니다.
지금은 일반 텍스트에 CSV와 같은 명확한 표 구조나 JSON, XML, HTML과 같은 중첩 구조가 없다고 가정합니다.
이 장의 뒷부분에서는 이러한 형식으로 작업하기 위해 특별히 설계된 몇 가지 도구를 소개합니다.

<!-- #TODO: SHOULD: What to do with this part? -->
<!-- Although the tools in this section can also be applied to these other formats (because they're still text), -->
<!-- keep in mind that the tools treat the data as plain text, and don't interpret the tabular or nested structure. -->
<!-- Sometimes you can get away with this,  -->


### 줄 필터링

첫 번째 정제 작업은 줄 필터링입니다.
즉, 입력 데이터에서 각 줄이 유지될지 아니면 삭제될지가 평가됩니다.


#### 위치 기준

줄을 필터링하는 가장 간단한 방법은 위치를 기준으로 하는 것입니다.
이는 예를 들어 파일의 상위 10개 줄을 검사하거나 다른 명령줄 도구의 출력에서 특정 행을 추출하려는 경우에 유용할 수 있습니다.
위치를 기준으로 필터링하는 방법을 설명하기 위해 10개의 줄이 포함된 더미 파일을 만들어 보겠습니다.

```{console}
seq -f "Line %g" 10 | tee lines
```

`head` [@head], `sed` [@sed] 또는 `awk`를 사용하여 처음 3줄을 인쇄할 수 있습니다.

```{console, callouts="NR"}
< lines head -n 3
< lines sed -n '1,3p'
< lines awk 'NR <= 3'
```
<1> `awk`에서 *NR*은 지금까지 본 총 입력 레코드 수를 나타냅니다.

마찬가지로 `tail` [@tail]을 사용하여 마지막 3줄을 인쇄할 수 있습니다.

```{console}
< lines tail -n 3
```

이를 위해 `sed`와 `awk`를 사용할 수도 있지만 `tail`이 훨씬 빠릅니다.
처음 3줄을 제거하는 방법은 다음과 같습니다.

```{console}
< lines tail -n +4
< lines sed '1,3d'
< lines sed -n '1,3!p'
```

`tail`을 사용하면 줄 수에 1을 더해야 합니다.
인쇄를 시작하려는 줄이라고 생각하십시오.
마지막 3줄을 제거하는 것은 `head`로 수행할 수 있습니다.

```{console}
< lines head -n -3
```

`sed`, `awk` 또는 `head`와 `tail`의 조합을 사용하여 특정 줄을 인쇄할 수 있습니다.
여기서는 4, 5, 6번 줄을 인쇄합니다.

```{console}
< lines sed -n '4,6p'
< lines awk '(NR>=4) && (NR<=6)'
< lines head -n 6 | tail -n 3
```

시작과 단계를 지정하여 `sed`로 홀수 줄을 인쇄하거나 모듈로 연산자를 사용하여 `awk`로 홀수 줄을 인쇄합니다.

```{console}
< lines sed -n '1~2p'
< lines awk 'NR%2'
```

짝수 줄 인쇄도 비슷한 방식으로 작동합니다.

```{console}
< lines sed -n '0~2p'
< lines awk '(NR+1)%2'
```

<!-- #TODO: SHOULD: Mention somewhere in the book that Linux doesn't care about file extensions -->

```{block2, type="rmdnote"}
이러한 많은 예는 작음 기호(`<`) 다음에 파일 이름이 옵니다.
파이프라인을 왼쪽에서 오른쪽으로 읽을 수 있도록 하기 때문에 이렇게 합니다.
이것은 제 개인적인 취향이라는 것을 알아두십시오.
파일 내용을 파이프하기 위해 `cat`을 사용할 수도 있습니다.
또한 많은 명령줄 도구는 파일 이름을 인수로 허용합니다.
```


#### 패턴 기준

때로는 내용에 따라 줄을 유지하거나 삭제하고 싶을 수 있습니다.
줄 필터링을 위한 표준 명령줄 도구인 `grep`을 사용하면 특정 패턴이나 정규식과 일치하는 모든 줄을 인쇄할 수 있습니다.
예를 들어 *이상한 나라의 앨리스*에서 모든 장 제목을 추출하려면 다음과 같이 합니다.

```{console alice_grep, callouts=1}
< alice.txt grep -i chapter
```
<1> `-i` 옵션은 일치가 대소문자를 구분하지 않도록 지정합니다.

정규식을 지정할 수도 있습니다.
예를 들어 *The*로 시작하는 제목만 인쇄하려면 다음과 같이 합니다.

```{console}
< alice.txt grep -E '^CHAPTER (.*)\. The'
```

정규식을 사용하려면 `-E` 옵션을 지정해야 합니다.
그렇지 않으면 `grep`이 패턴을 리터럴 문자열로 해석하여 대부분 일치 항목이 전혀 발생하지 않습니다.

```{console}
< alice.txt grep '^CHAPTER (.*)\. The'
```

`-v` 옵션을 사용하면 일치 항목을 반전시켜 `grep`이 패턴과 일치하지 *않는* 줄을 인쇄하도록 합니다.
아래 정규 표현식은 공백만 포함하는 줄과 일치합니다.
따라서 반전을 사용하고 `wc -l`을 사용하여 비어 있지 않은 줄 수를 계산할 수 있습니다.

```{console}
< alice.txt grep -Ev '^\s$' | wc -l
```


#### 무작위성 기준

데이터 파이프라인을 구성하는 과정에 있고 데이터가 많은 경우 파이프라인 디버깅이 번거로울 수 있습니다.
이 경우 데이터에서 더 작은 샘플을 생성하는 것이 유용할 수 있습니다.
이것이 `sample` [@sample]이 유용한 이유입니다.
`sample`의 주요 목적은 줄 단위로 입력의 특정 백분율만 출력하여 데이터의 하위 집합을 얻는 것입니다.


```{console set_random_seed, include=FALSE}
RANDOM=1337
```

```{console}
seq -f "Line %g" 1000 | sample -r 1%
```

여기서 모든 입력 줄은 인쇄될 확률이 1%입니다.
이 백분율은 분수(즉, `1/100`) 또는 확률(즉, `0.01`)로 지정할 수도 있습니다.

`sample`에는 파이프라인을 디버깅할 때 유용할 수 있는 다른 두 가지 목적이 있습니다.
첫째, 출력에 약간의 지연을 추가할 수 있습니다.
이는 입력이 상수 스트림(예: [3장](#chapter-3-obtaining-data)에서 본 위키백과 스트림)이고 데이터가 너무 빨리 들어와서 무슨 일이 일어나고 있는지 볼 수 없을 때 유용합니다.
둘째, `sample`에 타이머를 설정하여 진행 중인 프로세스를 수동으로 종료할 필요가 없습니다.
예를 들어 인쇄되는 각 줄 사이에 1초 지연을 추가하고 5초 동안만 실행하려면 다음을 입력합니다.

```{console callouts="ts"}
seq -f "Line %g" 1000 | sample -r 1% -d 1000 -s 5 | ts
```
<1> 도구 `ts`[@ts]는 각 줄 앞에 타임스탬프를 추가합니다.

불필요한 계산을 방지하려면 파이프라인에서 가능한 한 빨리 `sample`을 배치하십시오.
실제로 이 주장은 `head` 및 `tail`과 같이 데이터를 줄이는 모든 명령줄 도구에 적용됩니다.
파이프라인이 작동한다고 확신하면 파이프라인에서 제거합니다.


### 값 추출

이전 예제에서 실제 장 제목을 추출하려면 `grep`의 출력을 `cut`으로 파이프하는 간단한 방법을 사용할 수 있습니다.

```{console grep_chapter_cut}
grep -i chapter alice.txt | cut -d ' ' -f 3-
```

여기서 `cut`으로 전달되는 각 줄은 공백을 기준으로 필드로 분할된 다음 세 번째 필드에서 마지막 필드까지 인쇄됩니다.
총 필드 수는 입력 줄마다 다를 수 있습니다.
`sed`를 사용하면 훨씬 더 복잡한 방식으로 동일한 작업을 수행할 수 있습니다.

```{console sed_chapter_heading}
sed -rn 's/^CHAPTER ([IVXLCDM]{1,})\. (.*)$/\2/p' alice.txt | trim 3
```

(출력이 동일하므로 세 줄로 잘립니다.) 이 방법은 정규식과 역참조를 사용합니다.
여기서 `sed`는 `grep`이 수행한 작업도 대신합니다.
더 간단한 방법이 작동하지 않을 때만 이러한 복잡한 방법을 사용하는 것이 좋습니다.
예를 들어 *chapter*가 새 장의 시작을 나타내는 데만 사용되는 것이 아니라 텍스트 자체의 일부인 경우입니다.
물론 이를 해결할 수 있는 여러 수준의 복잡성이 있지만 이것은 매우 엄격한 접근 방식을 설명하기 위한 것이었습니다.
실제로는 복잡성과 유연성 사이의 적절한 균형을 찾는 것이 중요합니다.

`cut`은 문자 위치를 기준으로 분할할 수도 있다는 점에 유의할 가치가 있습니다.
이는 입력 줄당 동일한 문자 집합을 추출(또는 제거)하려는 경우에 유용합니다.

```{console}
grep -i chapter alice.txt | cut -c 9-
```

`grep`에는 `-o` 옵션을 사용하여 모든 일치 항목을 별도의 줄에 출력하는 훌륭한 기능이 있습니다.

```{console}
< alice.txt grep -oE '\w{2,}' | trim
```

하지만 *a*로 시작하고 *e*로 끝나는 모든 단어의 데이터 세트를 만들고 싶다면 어떻게 해야 할까요?
물론 그것을 위한 파이프라인도 있습니다.

```{console, callouts=list(1)}
< alice.txt tr '[:upper:]' '[:lower:]' |
grep -oE '\w{2,}' |
grep -E '^a.*e$' |
sort | uniq | sort -nr | trim
```
<1> 여기서 `tr`을 사용하여 텍스트를 소문자로 만듭니다. 다음 섹션에서 `tr`을 자세히 살펴보겠습니다.

두 개의 `grep` 명령을 하나로 결합할 수도 있지만 이 경우 이전 파이프라인을 재사용하고 조정하는 것이 더 쉽다고 판단했습니다.
작업을 완료하기 위해 실용적인 것을 부끄러워할 필요는 없습니다!


### 값 바꾸기 및 삭제

번역을 의미하는 명령줄 도구 `tr` [@tr]을 사용하여 개별 문자를 바꾸거나 삭제할 수 있습니다.
예를 들어 공백은 다음과 같이 밑줄로 바꿀 수 있습니다.

```{console}
echo 'hello world!' | tr ' ' '_'
```

두 개 이상의 문자를 바꿔야 하는 경우 결합할 수 있습니다.

```{console}
echo 'hello world!' | tr ' !' '_?'
```

`tr`은 `-d` 인수를 지정하여 개별 문자를 삭제하는 데에도 사용할 수 있습니다.

```{console}
echo 'hello world!' | tr -d ' !'
echo 'hello world!' | tr -d -c '[a-z]'
```

이 경우 이 두 명령은 동일한 작업을 수행합니다.
그러나 두 번째 명령은 두 가지 추가 기능을 사용합니다.
대괄호와 대시(`[-]`)를 사용하여 문자 *범위*(모든 소문자)를 지정하고 `-c` 옵션은 해당 보수를 사용해야 함을 나타냅니다.
즉, 이 명령은 소문자만 유지합니다.
`tr`을 사용하여 텍스트를 대문자로 변환할 수도 있습니다.

```{console}
echo 'hello world!' | tr '[a-z]' '[A-Z]'
echo 'hello world!' | tr '[:lower:]' '[:upper:]'
```

그러나 ASCII가 아닌 문자를 번역해야 하는 경우 `tr`은 단일 바이트 문자만 작동하므로 작동하지 않을 수 있습니다. 이 경우 대신 `sed`를 사용해야 합니다.

```{console}
echo 'hello world!' | tr '[a-z]' '[A-Z]'
echo 'hallo wêreld!' | tr '[a-z]' '[A-Z]'
echo 'hallo wêreld!' | tr '[:lower:]' '[:upper:]'
echo 'hallo wêreld!' | sed 's/[[:lower:]]*/\U&/g'
echo 'helló világ' | tr '[:lower:]' '[:upper:]'
echo 'helló világ' | sed 's/[[:lower:]]*/\U&/g'
```

<!-- #TODO: SHOULD: Give a proper intro about sed -->

개별 문자 이상으로 작업해야 하는 경우 `sed`가 유용할 수 있습니다.
*alice.txt*에서 장 제목을 추출하는 `sed`의 예를 이미 보았습니다.
추출, 삭제 및 바꾸기는 실제로 `sed`에서 모두 동일한 작업입니다.
다른 정규식만 지정하면 됩니다.
예를 들어 단어를 변경하고, 반복되는 공백을 제거하고, 선행 공백을 제거하려면 다음과 같이 합니다.

```{console, callouts=2:4}
echo ' hello     world!' |
sed -re 's/hello/bye/' |
sed -re 's/\s+/ /g' |
sed -re 's/\s+//'
```
<1> *hello*를 *bye*로 바꿉니다.
<2> 모든 공백을 하나의 공백으로 바꿉니다. `g` 플래그는 전역을 의미하며, 동일한 대체가 동일한 줄에 두 번 이상 적용될 수 있음을 의미합니다.
<3> 여기서 `g` 플래그를 지정하지 않았으므로 선행 공백만 제거합니다.

다시 말하지만, 이전 `grep` 예제와 마찬가지로 이 세 가지 `sed` 명령을 하나로 결합할 수 있습니다.

```{console}
echo ' hello     world!' |
sed -re 's/hello/bye/;s/\s+/ /g;s/\s+//'
```
하지만 말해주세요. 어떤 것이 더 읽기 쉽습니까?


## CSV



<!-- #TODO: MUST: ### Canonical Format -->

<!-- The `csvkit` documentation[@csvkit] opens with: [...] CSV, the king of tabular file formats. -->

<!-- Canonical format. The format that we want to convert to. -->
<!-- Two properties. it's plain text and it has a rectangular shape, meaning in consists of rows and columns. -->

<!-- As a format, -->

<!-- As a shape, -->

<!-- Even kings have their flaws. -->
<!-- CSV definitely has its flaws. No meta data. No rules regarding quoting, the delimiter, header. -->
<!-- Writing a robust CSV parser is really hard. Nevertheless, CSV has many advantages. -->

<!-- CSV, which is the main format I'll be working with in this chapter, is actually not the easiest format to work with. Many CSV datasets are broken or incompatible with each other because there is no standard syntax, unlike XML and JSON. -->

<!-- Export from Database, spreadsheets, rectangular. -->


<!-- - It's still plain text, so can be read, edited, and even created in any text editor. -->
<!-- - CSV can be imported by many programming languages such as Python, R, and JavaScript and many software such as Excel, Tableau, and Power BI. -->
<!-- - Related to the previous point, but worth mentioning separately: a CSV directly translates to a data frame in Python (with the pandas package), R, and Julia. This means that can immediately continue scrubbing, exploring, and modeling in those languages. -->
<!-- - Speaking of modeling, most machine learning algorithms expect data to be in a rectangular format, or more precisely, a matrix of numerical values where each row is a data point and each column is a feature. More on this in Chapter 9. -->



<!-- makes assumptions, same number of fields per row. -->
<!-- flat. not nested. -->
<!-- CVS is not nested structure -->
<!-- If you build an API, use JSON -->
<!-- If you -->
<!-- You rarely have to covert CSV to anything else when you're doing data analysis. -->


### 본문과 헤더와 열, 오 마이!

`tr` 및 `grep`과 같이 일반 텍스트를 정제하는 데 사용한 명령줄 도구는 CSV에 항상 적용할 수 없습니다.
그 이유는 이러한 명령줄 도구에는 헤더, 본문 및 열에 대한 개념이 없기 때문입니다.
`grep`을 사용하여 줄을 필터링하지만 항상 헤더를 출력에 포함하려면 어떻게 해야 할까요?
또는 `tr`을 사용하여 특정 열의 값만 대문자로 만들고 다른 열은 그대로 두려면 어떻게 해야 할까요?

이를 위한 다단계 해결 방법이 있지만 매우 번거롭습니다.
더 나은 방법이 있습니다.
CSV에 일반 명령줄 도구를 활용하기 위해 `body` [@body], `header` [@header], `cols` [@cols]라는 세 가지 명령줄 도구를 소개합니다.

첫 번째 명령줄 도구인 `body`부터 시작하겠습니다.
`body`를 사용하면 CSV 파일의 본문, 즉 헤더를 제외한 모든 것에 모든 명령줄 도구를 적용할 수 있습니다.
예를 들면 다음과 같습니다.

```{console}
echo -e "value\n7\n2\n5\n3" | body sort -n
```

CSV 파일의 헤더가 한 줄만 차지한다고 가정합니다.
작동 방식은 다음과 같습니다.

- 표준 입력에서 한 줄을 가져와 *\$header*라는 변수로 저장합니다.
- 헤더를 인쇄합니다.
- 표준 입력의 나머지 데이터에 `body`로 전달된 모든 명령줄 인수를 실행합니다.

또 다른 예입니다.
다음 CSV 파일의 줄 수를 센다고 상상해 보십시오.

```{console}
seq 5 | header -a count
```

`wc -l`을 사용하면 모든 줄 수를 셀 수 있습니다.

```{console}
seq 5 | header -a count | wc -l
```

본문의 줄(헤더를 제외한 모든 것)만 고려하려면 `body`를 추가합니다.

```{console}
seq 5 | header -a count | body wc -l
```

헤더는 사용되지 않으며 출력에도 다시 인쇄됩니다.

두 번째 명령줄 도구인 `header`는 이름에서 알 수 있듯이 CSV 파일의 헤더를 조작할 수 있도록 합니다.
인수가 제공되지 않으면 CSV 파일의 헤더가 인쇄됩니다.

```{console}
< tips.csv header
```

이것은 `head -n 1`과 동일합니다.
헤더가 권장되지 않는 한 줄 이상인 경우 `-n 2`를 지정할 수 있습니다.
CSV 파일에 헤더를 추가할 수도 있습니다.

```{console}
seq 5 | header -a count
```

이것은 `echo "count" | cat - <(seq 5)`와 동일합니다.
헤더 삭제는 `-d` 옵션을 사용하여 수행됩니다.

```{console}
< iris.csv header -d | trim
```

이것은 `tail -n +2`와 유사하지만 기억하기가 좀 더 쉽습니다.
헤더 바꾸기는 기본적으로 위 소스 코드를 보면 헤더를 먼저 삭제한 다음 추가하는 것이며 `-r`을 지정하여 수행됩니다. 여기서는 `body`와 결합합니다.

```{console}
seq 5 | header -a line | body wc -l | header -r count
```

그리고 마지막으로, `body` 명령줄 도구가 본문에 수행하는 것과 유사하게 헤더에만 명령을 적용할 수 있습니다.
예를 들면 다음과 같습니다.

```{console}
seq 5 | header -a line | header -e "tr '[a-z]' '[A-Z]'"
```

세 번째 명령줄 도구는 `cols`라고 하며, 특정 명령을 열의 하위 집합에만 적용할 수 있다는 점에서 `header` 및 `body`와 유사합니다.
예를 들어 팁 데이터 세트의 요일 열 값을 대문자로 만들고(다른 열과 헤더에 영향을 주지 않고) 싶다면 다음과 같이 `cols`를 `body`와 함께 사용합니다.

```{console}
< tips.csv cols -c day body "tr '[a-z]' '[A-Z]'" | head -n 5 | csvlook
```

여러 명령줄 도구와 인수를 `header -e`, `body`, `cols`에 명령으로 전달하면 까다로운 따옴표 인용이 발생할 수 있습니다.
이러한 문제가 발생하면 이를 위해 별도의 명령줄 도구를 만들고 해당 명령을 전달하는 것이 가장 좋습니다.

결론적으로 일반적으로 CSV 데이터용으로 특별히 만들어진 명령줄 도구를 사용하는 것이 좋지만, 필요한 경우 `body`, `header`, `cols`를 사용하여 CSV 파일에 고전적인 명령줄 도구를 적용할 수도 있습니다.


### CSV에서 SQL 쿼리 수행

이 장에서 언급된 명령줄 도구가 충분한 유연성을 제공하지 않는 경우 명령줄에서 데이터를 정제하는 또 다른 방법이 있습니다.
도구 `csvsql` [@csvsql]을 사용하면 CSV 파일에서 직접 SQL 쿼리를 실행할 수 있습니다.
SQL은 데이터 정제 작업을 정의하는 데 매우 강력한 언어입니다. 개별 명령줄 도구를 사용하는 것과는 매우 다른 방식입니다.

```{block2, type="rmdnote"}
데이터가 원래 관계형 데이터베이스에서 온 경우 가능하다면 해당 데이터베이스에서 SQL 쿼리를 실행한 다음 데이터를 CSV로 추출하십시오. [3장](#chapter-3-obtaining-data)에서 설명했듯이 이를 위해 명령줄 도구 `sql2csv`를 사용할 수 있습니다. 데이터베이스에서 데이터를 CSV 파일로 먼저 내보낸 다음 SQL을 적용하면 속도가 느릴 뿐만 아니라 CSV 데이터에서 열 유형이 올바르게 유추되지 않을 가능성도 있습니다.
```

아래 정제 작업에서는 `csvsql`을 포함하는 몇 가지 솔루션을 포함합니다. 기본 명령은 다음과 같습니다.

```{console}
seq 5 | header -a val | csvsql --query "SELECT SUM(val) AS sum FROM stdin"
```

표준 입력을 `csvsql`에 전달하면 테이블 이름이 *stdin*으로 지정됩니다.
열 유형은 데이터에서 자동으로 유추됩니다.
나중에 CSV 파일 결합 섹션에서 볼 수 있듯이 여러 CSV 파일을 지정할 수도 있습니다.
<!-- #TODO: MUST: Reference SQLite -->
`csvsql`은 SQL의 SQLite 방언을 사용하며, 이는 SQL 표준과 약간의 미묘한 차이가 있다는 점에 유의하십시오.
SQL은 일반적으로 다른 솔루션보다 장황하지만 훨씬 더 유연합니다.
SQL로 정제 문제를 해결하는 방법을 이미 알고 있다면 명령줄에서 사용하는 데 부끄러워할 필요가 없습니다!


### 열 추출 및 재정렬

<!-- #TODO: MUST: Replace csvcut with xsv select -->

명령줄 도구 `csvcut` [@csvcut]을 사용하여 열을 추출하고 재정렬할 수 있습니다.
예를 들어 아이리스 데이터 세트에서 숫자 값을 포함하는 열만 유지하고 가운데 두 열의 순서를 바꾸려면 다음과 같이 합니다.

```{console}
< iris.csv csvcut -c sepal_length,petal_length,sepal_width,petal_width | csvlook
```

또는 *보수*를 의미하는 `-C` 옵션을 사용하여 제외할 열을 지정할 수도 있습니다.

```{console}
< iris.csv csvcut -C species | csvlook
```

여기서 포함된 열은 동일한 순서로 유지됩니다.
열 이름 대신 열의 인덱스(1부터 시작)를 지정할 수도 있습니다.
예를 들어 홀수 열만 선택할 수 있습니다(필요한 경우!).

```{console}
echo 'a,b,c,d,e,f,g,h,i\n1,2,3,4,5,6,7,8,9' |
csvcut -c $(seq 1 2 9 | paste -sd,)
```

값에 쉼표가 없는 것이 확실하다면 `cut`을 사용하여 열을 추출할 수도 있습니다.
`cut`은 다음 명령에서 보여주듯이 열 순서를 바꾸지 않는다는 점에 유의하십시오.

```{console}
echo 'a,b,c,d,e,f,g,h,i\n1,2,3,4,5,6,7,8,9' | cut -d, -f 5,1,3
```

보시다시피 `-f` 옵션으로 열을 지정하는 순서는 중요하지 않으며, `cut`을 사용하면 항상 원래 순서대로 표시됩니다.
완전성을 위해 아이리스 데이터 세트의 숫자 열을 추출하고 재정렬하는 SQL 접근 방식도 살펴보겠습니다.

```{console}
< iris.csv csvsql --query "SELECT sepal_length, petal_length, "\
"sepal_width, petal_width FROM stdin" | head -n 5 | csvlook
```

### 행 필터링

CSV 파일에서 행을 필터링하는 것과 일반 텍스트 파일에서 줄을 필터링하는 것의 차이점은 특정 열의 값을 기준으로 이 필터링을 수행하고 싶을 수 있다는 것입니다.
위치를 기준으로 필터링하는 것은 기본적으로 동일하지만 CSV 파일의 첫 번째 줄이 일반적으로 헤더라는 점을 고려해야 합니다.
헤더를 유지하려면 항상 `body` 명령줄 도구를 사용할 수 있다는 것을 기억하십시오.

<!-- #TODO: #MUST use xsv slice and xsv search -->

```{console}
seq 5 | sed -n '3,5p'
seq 5 | header -a count | body sed -n '3,5p'
```

특정 열 내의 특정 패턴을 기준으로 필터링하는 경우 `csvgrep` [@csvgrep], `awk` 또는 물론 `csvsql`을 사용할 수 있습니다.
예를 들어 파티 크기가 5보다 작은 모든 청구서를 제외하려면 다음과 같이 합니다.

```{console}
csvgrep -c size -i -r "[1-4]" tips.csv
```

`awk`와 `csvsql` 모두 숫자 비교도 수행할 수 있습니다.
예를 들어 토요일이나 일요일에 40달러 이상의 모든 청구서를 가져오려면 다음과 같이 합니다.

```{console}
< tips.csv awk -F, 'NR==1 || ($1 > 40.0) && ($5 ~ /^S/)'
```

`csvsql` 솔루션은 열 이름 대신 인덱스를 사용하므로 더 장황하지만 더 강력합니다.

```{console}
csvsql --query "SELECT * FROM tips WHERE bill > 40 AND day LIKE 'S%'" tips.csv
```

SQL 쿼리의 *WHERE* 절의 유연성은 SQL이 날짜와 집합에서 작동하고 복잡한 절 조합을 형성할 수 있으므로 다른 명령줄 도구와 쉽게 일치시킬 수 없다는 점에 유의해야 합니다.


### 열 병합

열 병합은 관심 있는 값이 여러 열에 분산되어 있을 때 유용합니다.
이는 날짜(연도, 월, 일이 별도의 열일 수 있음) 또는 이름(성과 이름이 별도의 열일 수 있음)의 경우에 발생할 수 있습니다.
두 번째 상황을 고려해 보겠습니다.

<!-- #TODO: Must use composers.csv -->

입력 CSV는 작곡가 목록입니다.
작업이 성과 이름을 전체 이름으로 결합하는 것이라고 상상해 보십시오.
이 작업을 위해 `sed`, `awk`, `cols` + `tr`, `csvsql`이라는 네 가지 다른 접근 방식을 제시합니다.
입력 CSV를 살펴보겠습니다.

```{console}
csvlook -I names.csv
```

첫 번째 접근 방식인 `sed`는 두 개의 문을 사용합니다.
첫 번째는 헤더를 바꾸는 것이고 두 번째는 두 번째 행부터 적용되는 역참조가 있는 정규식입니다.

```{console}
< names.csv sed -re '1s/.*/id,full_name,born/g;2,$s/(.*),(.*),(.*),(.*)/\1,\3 \2,\4/g' |
csvlook -I
```

`awk` 접근 방식은 다음과 같습니다.

```{console}
< names.csv awk -F, 'BEGIN{OFS=","; print "id,full_name,born"} {if(NR > 1) {print $1,$3" "$2,$4}}' |
csvlook -I
```

`tr`과 결합된 `cols` 접근 방식:

```{console}
< names.csv |
cols -c first_name,last_name tr \",\" \" \" |
header -r full_name,id,born |
csvcut -c id,full_name,born |
csvlook -I
```

`csvsql`은 쿼리를 실행하기 위해 SQLite를 데이터베이스로 사용하며 `||`는 연결을 의미한다는 점에 유의하십시오.

```{console}
< names.csv csvsql --query "SELECT id, first_name || ' ' || last_name "\
"AS full_name, born FROM stdin" | csvlook -I
```

*last\_name*에 쉼표가 포함되어 있으면 어떻게 될까요? 명확성을 위해 원시 입력 CSV를 살펴보겠습니다.


<!-- Ludwig van Beethoven enters the party -->
<!-- ```{console} -->
<!-- echo 'Ludwig,"Beethoven, van",1770,"Bonn,\nGermany"' >> composers.csv -->
<!-- ``` -->
<!-- bat -A composers.csv -->
<!-- //# cat composers.csv | csvquote | tr -d $'\x1f'   # comma (unit separator in unicode) -->
<!-- //# cat composers.csv | csvquote | tr $'\x1e' ' '  # new line (record separator in unicode) -->

```{console}
cat names-comma.csv
```

음, 처음 세 가지 접근 방식은 모두 다른 방식으로 실패하는 것 같습니다. `csvsql`만 성과 이름을 결합할 수 있습니다.

```{console}
< names-comma.csv sed -re '1s/.*/id,full_name,born/g;2,$s/(.*),(.*),(.*),(.*)/\1,\3 \2,\4/g' | tail -n 1
```

```{console}
< names-comma.csv awk -F, 'BEGIN{OFS=","; print "id,full_name,born"} {if(NR > 1) {print $1,$3" "$2,$4}}' | tail -n 1
```

```{console}
< names-comma.csv | cols -c first_name,last_name tr \",\" \" \" |
header -r full_name,id,born | csvcut -c id,full_name,born | tail -n 1
```

```{console}
< names-comma.csv csvsql --query "SELECT id, first_name || ' ' || last_name AS full_name, born FROM stdin" | tail -n 1
```

```{console}
< names-comma.csv rush run -t 'unite(df, full_name, first_name, last_name, sep = " ")' - | tail -n 1
```

잠깐만요! 마지막 명령은 무엇입니까? R입니까? 사실 그렇습니다.
`rush`라는 명령줄 도구를 통해 평가된 R 코드입니다. 지금 당장 말할 수 있는 것은 이 접근 방식도 두 열을 병합하는 데 성공한다는 것입니다.
이 편리한 명령줄 도구에 대해서는 나중에 설명하겠습니다.


### 여러 CSV 파일 결합

<!-- #### Concatenate Vertically -->

<!-- Vertical concatenation may be necessary in cases where you have, for example, a data set which is generated on a daily basis, or where each data set represents a different, say, market or product. -->
<!-- Let’s simulate the former by splitting up *scientists.csv*, so that we have something to combine again. -->


<!-- <\!-- #TODO: use xsv split -\-> -->
<!-- <\!-- #TODO mention that this could also be useful for parallel processing (Chapter 8) -\-> -->
<!-- <\!-- #TODO: use xsv cat instead of csvstack -\-> -->

<!-- ```{console} -->
<!--  < scientists.xml xml2json | jq -r '.Root.data.record[] | [.field[0]."$t", .field[2]."$t", .field[3]."$t"] | @csv' | header -a "country,year,value" > scientists.csv -->
<!-- xsv split -s 2000 science scientists.csv -->
<!-- wc -l science/* -->
<!-- csvlook science/12000.csv -->
<!-- ``` -->

<!-- You could just concatenate the files using `cat` and removing the headers of all but the first file using as follows: -->

<!-- A more robust solution to combine multiple CSV files is to use `xsv`: -->

<!-- Out of order -->
<!-- ```{console} -->
<!-- xsv cat rows science/*.csv | tail -->
<!-- ``` -->

<!-- Note that the rows have -->

<!-- Back in order -->

<!-- ```{console} -->
<!-- echo $(ls -v science/*.csv) -->
<!-- xsv cat rows $(ls -v science/*.csv) | tail -->
<!-- ``` -->

<!-- ```{console} -->
<!-- awk 'NR==1 {H=$0; print} $0!=H' 2021-01-0*.csv | trim -->
<!-- ``` -->


<!-- ```{console} -->
<!-- shuf -ri 0-9999 -n 24 | nl -v0 -w1 -s, | header -a hour,value > 2021-01-01.csv -->
<!-- shuf -ri 0-9999 -n 24 | nl -v0 -w1 -s, | header -a hour,value > 2021-01-02.csv -->
<!-- shuf -ri 0-9999 -n 24 | nl -v0 -w1 -s, | header -a hour,value > 2021-01-03.csv -->
<!-- head -n 3 2021-*.csv -->
<!-- ``` -->

<!-- ```{console} -->
<!-- csvstack --group-name date --filenames 2021-*.csv | sed 's/\.csv,/,/' | csvlook -->
<!-- ``` -->

<!-- ```{console} -->
<!-- awk 'NR==1 {H=$0; print "date,"$0} $0!=H {print FILENAME","$0}' 2021-01-0*.csv | csvlook -->
<!-- ``` -->


#### 가로로 연결

나란히 놓고 싶은 세 개의 CSV 파일이 있다고 가정해 보겠습니다. 파이프라인 중간에 `csvcut`의 결과를 저장하기 위해 `tee` [@tee]를 사용합니다.

```{console}
< tips.csv csvcut -c bill,tip | tee bills.csv | head -n 3 | csvlook
< tips.csv csvcut -c day,time | tee datetime.csv |
head -n 3 | csvlook -I
< tips.csv csvcut -c sex,smoker,size | tee customers.csv |
head -n 3 | csvlook
```

행이 정렬되어 있다고 가정하면 파일을 함께 `paste` [@paste]하기만 하면 됩니다.

```{console}
paste -d, {bills,customers,datetime}.csv | head -n 3 | csvlook -I
```

여기서 명령줄 인수 `-d`는 `paste`에게 구분 기호로 쉼표를 사용하도록 지시합니다.

#### 결합

때로는 데이터를 세로 또는 가로 연결로 간단히 결합할 수 없습니다.
특히 관계형 데이터베이스의 경우 중복을 최소화하기 위해 데이터가 여러 테이블(또는 파일)에 분산되어 있는 경우가 있습니다.
아이리스 데이터 세트를 세 가지 유형의 아이리스 꽃에 대한 추가 정보, 즉 USDA 식별자로 확장하고 싶다고 상상해 보십시오.
마침 이러한 식별자가 있는 별도의 CSV 파일이 있습니다.

```{console}
csvlook irismeta.csv
```

이 데이터 세트와 아이리스 데이터 세트의 공통점은 *species* 열입니다.
`csvjoin` [@csvjoin]을 사용하여 두 데이터 세트를 결합할 수 있습니다.

```{console}
csvjoin -c species iris.csv irismeta.csv | csvcut -c sepal_length,sepal_width,species,usda_id | sed -n '1p;49,54p' | csvlook
```

물론 `csvsql`을 사용하는 SQL 접근 방식도 사용할 수 있으며, 이는 평소와 같이 약간 더 길지만 잠재적으로 훨씬 더 유연합니다.

```{console}
csvsql --query 'SELECT i.sepal_length, i.sepal_width, i.species, m.usda_id FROM iris i JOIN irismeta m ON (i.species = m.species)' iris.csv irismeta.csv | sed -n '1p;49,54p' | csvlook
```

<!-- ## JSON -->

<!-- ## XML -->

<!-- ## HTML -->

## XML/HTML 및 JSON 작업

이 섹션에서는 데이터를 한 형식에서 다른 형식으로 변환할 수 있는 몇 가지 명령줄 도구를 보여줄 것입니다.
데이터를 변환하는 데에는 두 가지 이유가 있습니다.

첫째, 많은 시각화 및 기계 학습 알고리즘이 데이터베이스 테이블이나 스프레드시트와 같이 표 형식의 데이터를 필요로 하는 경우가 많습니다.
CSV는 본질적으로 표 형식이지만 JSON 및 HTML/XML 데이터는 깊이 중첩된 구조를 가질 수 있습니다.

둘째, `cut` 및 `grep`과 같은 고전적인 도구를 포함한 많은 명령줄 도구는 일반 텍스트에서 작동합니다.
이는 텍스트가 명령줄 도구 간의 보편적인 인터페이스로 간주되기 때문입니다.
또한 다른 형식은 더 젊습니다. 이러한 각 형식은 일반 텍스트로 처리될 수 있으므로 이러한 명령줄 도구를 다른 형식에도 적용할 수 있습니다.

때로는 구조화된 데이터에 고전적인 도구를 적용하여 문제를 해결할 수 있습니다.
예를 들어 아래 JSON 데이터를 일반 텍스트로 처리하여 `sed`를 사용하여 *gender* 속성을 *sex*로 변경할 수 있습니다.

```{console}
sed -e 's/"gender":/"sex":/g' users.json | jq | trim
```

다른 많은 명령줄 도구와 마찬가지로 `sed`는 데이터 구조를 사용하지 않습니다.
데이터 구조를 사용하는 도구(예: 아래에서 설명하는 `jq`)를 사용하거나 데이터를 CSV와 같은 표 형식으로 먼저 변환한 다음 적절한 명령줄 도구를 적용하는 것이 좋습니다.

XML/HTML 및 JSON을 CSV로 변환하는 실제 사용 사례를 통해 시연할 것입니다.
여기서 사용할 명령줄 도구는 `curl`, `pup` [@pup], `xml2json` [@xml2json], `jq` 및 `json2csv` [@json2csv]입니다.

위키백과에는 풍부한 정보가 있습니다. 이 정보의 대부분은 데이터 세트로 간주될 수 있는 표로 정렬되어 있습니다.
예를 들어 [이 페이지](http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_border/area_ratio)에는 국경 길이, 면적 및 둘 사이의 비율과 함께 국가 및 영토 목록이 포함되어 있습니다.

이 데이터를 분석하는 데 관심이 있다고 상상해 보겠습니다. 이 섹션에서는 필요한 모든 단계와 해당 명령을 안내합니다. 모든 세부 사항을 다루지는 않으므로 즉시 모든 것을 이해하지 못할 수도 있습니다. 걱정하지 마십시오. 요점을 파악할 수 있을 것이라고 확신합니다. 이 섹션의 목적은 명령줄을 시연하는 것임을 기억하십시오. 이 섹션에서 사용된 모든 도구와 개념(그리고 그 이상)은 다음 장에서 설명합니다.

관심 있는 데이터 세트는 HTML에 포함되어 있습니다.
목표는 이 데이터 세트를 작업할 수 있는 표현으로 만드는 것입니다.
가장 첫 번째 단계는 `curl`을 사용하여 HTML을 다운로드하는 것입니다.

```{console}
curl -sL 'http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_border/area_ratio' > wiki.html
```

HTML은 *wiki.html*이라는 파일에 저장됩니다.
처음 10줄이 어떻게 보이는지 살펴보겠습니다.

```{console}
< wiki.html trim
```

정상적으로 보입니다.
관심 있는 루트 HTML 요소가 *wikitable* 클래스를 가진 *&lt;table&gt;*임을 확인할 수 있었다고 상상해 보십시오.
이를 통해 `grep`을 사용하여 관심 있는 부분을 볼 수 있습니다(`-A` 옵션은 일치하는 줄 뒤에 인쇄할 줄 수를 지정합니다).

```{console}
grep wikitable -A 21 wiki.html
```

이제 실제로 국가와 해당 값을 볼 수 있습니다.
다음 단계는 HTML 파일에서 필요한 요소를 추출하는 것입니다.
이를 위해 `pup`을 사용할 수 있습니다.

```{console}
< wiki.html pup 'table.wikitable tbody' | tee table.html | trim
```

`pup`에 전달된 표현식은 CSS 선택기입니다.
구문은 일반적으로 웹 페이지 스타일을 지정하는 데 사용되지만 HTML에서 특정 요소를 선택하는 데에도 사용할 수 있습니다.
이 경우 *wikitable* 클래스를 가진 *table*의 *tbody*를 선택하려고 합니다.
다음은 XML(및 HTML)을 JSON으로 변환하는 `xml2json`입니다.

```{console}
< table.html xml2json > table.json
jq . table.json | trim 20
```

HTML을 JSON으로 변환하는 이유는 JSON 데이터에서 작동하는 매우 강력한 도구인 `jq`가 있기 때문입니다.
다음 명령은 JSON 데이터의 특정 부분을 추출하여 작업할 수 있는 형태로 재구성합니다.


```{console}
< table.json jq -r '.tbody.tr[1:][] | [.td[]["$t"]] | @csv' | header -a rank,country,border,surface,ratio > countries.csv
```

이제 데이터가 작업할 수 있는 형태가 되었습니다.
위키백과 페이지에서 CSV 데이터 세트를 얻기까지 꽤 많은 단계가 있었습니다.
그러나 위의 모든 명령을 하나로 결합하면 실제로는 매우 간결하고 표현력이 풍부하다는 것을 알 수 있습니다.

```{console}
csvlook --max-column-width 28 countries.csv
```

이것으로 XML/HTML을 JSON으로, JSON을 CSV로 변환하는 시연을 마칩니다.
`jq`는 훨씬 더 많은 작업을 수행할 수 있고 XML 데이터로 작업하기 위한 특수 도구가 존재하지만, 제 경험상 데이터를 가능한 한 빨리 CSV 형식으로 변환하는 것이 잘 작동하는 경향이 있습니다.
이렇게 하면 매우 특정한 도구보다는 일반적인 명령줄 도구에 능숙해지는 데 더 많은 시간을 할애할 수 있습니다.


## 요약

이 장에서는 데이터 정리 또는 정제에 대해 살펴보았습니다.
보시다시피 데이터의 모든 지저분함을 마법처럼 제거할 수 있는 단일 도구는 없습니다. 원하는 결과를 얻으려면 종종 여러 가지 다른 도구를 결합해야 합니다.
`cut` 및 `sort`와 같은 고전적인 명령줄 도구는 구조화된 데이터를 해석할 수 없다는 점에 유의하십시오.
다행히 JSON 및 XML과 같은 하나의 데이터 형식을 CSV와 같은 다른 데이터 형식으로 변환하는 도구가 있습니다.
다음 장인 막간 장에서는 `make`를 사용하여 프로젝트를 관리하는 방법을 보여 드리겠습니다.
[7장](#chapter-7-exploring-data)에서 데이터를 탐색하고 시각화하는 것을 기다릴 수 없다면 이 장을 건너뛰어도 됩니다.



## 추가 탐색을 위해

- `awk`에 대해 더 자세히 설명하고 싶었습니다. 매우 강력한 도구이자 프로그래밍 언어입니다. 시간을 내어 배우는 것이 좋습니다. Doherty와 Robbins의 책 *sed & awk*와 온라인 [GNU Awk 사용자 가이드](https://www.gnu.org/software/gawk/manual/gawk.html)는 좋은 자료입니다.
- 이 장에서는 몇 군데에서 정규 표현식을 사용했습니다. 안타깝게도 정규 표현식에 대한 자습서는 이 책의 범위를 벗어납니다. 정규 표현식은 다양한 도구에서 사용할 수 있으므로 배우는 것이 좋습니다. Jan Goyvaerts와 Steven Levithan의 책 *Regular Expressions Cookbook*은 좋은 책입니다.
