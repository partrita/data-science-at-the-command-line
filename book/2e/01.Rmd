---
suppress-bibliography: true
---

```{r console_start, include=FALSE}
console_start()
```

```{console setup_history, include=FALSE}
 export CHAPTER="01"
 export HISTFILE=/history/history_${CHAPTER}
 rm -f $HISTFILE
```


<!--A[role="pagenumrestart"]
A-->
# 서론 {#chapter-1-introduction}

이 책은 커맨드 라인에서 데이터 과학을 수행하는 것에 관한 책입니다.
저의 목표는 커맨드 라인의 힘을 활용하는 방법을 가르쳐드림으로써 여러분을 더 효율적이고 생산적인 데이터 과학자로 만드는 것입니다.

제목에 *데이터 과학*과 *커맨드 라인*이라는 두 용어가 함께 쓰인 것에 대해 설명이 필요할 것 같습니다.
어떻게 50년도 더 된 기술[^1]이 이제 겨우 몇 년밖에 되지 않은 분야에 도움이 될 수 있을까요?

오늘날 데이터 과학자들은 파이썬, R, Julia, Apache Spark 등 흥미롭고 압도적인 기술과 프로그래밍 언어들 중에서 선택할 수 있습니다.
여러분은 이미 이들 중 하나 이상에 익숙할지도 모릅니다.
그렇다면 왜 여전히 데이터 과학을 위해 커맨드 라인에 관심을 가져야 할까요?
커맨드 라인이 다른 기술이나 프로그래밍 언어들이 제공하지 못하는 무엇을 가지고 있을까요?

이것들은 모두 타당한 질문입니다.
이 첫 번째 장에서 저는 다음과 같은 방식으로 그 답을 제시하고자 합니다.
먼저, 이 책의 근간이 될 데이터 과학의 실무적인 정의를 제공하겠습니다.
둘째로, 커맨드 라인이 가진 다섯 가지 중요한 장점을 나열하겠습니다.
이 장을 마칠 때쯤이면 여러분도 데이터 과학을 위해 커맨드 라인을 배울 가치가 충분하다는 점에 동의하시게 될 것입니다.


## 데이터 과학은 OSEMN(Awesome)하다

데이터 과학 분야는 아직 초기 단계에 있으며, 따라서 데이터 과학이 무엇을 포함하는지에 대한 다양한 정의가 존재합니다.
이 책 전체에 걸쳐 저는 @Mason2010이 제시한 매우 실무적인 정의를 따를 것입니다.
그들은 데이터 과학을 다음과 같은 다섯 단계로 정의합니다: (1) 데이터 획득(Obtaining), (2) 데이터 정제(Scrubbing), (3) 데이터 탐색(Exploring), (4) 데이터 모델링(Modeling), (5) 데이터 해석(Interpreting).
이 다섯 단계의 앞글자를 따서 OSEMN 모델이라고 부릅니다(발음은 *awesome*과 같습니다).
이 정의는 이 책의 뼈대 역할을 하는데, 아래에서 설명할 5단계 '데이터 해석'을 제외한 각 단계가 별도의 장으로 구성되어 있기 때문입니다.

비록 이 다섯 단계가 선형적이고 점진적인 방식으로 논의되지만, 실제로는 각 단계 사이를 오가거나 여러 단계를 동시에 수행하는 것이 매우 일반적입니다.
\@ref(fig:diagram-osemn)은 데이터 과학을 수행하는 것이 반복적이고 비선형적인 과정임을 보여줍니다.
예를 들어, 데이터를 모델링하고 결과를 확인한 후, 데이터셋의 특징(features)을 조정하기 위해 정제 단계로 되돌아가기로 결정할 수도 있습니다.

```{r diagram-osemn, echo=FALSE, fig.cap="데이터 과학은 반복적이고 비선형적인 과정입니다", fig.align="center", out.width="90%"}
knitr::include_graphics("images/dscl_0101.png")
```

아래에서는 각 단계가 무엇을 의미하는지 설명하겠습니다.


### 데이터 획득 (Obtaining Data)

데이터 없이는 데이터 과학을 할 수 없습니다.
따라서 첫 번째 단계는 데이터를 얻는 것입니다.
이미 데이터를 보유하고 있는 행운아가 아니라면, 다음과 같은 작업 중 하나 이상을 수행해야 할 수도 있습니다.

- 다른 위치(예: 웹페이지나 서버)에서 데이터 다운로드
- 데이터베이스나 API(예: MySQL이나 Twitter)에서 데이터 쿼리
- 다른 파일(예: HTML 파일이나 스프레드시트)에서 데이터 추출
- 데이터 직접 생성(예: 센서 데이터 읽기나 설문 조사 수행)

[3장](#chapter-3-obtaining-data)에서는 커맨드 라인을 사용하여 데이터를 획득하는 여러 가지 방법을 다룹니다.
획득한 데이터는 대부분 텍스트, CSV, JSON, HTML, XML 형식 중 하나일 것입니다.
다음 단계는 이 데이터를 정제하는 것입니다.


### 데이터 정제 (Scrubbing Data)

획득한 데이터에 결측치, 불일치, 오류, 이상한 문자 또는 불필요한 열이 포함되어 있는 경우는 매우 흔합니다.
이 경우, 데이터로 무언가 흥미로운 작업을 하기 전에 데이터를 *정제(scrub)*하거나 깨끗하게(clean) 만들어야 합니다.
일반적인 정제 작업은 다음과 같습니다.

- 행 필터링
- 특정 열 추출
- 값 치환
- 단어 추출
- 결측치 및 중복 처리
- 데이터를 한 형식에서 다른 형식으로 변환

데이터 과학자로서 우리는 흥미로운 시각화와 통찰력 있는 모델을 만드는 것(3, 4단계)을 좋아하지만, 대개는 필요한 데이터를 먼저 획득하고 정제하는 데(1, 2단계) 훨씬 많은 노력을 들입니다.
*Data Jujitsu*에서 @Patil2012는 "어떤 데이터 프로젝트에서든 업무의 80%는 데이터를 청소하는 일이다"라고 언급했습니다.
[5장](#chapter-5-scrubbing-data)에서는 커맨드 라인이 이러한 데이터 정제 작업을 완수하는 데 어떻게 도움이 되는지 보여줍니다.


### 데이터 탐색 (Exploring Data)

데이터를 정제했다면 이제 탐색할 준비가 되었습니다.
탐색을 통해 여러분은 데이터를 진정으로 알게 되므로 이 단계부터 흥미진진해집니다.
[7장](#chapter-7-exploring-data)에서는 커맨드 라인을 사용하여 다음 작업을 수행하는 방법을 보여드립니다.

- 데이터 살펴보기
- 데이터에서 통계량 도출
- 통찰력 있는 시각화 생성

[7장](#chapter-7-exploring-data)에서 소개되는 커맨드 라인 도구에는 `csvstat` [@csvstat]과 `rush` [@rush]가 포함됩니다.


### 데이터 모델링 (Modeling Data)

데이터를 설명하거나 앞으로 일어날 일을 예측하고 싶다면, 데이터에 대한 통계 모델을 만들고 싶을 것입니다.
모델을 만드는 기법으로는 클러스터링, 분류, 회귀, 차원 축소 등이 있습니다.
커맨드 라인은 새로운 유형의 모델을 처음부터 프로그래밍하기에는 적합하지 않습니다.
하지만 커맨드 라인에서 모델을 빌드하는 것은 매우 유용합니다.
[9장](#chapter-9-modeling-data)에서는 로컬에서 모델을 빌드하거나 API를 활용하여 클라우드에서 연산을 수행하는 여러 커맨드 라인 도구를 소개하겠습니다.


### 데이터 해석 (Interpreting Data)

OSEMN 모델의 마지막이자 어쩌면 가장 중요한 단계는 데이터를 해석하는 것입니다.
이 단계는 다음을 포함합니다.

- 데이터에서 결론 도출
- 결과의 의미 평가
- 결과 공유 및 커뮤니케이션

솔직히 말해서, 이 단계에서 컴퓨터의 역할은 거의 없으며 커맨드 라인도 딱히 개입할 여지가 없습니다.
일단 이 단계에 도달했다면, 그다음은 여러분에게 달려 있습니다.
이 장은 OSEMN 모델 중 유일하게 별도의 장이 없는 단계입니다.
대신 @Shron2014의 명저 *Thinking with Data*를 참고하시길 권합니다.


## 간주(Intermezzo) 장들

OSEMN 단계들을 다루는 장들 외에도 네 개의 간주(Intermezzo) 장이 있습니다.
각 장은 데이터 과학에 관한 보다 일반적인 주제와 이를 위해 커맨드 라인이 어떻게 사용되는지를 논의합니다.
이 주제들은 데이터 과학 프로세스의 어떤 단계에도 적용될 수 있습니다.

[4장](#chapter-4-creating-command-line-tools)에서는 커맨드 라인을 위한 재사용 가능한 도구를 만드는 방법을 다룹니다.
이런 개인용 도구는 커맨드 라인에 직접 입력했던 긴 명령어나 파이썬, R 등으로 작성했던 기존 코드에서 만들어질 수 있습니다.
직접 도구를 만들 수 있게 되면 효율성과 생산성이 크게 향상됩니다.

커맨드 라인은 데이터 과학을 위한 대화형 환경이기 때문에 전체 워크플로우를 추적하는 것이 어려울 수 있습니다.
[6장](#chapter-6-project-management-with-make)에서는 작업과 작업 간의 의존성 관점에서 데이터 과학 워크플로우를 정의할 수 있게 해주는 `make`라는 커맨드 라인 도구를 소개합니다.
이 도구는 여러분뿐만 아니라 동료들의 워크플로우 재현성(reproducibility)을 높여줍니다.

[8장](#chapter-8-parallel-pipelines)에서는 명령어와 도구들을 병렬로 실행하여 속도를 높이는 방법을 설명합니다.
GNU Parallel [@parallel]이라는 커맨드 라인 도구를 사용하면, 매우 큰 데이터셋에 도구들을 적용하고 여러 코어 또는 원격 머신에서 실행할 수 있습니다.

[10장](#chapter-10-polyglot-data-science)에서는 R, RStudio, 파이썬, Jupyter Notebook, 심지어 Apache Spark와 같은 다른 환경과 프로그래밍 언어에서 커맨드 라인의 힘을 활용하는 방법을 다룹니다.


## 커맨드 라인이란 무엇인가?

데이터 과학을 위해 왜 커맨드 라인을 사용해야 하는지 논의하기 전에, 커맨드 라인이 실제로 어떻게 생겼는지 잠시 살펴보겠습니다(이미 익숙하실 수도 있습니다).
\@ref(fig:mac-terminal)과 \@ref(fig:ubuntu-terminal)은 각각 macOS와 우분투에서 기본적으로 나타나는 커맨드 라인의 스크린샷입니다.
우분투는 GNU/리눅스의 특정 배포판이며, 이 책에서 제가 사용할 환경입니다.

```{r mac-terminal, echo=FALSE, fig.cap="macOS의 커맨드 라인", fig.align="center"}
knitr::include_graphics("images/screenshot_terminal_mac_dst.png")
```

```{r ubuntu-terminal, echo=FALSE, fig.cap="우분투의 커맨드 라인", fig.align="center"}
knitr::include_graphics("images/screenshot_terminal_ubuntu_dst.png")
```

두 스크린샷에 보이는 창을 *터미널(terminal)*이라고 부릅니다.
이것은 쉘(shell)과 상호작용할 수 있게 해주는 프로그램입니다.
그 쉘이 제가 입력한 명령어들을 실행합니다.
[2장](#chapter-2-getting-started)에서 이 두 용어에 대해 더 자세히 설명하겠습니다.

```{block2, type="rmdnote"}
마이크로소프트 윈도우의 커맨드 라인(명령 프롬프트 또는 PowerShell)은 이 책에서 제시하는 명령어들과 근본적으로 다르고 호환되지 않기 때문에 보여드리지 않습니다.
다행인 점은 윈도우에 Docker 이미지를 설치하여 내용을 따라올 수 있다는 것입니다.
Docker 이미지를 설치하는 방법은 [2장](#chapter-2-getting-started)에서 설명합니다.
```

명령어를 입력하는 것은 *그래픽 사용자 인터페이스*(GUI)를 통해 컴퓨터와 상호작용하는 것과는 매우 다른 방식입니다.
주로 마이크로소프트 엑셀 등으로 데이터를 처리하는 데 익숙하다면 이 방식이 처음에는 위협적으로 느껴질 수 있습니다.
두려워하지 마세요.
커맨드 라인 작업에 매우 빠르게 익숙해질 것이라는 제 말을 믿으셔도 좋습니다.

이 책에서 제가 입력하는 명령어와 그 결과물은 텍스트로 표시됩니다.
예를 들어, 두 스크린샷의 터미널 내용은 다음과 같이 보일 것입니다.

```{console, keep_last_prompt=TRUE}
whoami
date
echo 'The command line is awesome!' | cowsay -f tux
```

또한 각 명령어 앞에 달러 표시(**`$`**)가 붙어 있는 것을 보실 수 있습니다.
이를 프롬프트(prompt)라고 부릅니다.
스크린샷의 프롬프트는 사용자 이름, 날짜, 펭귄 등 더 많은 정보를 보여주었습니다.
예제에서는 달러 표시만 보여주는 것이 관례인데, 그 이유는 프롬프트가 (1) 세션 중에 변경될 수 있고(디렉터리를 이동할 때), (2) 사용자가 커스터마이징할 수 있으며(예: 시간이나 현재 작업 중인 `git` [@git] 브랜치를 표시할 수도 있음), (3) 명령어 자체와는 무관하기 때문입니다.

다음 장에서는 필수적인 커맨드 라인 개념들에 대해 훨씬 더 많이 설명하겠습니다.
이제 왜 데이터 과학을 위해 커맨드 라인을 배워야 하는지 먼저 알아볼 차례입니다.


## 왜 커맨드 라인에서 데이터 과학을 하는가?

커맨드 라인은 여러분을 더 효율적이고 생산적인 데이터 과학자로 만들어 줄 수 있는 수많은 장점을 가지고 있습니다.
그 장점들을 크게 묶어보자면 커맨드 라인은 기민하고(agile), 보완적이며(augmenting), 확장 가능하고(scalable), 유연하며(extensible), 어디에나 존재(ubiquitous)합니다.
각 장점에 대해 아래에서 자세히 설명하겠습니다.


### 커맨드 라인은 기민하다 (Agile)

커맨드 라인의 첫 번째 장점은 기민하게 움직일 수 있게 해준다는 점입니다.
데이터 과학은 매우 대화형이고 탐색적인 성격을 띠고 있으며, 여러분의 작업 환경도 이를 허용해야 합니다.
커맨드 라인은 두 가지 수단을 통해 이를 달성합니다.

첫째, 커맨드 라인은 소위 *반복적 실행 환경*(REPL: read-eval-print-loop)을 제공합니다.
이는 명령어를 입력하고 **`Enter`**를 누르면 명령이 즉시 평가된다는 의미입니다.
REPL은 스크립트, 대형 프로그램, Hadoop 작업 등에 수반되는 '편집-컴파일-실행-디버그' 주기보다 데이터 과학을 수행하기에 훨씬 편리한 경우가 많습니다.
명령어는 즉시 실행되고, 마음대로 중단할 수 있으며, 빠르게 수정할 수 있습니다.
이러한 짧은 반복 주기는 데이터를 마음껏 다루어 볼 수 있게 해줍니다.

둘째, 커맨드 라인은 파일 시스템과 매우 밀접해 있습니다.
데이터가 데이터 과학의 주재료인 만큼, 데이터셋이 포함된 파일들을 쉽게 다룰 수 있는 것은 매우 중요합니다.
커맨드 라인은 이를 위한 수많은 편리한 도구들을 제공합니다.


### 커맨드 라인은 보완적이다 (Augmenting)

커맨드 라인은 다른 기술들과 잘 통합됩니다.
여러분의 데이터 과학 워크플로우가 현재 어떤 기술(R, 파이썬, 엑셀 등)을 포함하고 있든, 제가 그 워크플로우를 버리라고 제안하는 것이 아님을 알아주셨으면 합니다.
대신 커맨드 라인을 여러분이 현재 사용하는 기술들을 증폭시켜 주는 보완적인 기술로 생각해보세요.
이는 세 가지 방식으로 이루어질 수 있습니다.

첫째, 커맨드 라인은 서로 다른 많은 데이터 과학 도구들 사이의 접착제 역할을 할 수 있습니다.
도구들을 연결하는 한 가지 방법은 첫 번째 도구의 출력을 두 번째 도구의 입력으로 연결하는 것입니다.
[2장](#chapter-2-getting-started)에서 이것이 어떻게 작동하는지 설명합니다.

둘째, 여러분의 작업 환경 내에서 작업의 일부를 커맨드 라인에 위임할 수 있습니다.
예를 들어 파이썬, R, Apache Spark에서는 커맨드 라인 도구를 실행하고 그 결과를 캡처할 수 있습니다.
[10장](#chapter-10-polyglot-data-science)에서 예제와 함께 이를 보여드립니다.

셋째, 여러분의 코드(예: 파이썬이나 R 스크립트)를 재사용 가능한 커맨드 라인 도구로 변환할 수 있습니다.
그렇게 되면 그 도구가 어떤 언어로 작성되었는지는 더 이상 중요하지 않게 됩니다.
이제 커맨드 라인에서 직접 사용하거나, 앞서 언급한 대로 커맨드 라인과 통합되는 어떤 환경에서도 해당 도구를 사용할 수 있습니다.
[4장](#chapter-4-creating-command-line-tools)에서 그 방법을 설명합니다.

결국 모든 기술에는 강점과 약점이 있으므로, 여러 가지를 알고 상황에 가장 적합한 것을 골라 쓰는 것이 좋습니다.
때로는 그것이 R일 수도 있고, 때로는 커맨드 라인일 수도 있으며, 때로는 펜과 종이일 수도 있습니다.
이 책을 마칠 때쯤이면 여러분은 언제 커맨드 라인을 사용해야 할지, 그리고 언제 즐겨 쓰는 프로그래밍 언어나 통계 컴퓨팅 환경을 계속 사용하는 것이 나을지에 대해 확고한 이해를 갖게 될 것입니다.


### 커맨드 라인은 확장 가능하다 (Scalable)

앞서 말씀드렸듯이 커맨드 라인에서 작업하는 것은 GUI를 사용하는 것과 매우 다릅니다.
커맨드 라인에서는 타이핑을 통해 무언가를 하는 반면, GUI에서는 마우스로 가리키고 클릭함으로써 작업을 수행합니다.

커맨드 라인에서 수동으로 입력하는 모든 것은 스크립트와 도구를 통해 자동화될 수 있습니다.
덕분에 실수를 했거나, 입력 데이터가 바뀌었거나, 동료가 같은 분석을 수행하고 싶어 할 때 명령어를 다시 실행하기가 매우 쉬워집니다.
게다가 명령어는 특정 간격으로, 원격 서버에서, 그리고 데이터의 수많은 조각에 대해 병렬로 실행될 수 있습니다([8장](#chapter-8-parallel-pipelines)에서 더 자세히 다룹니다).

커맨드 라인은 자동화가 가능하기 때문에 확장성과 반복성을 갖게 됩니다.
가리키고 클릭하는 동작을 자동화하는 것은 쉽지 않으며, 이 때문에 GUI는 확장 가능하고 반복적인 데이터 과학을 하기에 덜 적합한 환경입니다.


### 커맨드 라인은 유연하다 (Extensible)

커맨드 라인 자체는 50년도 더 전에 발명되었습니다.
핵심 기능은 대체로 변하지 않았지만, 커맨드 라인의 핵심 일꾼들인 *도구들*은 매일같이 개발되고 있습니다.

커맨드 라인 그 자체는 언어에 종속되지 않습니다(language-agnostic).
덕분에 커맨드 라인 도구들은 수많은 서로 다른 프로그래밍 언어로 작성될 수 있습니다.
오픈 소스 커뮤니티는 우리가 데이터 과학을 위해 사용할 수 있는 수많은 무료 고품질 커맨드 라인 도구들을 내놓고 있습니다.

이러한 커맨드 라인 도구들은 서로 협력할 수 있으며, 이는 커맨드 라인을 매우 유연하게 만듭니다.
또한 여러분만의 도구를 만들 수도 있어 커맨드 라인의 실질적인 기능을 확장할 수 있습니다.


### 커맨드 라인은 어디에나 존재한다 (Ubiquitous)

커맨드 라인은 우분투 리눅스와 macOS를 포함한 모든 유닉스 계열 운영체제에 포함되어 있어 많은 곳에서 찾아볼 수 있습니다.
게다가 전 세계 상위 500대 슈퍼컴퓨터의 100%가 리눅스를 사용하고 있습니다.[^2]
따라서 혹시라도 그런 슈퍼컴퓨터 중 하나를 만져볼 기회가 생긴다면(혹은 쥬라기 공원에서 문 잠금 장치가 작동하지 않는 상황에 처한다면), 커맨드 라인 사용법을 잘 알고 있는 편이 좋을 것입니다!

하지만 리눅스는 슈퍼컴퓨터에서만 돌아가는 것이 아닙니다.
서버, 노트북, 그리고 임베디드 시스템에서도 돌아갑니다.
요즘 많은 기업들이 클라우드 컴퓨팅을 제공하며, 즉석에서 새로운 머신을 쉽게 실행할 수 있습니다.
그런 머신(또는 일반적인 서버)에 접속하게 된다면, 거의 확실하게 커맨드 라인을 마주하게 될 것입니다.

또한 커맨드 라인이 단순히 일시적인 유행이 아니라는 점에 주목하는 것도 중요합니다.
이 기술은 50년 넘게 존재해 왔으며, 앞으로 50년 동안도 계속될 것이라고 확신합니다.
따라서 (데이터 과학을 위해서든 일반적인 용도를 위해서든) 커맨드 라인 사용법을 배우는 것은 투자할 만한 가치가 있는 일입니다.


## 요약

이 장에서는 책 전체의 가이드로 사용할 데이터 과학을 위한 OSEMN 모델을 소개했습니다.
유닉스 커맨드 라인에 대한 몇 가지 배경 지식을 제공했으며, 커맨드 라인이 데이터 과학을 수행하기에 적합한 환경임을 납득해 주셨기를 바랍니다.
다음 장에서는 데이터셋과 도구들을 설치하고 기본적인 개념들을 설명하며 본격적으로 시작해보겠습니다.


## 더 읽을거리

- 브라이언 커니핸(Brian W. Kernighan)의 저서 *UNIX: A History and a Memoir*는 유닉스가 무엇인지, 어떻게 개발되었는지, 그리고 왜 중요한지에 대한 이야기를 들려줍니다.
- 2018년 런던 Strata에서 저는 *50 Reasons to Learn the Shell for Doing Data Science*라는 제목으로 발표했습니다. 더 많은 설득이 필요하다면 [슬라이드](https://datascienceatthecommandline.com/resources/50-reasons.pdf)를 읽어보세요.
- Max Shron의 짧지만 알찬 책 *Thinking with Data*는 '어떻게'보다 '왜'에 집중하여, 올바른 질문을 던지고 올바른 문제를 해결하는 데 도움이 될 데이터 과학 프로젝트 정의를 위한 프레임워크를 제공합니다.

[^1]: 유닉스 운영체제의 개발은 1969년에 시작되었습니다. 시작부터 커맨드 라인을 갖추고 있었습니다. [2.3절](#essential-concepts)에서 다룰 파이프(pipes)라는 중요한 개념은 1973년에 추가되었습니다.

[^2]: 얼마나 많은 슈퍼컴퓨터가 리눅스를 실행하는지 추적하는 [TOP500](https://top500.org/statistics/details/osfam/1/)을 참고하세요.
