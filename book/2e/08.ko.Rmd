---
suppress-bibliography: true
---

```{r console_start, include=FALSE}
console_start()
```

```{console setup_history, include=FALSE}
 export CHAPTER="08"
 export HISTFILE=/history/history_${CHAPTER}
 rm -f $HISTFILE
```


# 병렬 파이프라인 {#chapter-8-parallel-pipelines}

<!-- #TODO: SHOULD: Dicuss progress bar -->

이전 장에서는 전체 작업을 한 번에 처리하는 명령과 파이프라인을 다루었습니다.
그러나 실제로는 동일한 명령이나 파이프라인을 여러 번 실행해야 하는 작업에 직면할 수 있습니다.
예를 들어 다음과 같은 작업이 필요할 수 있습니다.

- 수백 개의 웹 페이지 스크랩
- 수십 개의 API 호출을 수행하고 해당 출력을 변환
- 다양한 매개변수 값에 대해 분류기를 학습
- 데이터 세트의 모든 기능 쌍에 대한 산점도를 생성

위의 예에서 어떤 형태의 반복이 포함됩니다.
즐겨 사용하는 스크립팅 또는 프로그래밍 언어를 사용하면 for 루프 또는 while 루프로 이를 처리합니다.
명령줄에서 가장 먼저 하고 싶은 일은 **`위쪽 화살표`** 키를 눌러 이전 명령을 다시 가져오고 필요한 경우 수정하고 **`Enter`** 키를 눌러 명령을 다시 실행하는 것입니다.
두세 번은 괜찮지만 수십 번 이 작업을 수행한다고 상상해 보십시오.
이러한 접근 방식은 금방 번거롭고 비효율적이며 오류가 발생하기 쉽습니다.
좋은 소식은 명령줄에서도 이러한 루프를 작성할 수 있다는 것입니다.
이 장은 바로 그것에 관한 것입니다.

때로는 빠른 명령을 차례로 반복하는 것(*직렬* 방식)으로 충분합니다.
여러 코어(그리고 어쩌면 여러 시스템)가 있는 경우 특히 데이터 집약적인 작업에 직면했을 때 이를 활용할 수 있다면 좋을 것입니다.
여러 코어 또는 시스템을 사용하면 총 실행 시간이 크게 줄어들 수 있습니다.
이 장에서는 정확히 이를 처리할 수 있는 매우 강력한 도구인 `parallel`[@parallel]을 소개합니다. 이를 통해 숫자, 줄, 파일과 같은 다양한 인수에 대해 명령이나 파이프라인을 적용할 수 있습니다.
또한 이름에서 알 수 있듯이 명령을 *병렬*로 실행할 수 있습니다.


## 개요

이 막간 장에서는 명령과 파이프라인을 여러 번 실행해야 하는 작업의 속도를 높이는 몇 가지 접근 방식을 설명합니다.
제 주요 목표는 `parallel`의 유연성과 강력함을 보여주는 것입니다.
이 도구는 이 책에서 설명하는 다른 모든 도구와 결합할 수 있으므로 데이터 과학을 위해 명령줄을 사용하는 방식을 긍정적으로 바꿀 것입니다.
이 장에서는 다음에 대해 배웁니다.

- 숫자, 줄, 파일 범위에 대해 직렬로 명령을 실행
- 큰 작업을 여러 개의 작은 작업으로 나누기
- 병렬로 파이프라인 실행
- 여러 시스템에 파이프라인 배포

이 장은 다음 파일로 시작합니다.

```{console cd}
cd /data/ch08
l
```

이러한 파일을 가져오는 지침은 [2장](#chapter-2-getting-started)에 있습니다.
다른 모든 파일은 명령줄 도구를 사용하여 다운로드하거나 생성됩니다.


## 직렬 처리

병렬화에 대해 알아보기 전에 직렬 방식으로 루핑하는 것을 간략하게 설명하겠습니다.
이 기능은 항상 사용할 수 있고 구문이 다른 프로그래밍 언어의 루핑과 매우 유사하며 `parallel`을 정말로 감사하게 될 것이므로 이 방법을 아는 것이 좋습니다.

이 장의 소개에 제공된 예에서 루프할 세 가지 유형의 항목을 추출할 수 있습니다. 숫자, 줄, 파일입니다.
이 세 가지 유형의 항목은 다음 세 하위 섹션에서 각각 설명합니다.


### 숫자에 대한 루핑

0에서 100 사이의 모든 짝수의 제곱을 계산해야 한다고 상상해 보십시오. `bc`[@bc]라는 도구가 있는데, 이는 방정식을 파이프할 수 있는 *기본 계산기*입니다.
4의 제곱을 계산하는 명령은 다음과 같습니다.

```{console bc}
echo "4^2" | bc
```

일회성 계산에는 이것으로 충분합니다.
그러나 소개에서 언급했듯이 **`위쪽 화살표`** 키를 누르고 숫자를 변경하고 **`Enter`** 키를 50번 누르는 것은 미친 짓일 것입니다!
이 경우 for 루프를 사용하여 셸이 힘든 작업을 수행하도록 하는 것이 좋습니다.

```{console for_loop}
for i in {0..100..2}  #<1>
do
echo "$i^2" | bc      #<2>
done | trim
```
<1> Z 셸에는 중괄호 확장이라는 기능이 있으며, 이는 *`{0..100..2}`*를 공백으로 구분된 목록(*`0 2 4 … 98 100`*)으로 변환합니다. 변수 *`i`*는 첫 번째 반복에서 값 "0"을, 두 번째 반복에서 "1"을 할당받는 식으로 진행됩니다.
<3> 이 변수의 값은 달러 기호(*`$`*)를 접두사로 붙여 사용할 수 있습니다. 셸은 `echo`가 실행되기 전에 *`$i`*를 해당 값으로 바꿉니다. *`do`*와 *`done`* 사이에 둘 이상의 명령이 있을 수 있습니다.

구문이 즐겨 사용하는 프로그래밍 언어와 비교하여 약간 이상하게 보일 수 있지만 셸에서 항상 사용할 수 있으므로 기억할 가치가 있습니다.
잠시 후 명령을 반복하는 더 좋고 유연한 방법을 소개합니다.


### 줄에 대한 루핑

루프할 수 있는 두 번째 유형의 항목은 줄입니다.
이러한 줄은 파일이나 표준 입력에서 올 수 있습니다.
줄에는 숫자, 날짜, 이메일 주소 등 무엇이든 포함될 수 있으므로 매우 일반적인 접근 방식입니다.

모든 연락처에 이메일을 보내고 싶다고 상상해 보십시오.
먼저 무료 [Random User Generator API](https://randomuser.me)를 사용하여 가짜 사용자를 생성해 보겠습니다.

```{console emails}
curl -s "https://randomuser.me/api/1.2/?results=5&seed=dsatcl2e" > users.json
< users.json jq -r '.results[].email' > emails
bat emails
```

while 루프를 사용하여 *emails*의 줄을 반복할 수 있습니다.

```{console while_loop}
while read line                         #<1>
do
echo "Sending invitation to ${line}."   #<2>
done < emails                           #<3>
```
<1> 이 경우 Z 셸이 입력에 몇 줄이 포함되어 있는지 미리 알 수 없으므로 while 루프를 사용해야 합니다.
<2> 이 경우 *line* 변수 주위의 중괄호는 필요하지 않지만(변수 이름에 마침표를 포함할 수 없으므로) 여전히 좋은 습관입니다.
<3> 이 리디렉션은 `while` 앞에 배치할 수도 있습니다.

특수 파일 표준 입력 */dev/stdin*을 지정하여 while 루프에 대화식으로 입력을 제공할 수도 있습니다. 완료되면 **`Ctrl-D`**를 누릅니다.

```{console while_interactive}
while read line; do echo "You typed: ${line}."; done < /dev/stdin#! expect_prompt=FALSE
하나#! expect_prompt=FALSE
둘#! expect_prompt=FALSE
셋#! expect_prompt=FALSE
C-D#! literal=FALSE, expect_prompt=TRUE
```

그러나 이 방법은 **`Enter`** 키를 누르면 해당 입력 줄에 대해 _`do`_와 _`done`_ 사이의 명령이 즉시 실행된다는 단점이 있습니다. 되돌릴 수 없습니다.


### 파일에 대한 루핑

이 섹션에서는 자주 반복해야 하는 세 번째 유형의 항목인 파일에 대해 설명합니다.

특수 문자를 처리하려면 `ls`[@ls] 대신 *글로빙*(즉, 경로 이름 확장)을 사용하십시오.

```{console for}
for chapter in /data/*
do
echo "Processing Chapter ${chapter}."
done
```

중괄호 확장과 마찬가지로 _`/data/*`_ 표현식은 for 루프에서 처리되기 전에 먼저 Z 셸에 의해 목록으로 확장됩니다.

파일을 나열하는 더 정교한 대안은 다음과 같은 `find`[@find]입니다.

- 디렉터리를 아래로 탐색할 수 있습니다.
- 크기, 액세스 시간 및 권한과 같은 속성에 대한 정교한 검색을 허용합니다.
- 공백 및 줄 바꿈과 같은 특수 문자를 처리합니다.

예를 들어 다음 `find` 호출은 */data* 디렉터리 아래에 있고 확장자가 *csv*이고 크기가 2KB 미만인 모든 파일을 나열합니다.

```{console find_csv}
find /data -type f -name '*.csv' -size -2k
```


## 병렬 처리

다음과 같이 매우 오래 실행되는 도구가 있다고 가정해 보겠습니다.

```{console bat_slow, callouts=list("ts", "RANDOM", "sleep")}
bat slow.sh
```
<1> `ts`[@ts]는 타임스탬프를 추가합니다.
<2> 매직 변수 *`RANDOM`*은 0에서 32767 사이의 의사 난수 정수를 반환하는 내부 Bash 함수를 호출합니다. 해당 정수를 5로 나눈 나머지에 1을 더하면 *duration*이 1에서 5 사이가 됩니다.
<3> `sleep`은 지정된 시간(초) 동안 실행을 일시 중지합니다.

이 프로세스는 아마도 사용 가능한 모든 리소스를 차지하지 않을 것입니다.
그리고 이 명령을 여러 번 실행해야 하는 경우가 있습니다.
예를 들어 전체 파일 시퀀스를 다운로드해야 합니다.

병렬화하는 순진한 방법은 백그라운드에서 명령을 실행하는 것입니다.
`slow.sh`를 세 번 실행해 보겠습니다.

```{console subshell, keep_last_prompt=TRUE, callouts=list(2, 4)}
for i in {A..C}; do
./slow.sh $i &
done#! hold=7
```
<1> 앰퍼샌드(`&`)는 명령을 백그라운드로 보내고 for 루프가 즉시 다음 반복으로 계속되도록 합니다.
<2> 이 줄은 Z 셸에서 제공하는 작업 번호와 더 세분화된 작업 제어에 사용할 수 있는 프로세스 ID를 보여줍니다. 이 주제는 강력하지만 이 책의 범위를 벗어납니다.

```{block2, type="rmdnote"}
모든 것을 병렬화할 수 있는 것은 아닙니다.
API 호출은 특정 수로 제한될 수 있거나 일부 명령은 하나의 인스턴스만 가질 수 있습니다.
```

\@ref(fig:diagram-parallel-processing)은 개념적 수준에서 직렬 처리, 순진한 병렬 처리, GNU Parallel을 사용한 병렬 처리 간의 동시 프로세스 수와 모든 것을 실행하는 데 걸리는 총 시간 측면에서 차이점을 보여줍니다.

```{r diagram-parallel-processing, echo=FALSE, fig.cap="직렬 처리, 순진한 병렬 처리, GNU Parallel을 사용한 병렬 처리", fig.align="center", out.width="60%"}
knitr::include_graphics("images/dscl_0801.png")
```

이 순진한 접근 방식에는 두 가지 문제가 있습니다.
첫째, 동시에 실행 중인 프로세스 수를 제어할 방법이 없습니다.
너무 많은 작업을 한 번에 시작하면 CPU, 메모리, 디스크 액세스, 네트워크 대역폭과 같은 동일한 리소스를 놓고 경쟁할 수 있습니다.
이로 인해 모든 것을 실행하는 데 시간이 더 오래 걸릴 수 있습니다.
둘째, 어떤 출력이 어떤 입력에 속하는지 알기 어렵습니다.
더 나은 접근 방식을 살펴보겠습니다.


### GNU Parallel 소개

`parallel`을 소개합니다. 이는 명령과 파이프라인을 병렬화하고 배포할 수 있는 명령줄 도구입니다.
이 도구의 장점은 기존 도구를 그대로 사용할 수 있다는 것입니다. 수정할 필요가 없습니다.

```{block2, type="rmdcaution"}
`parallel`이라는 이름의 명령줄 도구가 두 개 있다는 점에 유의하십시오.
Docker 이미지를 사용하는 경우 이미 올바른 도구가 설치되어 있습니다.
그렇지 않으면 `parallel --version`을 실행하여 올바른 도구가 설치되었는지 확인할 수 있습니다.
"GNU parallel"이라고 표시되어야 합니다.
```

`parallel`의 세부 사항을 살펴보기 전에 위에서 언급한 for 루프를 병렬화하는 것이 얼마나 쉬운지 보여주는 간단한 예제를 살펴보겠습니다.

```{console teaser_2}
seq 0 2 100 | parallel "echo {}^2 | bc" | trim
```

이것은 `parallel`의 가장 간단한 형태입니다. 루프할 항목은 표준 입력을 통해 전달되고 `parallel`이 실행해야 하는 명령 외에는 인수가 없습니다.
`parallel`이 입력을 프로세스 간에 동시에 배포하고 출력을 수집하는 방법을 보여주는 그림은 \@ref(fig:diagram-parallel-output)을 참조하십시오.

```{r diagram-parallel-output, echo=FALSE, fig.cap="GNU Parallel은 입력을 프로세스 간에 동시에 배포하고 출력을 수집합니다.", fig.align="center"}
knitr::include_graphics("images/dscl_0802.png")
```

보시다시피 기본적으로 for 루프 역할을 합니다.
다음은 이전 섹션의 for 루프를 대체하는 또 다른 예제입니다.

```{console teaser_1}
parallel --jobs 2 ./slow.sh ::: {A..C}
```

여기서 `--jobs` 옵션을 사용하여 `parallel`이 동시에 최대 두 개의 작업을 실행할 수 있도록 지정합니다. `slow.sh`에 대한 인수는 표준 입력 대신 인수로 지정됩니다.

159개의 다양한 옵션을 사용하여 `parallel`은 많은 기능을 제공합니다.
(아마도 너무 많을 것입니다.)
다행히 효과적으로 사용하려면 몇 가지만 알면 됩니다.
덜 일반적인 옵션을 사용해야 하는 경우 매뉴얼 페이지가 매우 유용합니다.


### 입력 지정

`parallel`에 대한 가장 중요한 인수는 모든 입력에 대해 실행하려는 명령 또는 파이프라인입니다.
문제는 입력 항목을 명령줄의 어디에 삽입해야 하는가입니다.
아무것도 지정하지 않으면 입력 항목이 파이프라인 끝에 추가됩니다.

```{console, remove="keep"}
seq 3 | parallel cowsay#! enter=FALSE
C-C#! literal=FALSE
parallel --jobs 1 --keep-order cowsay ::: 1 2 3
```

위는 다음을 실행하는 것과 동일합니다.

```{console}
cowsay 1 > /dev/null #<1>
cowsay 2 > /dev/null
cowsay 3 > /dev/null
```
<1> 출력이 이전과 동일하므로 */dev/null*로 리디렉션하여 표시하지 않습니다.

이것은 종종 작동하지만 자리 표시자를 사용하여 입력 항목을 명령에 삽입할 위치를 명시적으로 지정하는 것이 좋습니다.
이 경우 전체 입력 줄(숫자)을 한 번에 사용하려고 하므로 하나의 자리 표시자만 필요합니다.
한 쌍의 중괄호(`{}`)를 사용하여 자리 표시자, 즉 입력 항목을 넣을 위치를 지정합니다.

```{console}
seq 3 | parallel cowsay {} > /dev/null
```


```{block2, type="rmdnote"}
`parallel`에 입력을 제공하는 다른 방법이 있습니다.
대부분의 명령줄 도구가 파이프라인으로 함께 연결되는 방식이므로 이 장 전체에서처럼 입력을 파이프하는 것을 선호합니다.
다른 방법에는 다른 곳에서는 볼 수 없는 구문이 포함됩니다.
그렇긴 하지만 여러 목록의 모든 가능한 조합을 반복하는 것과 같은 추가 기능을 사용할 수 있으므로 자세히 알아보려면 `parallel`의 매뉴얼 페이지를 읽어보십시오.
```

입력 항목이 파일 이름인 경우 파일 이름의 일부만 사용하는 데 사용할 수 있는 몇 가지 수정자가 있습니다.
예를 들어 `{/}`를 사용하면 파일 이름의 기본 이름만 사용됩니다.

```{console callouts=list(1)}
find /data/ch03 -type f | parallel echo '{#}\) \"{}\" has basename \"{/}\"'
```
<1> 괄호(`)`) 및 따옴표(`"`)와 같은 문자는 셸에서 특별한 의미를 갖습니다. 문자 그대로 사용하려면 앞에 백슬래시 `\`를 붙입니다. 이것을 *이스케이프*라고 합니다.

입력 줄에 구분 기호로 구분된 여러 부분이 있는 경우 자리 표시자에 숫자를 추가할 수 있습니다. 예를 들면 다음과 같습니다.

```{console remove=c(1, 2)}
touch input.csv
< input.csv parallel --colsep , "mv {2} {1}" > /dev/null#! enter=FALSE
C-C#! literal=FALSE
```

여기서는 동일한 자리 표시자 수정자를 적용할 수 있습니다.
동일한 입력 항목을 재사용할 수도 있습니다.
`parallel`에 대한 입력이 헤더가 있는 CSV 파일인 경우 열 이름을 자리 표시자로 사용할 수 있습니다.

```{console}
< input.csv parallel -C, --header : "invite {name} {email}"#! enter=FALSE
C-C#! literal=FALSE
```

```{block2, type="rmdtip"}
자리 표시자가 올바르게 설정되었는지 궁금한 경우 `--dryrun` 옵션을 추가할 수 있습니다.
실제로 명령을 실행하는 대신 `parallel`은 실행될 것처럼 모든 명령을 정확하게 인쇄합니다.
```


### 동시 작업 수 제어

기본적으로 parallel은 CPU 코어당 하나의 작업을 실행합니다.
`--jobs` 또는 `-j` 옵션을 사용하여 동시에 실행될 작업 수를 제어할 수 있습니다.
숫자를 지정하면 해당 수의 작업이 동시에 실행됩니다.
숫자 앞에 더하기 기호를 붙이면 `parallel`은 *N*개의 작업과 CPU 코어 수를 더한 만큼 실행합니다. 숫자 앞에 빼기 기호를 붙이면 parallel은 *N-M*개의 작업을 실행합니다.
여기서 *N*은 CPU 코어 수입니다.
백분율을 지정할 수도 있으며 기본값은 CPU 코어 수의 100%입니다.
동시에 실행할 최적의 작업 수는 실행 중인 실제 명령에 따라 다릅니다.

<!-- #TODO: Figure out where to type concurrently instead of parallel. And make sure all occurences of parallel are surrounded by backticks -->

```{console}
seq 5 | parallel -j0 "echo Hi {}"
```

```{console}
seq 5 | parallel -j200% "echo Hi {}"
```

`-j1`을 지정하면 명령이 직렬로 실행됩니다. 이것은 도구의 이름에 걸맞지 않지만 여전히 용도가 있습니다. 예를 들어 한 번에 하나의 연결만 허용하는 API에 액세스해야 하는 경우입니다. `-j0`을 지정하면 parallel은 가능한 한 많은 작업을 병렬로 실행합니다. 이것은 앰퍼샌드가 있는 루프와 비교할 수 있습니다. 이것은 권장되지 않습니다.


### 로깅 및 출력

각 명령의 출력을 저장하려면 다음을 수행하고 싶을 수 있습니다.

```{console}
seq 5 | parallel "echo \"Hi {}\" > hi-{}.txt"
```

이렇게 하면 출력이 개별 파일에 저장됩니다.
또는 모든 것을 하나의 큰 파일에 저장하려면 다음을 수행할 수 있습니다.

```{console}
seq 5 | parallel "echo Hi {}" >> one-big-file.txt
```

그러나 `parallel`은 각 작업의 출력을 별도의 파일에 저장하는 `--results` 옵션을 제공합니다.
각 작업에 대해 `parallel`은 세 개의 파일을 만듭니다. 작업 번호를 보유하는 *seq*, 작업에서 생성된 출력을 포함하는 *stdout*, 작업에서 생성된 모든 오류를 포함하는 *stderr*입니다.
이 세 파일은 입력 값을 기반으로 하는 하위 디렉터리에 배치됩니다.

`parallel`은 여전히 모든 출력을 인쇄하며, 이 경우 중복됩니다.
다음과 같이 표준 입력과 표준 출력을 모두 */dev/null*로 리디렉션할 수 있습니다.

```{console}
seq 10 | parallel --results outdir "curl 'https://anapioficeandfire.com/api/characters/{}' | jq -r '.aliases[0]'" 2>/dev/null 1>&2
tree outdir | trim
```

`--results` 옵션이 어떻게 작동하는지에 대한 그림 개요는 \@ref(fig:diagram-parallel-results)를 참조하십시오.

```{r diagram-parallel-results, echo=FALSE, fig.cap="GNU Parallel은 `--results` 옵션을 사용하여 출력을 별도의 파일에 저장합니다.", fig.align="center"}
knitr::include_graphics("images/dscl_0803.png")
```

여러 작업을 병렬로 실행하는 경우 작업이 실행되는 순서가 입력 순서와 일치하지 않을 수 있습니다.
따라서 작업 출력도 뒤섞입니다.
동일한 순서를 유지하려면 `--keep-order` 옵션 또는 `-k` 옵션을 지정하기만 하면 됩니다.

때로는 어떤 입력이 어떤 출력을 생성했는지 기록하는 것이 유용합니다.
`parallel`을 사용하면 `--tag` 옵션을 사용하여 출력에 *태그*를 지정할 수 있으며, 이는 각 줄 앞에 입력 항목을 추가합니다.

```{console tag}
seq 5 | parallel --tag "echo 'sqrt({})' | bc -l"
parallel --tag --keep-order "echo '{1}*{2}' | bc -l" ::: 3 4 ::: 5 6 7
```


### 병렬 도구 만들기

이 장의 시작 부분에서 사용한 `bc` 도구는 자체적으로 병렬이 아닙니다.
그러나 `parallel`을 사용하여 병렬화할 수 있습니다.
Docker 이미지에는 `pbc` [@pbc]라는 도구가 포함되어 있습니다.
해당 코드는 다음과 같습니다.

```{console bat_pbc}
bat $(which pbc)
```

이 도구를 사용하면 이 장의 시작 부분에서 사용한 코드를 단순화할 수 있습니다.
그리고 쉼표로 구분된 값을 동시에 처리할 수 있습니다.

```{console run_pbc}
seq 100 | pbc '{1}^2' | trim
paste -d, <(seq 4) <(seq 4) <(seq 4) | pbc 'sqrt({1}+{2})^{3}'
```


## 분산 처리

때로는 로컬 시스템이 모든 코어를 사용하더라도 제공할 수 있는 것보다 더 많은 성능이 필요합니다.
다행히 `parallel`은 원격 시스템의 성능도 활용할 수 있으므로 파이프라인 속도를 크게 높일 수 있습니다.

좋은 점은 `parallel`을 원격 시스템에 설치할 필요가 없다는 것입니다.
필요한 것은 *Secure Shell* 프로토콜(또는 SSH)을 통해 원격 시스템에 연결할 수 있다는 것뿐이며, `parallel`도 파이프라인을 배포하는 데 이를 사용합니다.
(`parallel`이 설치되어 있으면 각 원격 시스템에서 사용할 코어 수를 결정하는 데 도움이 될 수 있습니다. 이에 대해서는 나중에 자세히 설명합니다.)

<!-- #TODO: Introduce AWS EC2 -->
<!-- #TODO: COULD: Talk about Google Cloud and MS Azure -->

먼저 실행 중인 AWS EC2 인스턴스 목록을 가져옵니다.
원격 시스템이 없더라도 걱정하지 마십시오. `parallel`이 사용할 원격 시스템을 알려주는 `--slf hostnames`의 모든 항목을 `--sshlogin :`으로 바꿀 수 있습니다.
이렇게 하면 이 섹션의 예제를 계속 따라 할 수 있습니다.

어떤 원격 시스템을 사용할지 알게 되면 다음 세 가지 분산 처리 유형을 고려합니다.

- 원격 시스템에서 일반 명령을 실행
- 로컬 데이터를 원격 시스템 간에 직접 배포
- 원격 시스템으로 파일을 보내고 처리한 다음 결과를 검색


### 실행 중인 AWS EC2 인스턴스 목록 가져오기

<!-- #TODO: Add links to GCP And Azure -->

이 섹션에서는 줄당 원격 시스템의 호스트 이름 하나가 포함된 *hostnames*라는 파일을 만듭니다.
Amazon Web Services(AWS)를 예로 사용하고 있습니다.
AWS 계정이 있고 인스턴스를 시작하는 방법을 알고 있다고 가정합니다.
다른 클라우드 컴퓨팅 서비스(예: Google Cloud Platform 또는 Microsoft Azure)를 사용하거나 자체 서버가 있는 경우 다음 섹션으로 계속 진행하기 전에 직접 *hostnames* 파일을 만드십시오.

AWS API에 대한 명령줄 인터페이스인 `aws` [@aws]를 사용하여 실행 중인 AWS EC2 인스턴스 목록을 얻을 수 있습니다.
`aws`를 사용하면 온라인 AWS 관리 콘솔에서 할 수 있는 거의 모든 작업을 수행할 수 있습니다.

`aws ec2 describe-instances` 명령은 모든 EC2 인스턴스에 대한 많은 정보를 JSON 형식으로 반환합니다(자세한 내용은 [온라인 설명서](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/ec2/describe-instances.html) 참조).
`jq`를 사용하여 관련 필드를 추출할 수 있습니다.

```{console, remove=list(3, "echo")}
aws ec2 describe-instances | jq '.Reservations[].Instances[] | {public_dns: .PublicDnsName, state: .State.Name}'#! enter=FALSE
C-C#!literal=FALSE
echo '{' &&
echo '  "state": "실행 중",' &&
echo '  "public_dns": "ec2-54-88-122-140.compute-1.amazonaws.com"' &&
echo '}' &&
echo '{' &&
echo '  "state": "중지됨",' &&
echo '  "public_dns": null' &&
echo '}' &&
```

EC2 인스턴스의 가능한 상태는 *`pending`*, *`running`*, *`shutting-down`*, *`terminated`*, *`stopping`*, *`stopped`*입니다.
실행 중인 인스턴스에만 파이프라인을 배포할 수 있으므로 다음과 같이 실행 중이 아닌 인스턴스를 필터링합니다.

```{console, remove=list(3, "echo")}
aws ec2 describe-instances | jq -r '.Reservations[].Instances[] | select(.State.Name=="running") | .PublicDnsName' | tee hostnames#! enter=FALSE
C-C#! literal=FALSE
echo 'ec2-54-88-122-140.compute-1.amazonaws.com' &&
echo 'ec2-54-88-89-208.compute-1.amazonaws.com'
```

(`-r` 또는 `--raw-output` 옵션을 생략하면 호스트 이름이 큰따옴표로 묶입니다.)
나중에 `parallel`에 전달할 수 있도록 출력을 *hostnames*에 저장합니다.

언급했듯이 `parallel`은 `ssh`[@ssh]를 사용하여 원격 시스템에 연결합니다.
매번 자격 증명을 입력하지 않고 EC2 인스턴스에 연결하려면 *\~/.ssh/config* 파일에 다음과 같은 텍스트를 추가할 수 있습니다.

```{console, include=FALSE}
mkdir -p ~/.ssh
echo "Host *.amazonaws.com\n\tIdentityFile ~/.ssh/MyKeyFile.pem\n\tUser ubuntu" > ~/.ssh/config
```

```{console}
bat ~/.ssh/config
```

실행 중인 배포판에 따라 사용자 이름이 *`ubuntu`*와 다를 수 있습니다.


### 원격 시스템에서 명령 실행

분산 처리의 첫 번째 유형은 원격 시스템에서 일반 명령을 실행하는 것입니다.
먼저 각 EC2 인스턴스에서 `hostname`[@hostname] 도구를 실행하여 `parallel`이 작동하는지 다시 확인해 보겠습니다.

```{console, remove=list(2, "echo")}
parallel --nonall --sshloginfile hostnames hostname#! enter=FALSE
C-C#! literal=FALSE
echo 'ip-172-31-23-204\nip-172-31-23-205'
```

여기서 `--sshloginfile` 또는 `--slf` 옵션은 *hostnames* 파일을 참조하는 데 사용됩니다.
`--nonall` 옵션은 `parallel`에게 매개변수 없이 *hostnames* 파일의 모든 원격 시스템에서 동일한 명령을 실행하도록 지시합니다.
원격 시스템을 활용할 수 없는 경우 `--slf hostnames`를 `--sshlogin :`으로 바꾸면 명령이 로컬 시스템에서 실행됩니다.

```{console, remove=list(2, "echo")}
parallel --nonall --sshlogin : hostname#! enter=FALSE
C-C#! literal=FALSE
echo 'data-science-toolbox'
```

모든 원격 시스템에서 동일한 명령을 한 번 실행하는 데는 시스템당 하나의 코어만 필요합니다. `parallel`에 전달된 인수 목록을 배포하려는 경우 잠재적으로 둘 이상의 코어를 사용할 수 있습니다. 코어 수가 명시적으로 지정되지 않은 경우 `parallel`은 이를 결정하려고 시도합니다.

```{console, remove=list(2, 4, "fake")}
alias fake=echo
seq 2 | parallel --slf hostnames echo 2>&1#! enter=FALSE
C-C#! literal=FALSE
fake 'bash: parallel: 명령을 찾을 수 없습니다' &&
fake -n 'parallel: 경고: ec2-54-88-122-140.compute-1.amazonaws.com에서 CPU 수를 파악할 수 없습니다' &&
fake ' (). 1을 사용합니다.' &&
fake '1' &&
fake '2'
```

이 경우 두 원격 시스템 중 하나에 `parallel`이 설치되어 있습니다.
그중 하나에서 `parallel`을 찾을 수 없다는 경고 메시지가 표시됩니다.
결과적으로 `parallel`은 코어 수를 결정할 수 없으며 기본적으로 하나의 코어를 사용합니다.
이 경고 메시지가 표시되면 다음 네 가지 중 하나를 수행할 수 있습니다.

- 걱정하지 말고 시스템당 하나의 코어를 사용하는 것에 만족하십시오.
- `--jobs` 또는 `-j` 옵션을 통해 각 시스템에 대한 작업 수를 지정하십시오.
- 예를 들어 두 개의 코어를 원하면 *2/*를 *hostnames* 파일의 각 호스트 이름 앞에 넣어 시스템당 사용할 코어 수를 지정하십시오.
- 패키지 관리자를 사용하여 `parallel`을 설치하십시오. 예를 들어 원격 시스템이 모두 Ubuntu를 실행하는 경우 다음과 같습니다.

```{console}
parallel --nonall --slf hostnames "sudo apt-get install -y parallel"#! enter=FALSE
C-C#! literal=FALSE
```


### 원격 시스템 간 로컬 데이터 배포

분산 처리의 두 번째 유형은 원격 시스템 간에 로컬 데이터를 직접 배포하는 것입니다.
여러 원격 시스템을 사용하여 처리하려는 매우 큰 데이터 세트가 있다고 상상해 보십시오.
간단하게 하기 위해 1에서 1000까지의 모든 정수를 합산합니다.
먼저 원격 시스템의 호스트 이름과 `wc`를 사용하여 수신한 입력 길이를 인쇄하여 입력이 실제로 배포되는지 다시 확인해 보겠습니다.

```{console, remove=list(3, "echo")}
seq 1000 | parallel -N100 --pipe --slf hostnames "(hostname; wc -l) | paste -sd:"#! enter=FALSE
C-C#! literal=FALSE
echo 'ip-172-31-23-204:100' &&
echo 'ip-172-31-23-205:100' &&
echo 'ip-172-31-23-205:100' &&
echo 'ip-172-31-23-204:100' &&
echo 'ip-172-31-23-205:100' &&
echo 'ip-172-31-23-204:100' &&
echo 'ip-172-31-23-205:100' &&
echo 'ip-172-31-23-204:100' &&
echo 'ip-172-31-23-205:100' &&
echo 'ip-172-31-23-204:100'
```

훌륭합니다. 1000개의 숫자가 100개 하위 집합(`-N100`으로 지정됨)으로 균등하게 배포되는 것을 볼 수 있습니다.
이제 모든 숫자를 합산할 준비가 되었습니다.

```{console, remove=list(2, "echo")}
seq 1000 | parallel -N100 --pipe --slf hostnames "paste -sd+ | bc" | paste -sd+ | bc#! enter=FALSE
C-C#! literal=FALSE
echo '500500'
```

여기서는 원격 시스템에서 받은 10개의 합계도 즉시 합산합니다.
`parallel` 없이 동일한 계산을 수행하여 답이 올바른지 확인해 보겠습니다.

```{console}
seq 1000 | paste -sd+ | bc
```

좋습니다. 작동합니다.
원격 시스템에서 실행하려는 더 큰 파이프라인이 있는 경우 별도의 스크립트에 넣고 `parallel`을 사용하여 업로드할 수도 있습니다.
`add`라는 매우 간단한 명령줄 도구를 만들어 이를 보여 드리겠습니다.

```{console create_add, marker="#~"}
echo '#!/usr/bin/env bash' > add
echo 'paste -sd+ | bc' >> add
bat add
chmod u+x add
seq 1000 | ./add
```

`--basefile` 옵션을 사용하면 `parallel`이 작업을 실행하기 전에 먼저 *add* 파일을 모든 원격 시스템에 업로드합니다.

<!-- #TODO: Explain --pipe -->

```{console use_add, remove=list(4, "echo")}
seq 1000 |
parallel -N100 --basefile add --pipe --slf hostnames './add' |
./add #! enter=FALSE
C-C#! literal=FALSE
echo '500500'
```

1000개의 숫자를 합산하는 것은 물론 장난감 예일뿐입니다.
또한 로컬에서 수행하는 것이 훨씬 빨랐을 것입니다.
그래도 이 예에서 `parallel`이 얼마나 강력할 수 있는지 분명히 알 수 있기를 바랍니다.


### 원격 시스템에서 파일 처리

분산 처리의 세 번째 유형은 원격 시스템으로 파일을 보내고 처리한 다음 결과를 검색하는 것입니다.
뉴욕시의 각 자치구에 대해 311에 서비스 요청이 얼마나 자주 접수되는지 계산하고 싶다고 상상해 보십시오.
아직 로컬 시스템에 해당 데이터가 없으므로 먼저 무료 [NYC Open Data API](https://data.cityofnewyork.us/)에서 가져오겠습니다.

```{console nyc}
seq 0 100 900 | parallel  "curl -sL 'http://data.cityofnewyork.us/resource/erm2-nwe9.json?\$limit=100&\$offset={}' | jq -c '.[]' | gzip > nyc-{#}.json.gz"
```

이제 압축된 JSON 데이터가 포함된 10개의 파일이 있습니다.

```{console nyc_ls}
l nyc*json.gz
```

`jq -c '.[]'`는 JSON 객체 배열을 평면화하여 파일당 총 100줄, 줄당 하나의 객체로 만드는 데 사용됩니다.
`zcat` [@zcat]을 사용하면 압축 파일의 내용을 직접 인쇄할 수 있습니다.

```{console zcat_trim}
zcat nyc-1.json.gz | trim
```

JSON 한 줄이 어떻게 생겼는지 살펴보겠습니다.

```{console zcat_head_1}
zcat nyc-1.json.gz | head -n 1
```

로컬 시스템에서 자치구당 총 서비스 요청 수를 가져오려면 다음 명령을 실행합니다.

```{console, callouts=1:6}
zcat nyc*json.gz |
jq -r '.borough' |
tr '[A-Z] ' '[a-z]_' |
sort | uniq -c | sort -nr |
awk '{print $2","$1}' |
header -a borough,count |
csvlook
```
<1> `zcat`을 사용하여 모든 압축 파일을 확장합니다.
<2> 각 호출에 대해 `jq`를 사용하여 자치구 이름을 추출합니다.
<3> 자치구 이름을 소문자로 변환하고 공백을 밑줄로 바꿉니다(`awk`는 기본적으로 공백을 기준으로 분할하므로).
<4> `sort` 및 `uniq`를 사용하여 각 자치구의 발생 횟수를 계산합니다.
<5> 두 열을 반대로 하고 `awk`를 사용하여 쉼표로 구분합니다.
<6> `header`를 사용하여 헤더를 추가합니다.

잠시 동안 우리 자신의 시스템이 너무 느려서 이 파이프라인을 로컬에서 수행할 수 없다고 상상해 보십시오.
`parallel`을 사용하여 로컬 파일을 원격 시스템 간에 배포하고 처리하도록 한 다음 결과를 검색할 수 있습니다.

```{console, callouts=c(1, 2, 3, 4, 6)}
ls *.json.gz |
parallel -v --basefile jq \
--trc {.}.csv \
--slf hostnames \
"zcat {} | ./jq -r '.borough' | tr '[A-Z] ' '[a-z]_' | sort | uniq -c | awk '{print \$2\",\"\$1}' > {.}.csv"#! enter=FALSE
C-C#! literal=FALSE
```
<1> 파일 목록을 인쇄하고 `parallel`로 파이프합니다.
<2> `jq` 바이너리를 각 원격 시스템으로 전송합니다. 다행히 `jq`에는 종속성이 없습니다. `--trc` 옵션(이는 `--cleanup` 옵션을 의미함)을 지정했으므로 이 파일은 나중에 원격 시스템에서 제거됩니다. 파이프라인이 `jq` 대신 `./jq`를 사용한다는 점에 유의하십시오. 파이프라인은 업로드된 버전을 사용해야 하며 검색 경로에 있을 수도 있고 없을 수도 있는 버전을 사용해서는 안 되기 때문입니다.
<3> 명령줄 인수 `--trc {.}.csv`는 `--transfer --return {.}.csv --cleanup`의 약자입니다. (대체 문자열 *`{.}`*은 마지막 확장자가 없는 입력 파일 이름으로 바뀝니다.) 여기서 이는 JSON 파일이 원격 시스템으로 전송되고 CSV 파일이 로컬 시스템으로 반환되며 각 작업 후 두 파일 모두 원격 시스템에서 제거됨을 의미합니다.
<4> 호스트 이름 목록을 지정합니다. 로컬에서 시도하려면 `--slf hostnames` 대신 `--sshlogin :`을 지정할 수 있다는 것을 기억하십시오.
<5> `awk` 표현식의 이스케이프 처리에 유의하십시오. 따옴표 처리는 때때로 까다로울 수 있습니다. 여기서 달러 기호와 큰따옴표는 이스케이프 처리됩니다. 따옴표 처리가 너무 혼란스러워지면 `add`와 같이 파이프라인을 별도의 명령줄 도구에 넣을 수 있다는 것을 기억하십시오.

```{console, include=FALSE}
ls *.json.gz |
parallel -v --basefile jq \
--trc {.}.csv \
--sshlogin : \
"zcat {} | jq -r '.borough' | tr '[A-Z] ' '[a-z]_' | sort | uniq -c | awk '{print \$2\",\"\$1}' > {.}.csv"
```

이 프로세스 중에 원격 시스템 중 하나에서 `ls`를 실행하면 `parallel`이 실제로 바이너리 `jq`, JSON 파일 및 CSV 파일을 전송(및 정리)하는 것을 볼 수 있습니다.

```{console, remove=list(2, "echo")}
ssh $(head -n 1 hostnames) ls#! enter=FALSE
C-C#! literal=FALSE
echo 'nyc-1.json.csv' &&
echo 'nyc-1.json.gz' &&
echo 'jq' &&
```

각 CSV 파일은 다음과 같습니다.

```{console, remove=list(2, "echo")}
cat nyc-1.json.csv #! enter=FALSE
C-C#! literal=FALSE
echo 'bronx,3' &&
echo 'brooklyn,5' &&
echo 'manhattan,24' &&
echo 'queens,3' &&
echo 'staten_island,2'
```

`rush`[@rush]와 tidyverse를 사용하여 각 CSV 파일의 개수를 합산할 수 있습니다.

```{console}
cat nyc*csv | header -a borough,count |
rush run -t 'group_by(df, borough) %>% summarize(count = sum(count))' - |
csvsort -rc count | csvlook
```

또는 결과를 집계하기 위해 SQL을 선호하는 경우 [5장](#chapter-5-scrubbing-data)에서 설명한 대로 `csvsql`을 사용할 수 있습니다.

```{console}
cat nyc*csv | header -a borough,count |
csvsql --query 'SELECT borough, SUM(count) AS count FROM stdin GROUP BY borough ORDER BY count DESC' |
csvlook
```


## 요약

데이터 과학자로서 여러분은 데이터를 다루며 때로는 많은 양의 데이터를 다룹니다.
즉, 때로는 명령을 여러 번 실행하거나 데이터 집약적인 명령을 여러 코어에 분산해야 합니다.
이 장에서는 명령을 병렬화하는 것이 얼마나 쉬운지 보여주었습니다.
`parallel`은 일반적인 명령줄 도구의 속도를 높이고 배포하는 매우 강력하고 유연한 도구입니다.
많은 기능을 제공하며 이 장에서는 겨우 표면만 긁었을 뿐입니다.
다음 장에서는 OSEMN 모델의 네 번째 단계인 데이터 모델링을 다룰 것입니다.


## 추가 탐색을 위해

- `parallel`과 가장 중요한 옵션에 대한 기본적인 이해가 되면 [온라인 자습서](https://www.gnu.org/software/parallel/parallel_tutorial.html)를 살펴보는 것이 좋습니다. 다른 것들 중에서 입력을 지정하는 다양한 방법, 모든 작업의 로그를 유지하는 방법, 작업 시간 초과, 재개 및 재시도 방법을 배울 수 있습니다. `parallel`의 제작자인 Ole Tange가 이 자습서에서 말했듯이 "명령줄이 당신을 사랑하게 될 것입니다."
