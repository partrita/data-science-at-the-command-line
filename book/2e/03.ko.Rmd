---
suppress-bibliography: true
---

```{r console_start, include=FALSE}
console_start()
```

```{console setup_history, include=FALSE}
 export CHAPTER="03"
 export HISTFILE=/history/history_${CHAPTER}
 rm -f $HISTFILE
```

```{console install_pup_arm64, include=FALSE}
curl -sL https://github.com/ericchiang/pup/releases/download/v0.4.0/pup_v0.4.0_linux_arm64.zip -o pup.zip
unzip pup.zip
sudo mv pup /usr/bin/
rm pup.zip
```

# 데이터 획득 {#chapter-3-obtaining-data}

이 장에서는 OSEMN 모델의 첫 번째 단계인 데이터 획득을 다룹니다.
결국 데이터가 없으면 수행할 수 있는 데이터 과학이 많지 않습니다.
데이터 과학 문제를 해결하는 데 필요한 데이터가 이미 존재한다고 가정합니다.
첫 번째 작업은 이 데이터를 컴퓨터(그리고 가능하면 Docker 컨테이너 내부)로 가져와 작업할 수 있는 형태로 만드는 것입니다.

유닉스 철학에 따르면 텍스트는 보편적인 인터페이스입니다.
거의 모든 명령줄 도구는 텍스트를 입력으로 사용하거나 텍스트를 출력으로 생성하거나 둘 다 수행합니다.
이것이 명령줄 도구가 함께 잘 작동할 수 있는 주된 이유입니다.
그러나 보게 되겠지만 텍스트조차도 여러 형태로 나타날 수 있습니다.

데이터는 서버에서 다운로드하거나 데이터베이스를 쿼리하거나 웹 API에 연결하는 등 여러 가지 방법으로 얻을 수 있습니다.
때로는 데이터가 압축된 형태나 Microsoft Excel 스프레드시트와 같은 이진 형식으로 제공됩니다.
이 장에서는 `curl`[@curl], `in2csv`[@in2csv], `sql2csv`[@sql2csv], `tar`[@tar]를 포함하여 명령줄에서 이 문제를 해결하는 데 도움이 되는 몇 가지 도구에 대해 설명합니다.


## 개요

이 장에서는 다음을 수행하는 방법을 배웁니다.

- 로컬 파일을 Docker 이미지에 복사
- 인터넷에서 데이터 다운로드
- 파일 압축 해제
- 스프레드시트에서 데이터 추출
- 관계형 데이터베이스 쿼리
- 웹 API 호출

이 장은 다음 파일로 시작합니다.

```{console cd}
cd /data/ch03
l
```

이러한 파일을 가져오는 지침은 [2장](#chapter-2-getting-started)에 있습니다.
다른 모든 파일은 명령줄 도구를 사용하여 다운로드하거나 생성됩니다.


## 로컬 파일을 Docker 컨테이너에 복사

필요한 파일이 이미 컴퓨터에 있는 경우가 일반적입니다.
이 섹션에서는 해당 파일을 Docker 컨테이너에 가져오는 방법을 설명합니다.

[2장](#chapter-2-getting-started)에서 Docker 컨테이너가 격리된 가상 환경이라고 언급했습니다.
다행히 한 가지 예외가 있습니다. 파일을 Docker 컨테이너 안팎으로 전송할 수 있습니다.
`docker run`을 실행한 로컬 디렉터리는 Docker 컨테이너의 디렉터리에 매핑됩니다.
이 디렉터리는 */data*라고 합니다.
이것은 홈 디렉터리인 */home/dst*가 아닙니다.

로컬 컴퓨터에 하나 이상의 파일이 있고 일부 명령줄 도구를 적용하려면 해당 매핑된 디렉터리로 파일을 복사하거나 이동하기만 하면 됩니다.
다운로드 디렉터리에 *logs.csv*라는 파일이 있다고 가정해 보겠습니다.

Windows를 실행하는 경우 명령 프롬프트 또는 PowerShell을 열고 다음 두 명령을 실행합니다.

```powershell
> cd %UserProfile%\Downloads
> copy logs.csv MyDataScienceToolbox\
```

Linux 또는 macOS를 실행하는 경우 터미널을 열고 운영 체제에서 다음 명령을 실행합니다(Docker 컨테이너 내부가 아님).

```{console cp}
cp ~/Downloads/logs.csv ~/my-data-science-toolbox#! enter=FALSE
```

```{console cp_cancel, include=FALSE}
C-C#! literal=FALSE
```

Windows 탐색기 또는 macOS Finder와 같은 그래픽 파일 관리자를 사용하여 파일을 올바른 디렉터리로 끌어다 놓을 수도 있습니다.


## 인터넷에서 다운로드

인터넷은 의심할 여지 없이 흥미로운 데이터의 가장 큰 리소스입니다.
명령줄 도구 `curl`은 인터넷에서 데이터를 다운로드하는 데 있어 명령줄의 스위스 군용 칼로 간주될 수 있습니다.


### `curl` 소개

URL(*uniform resource locator*의 약자)로 이동하면 브라우저가 다운로드한 데이터를 해석합니다.
예를 들어 브라우저는 HTML 파일을 렌더링하고 비디오 파일을 자동으로 재생하며 PDF 파일을 표시합니다.
그러나 `curl`을 사용하여 URL에 액세스하면 데이터를 다운로드하고 기본적으로 표준 출력으로 인쇄합니다.
`curl`은 아무런 해석도 하지 않지만 다행히 다른 명령줄 도구를 사용하여 데이터를 추가로 처리할 수 있습니다.

`curl`의 가장 쉬운 호출은 URL을 명령줄 인수로 지정하는 것입니다.
위키백과에서 기사를 다운로드해 보겠습니다.

```{console curl_simple, callouts=2}
curl "https://en.wikipedia.org/wiki/List_of_windmills_in_the_Netherlands" |
trim
```
<1> `trim`은 출력을 책에 보기 좋게 맞추기 위해서만 사용됩니다.

보시다시피 `curl`은 위키백과 서버에서 반환된 원시 HTML을 다운로드합니다. 해석은 수행되지 않으며 모든 내용이 즉시 표준 출력에 인쇄됩니다.
URL 때문에 이 기사가 네덜란드의 모든 풍차를 나열할 것이라고 생각할 수 있습니다.
그러나 분명히 풍차가 너무 많이 남아 있어서 각 주마다 자체 페이지가 있습니다.
흥미롭네요.

기본적으로 `curl`은 다운로드 속도와 예상 완료 시간을 보여주는 진행률 표시기를 출력합니다.
이 출력은 표준 출력으로 기록되지 않고 *표준 오류*라는 별도의 채널로 기록되므로 파이프라인에 다른 도구를 추가해도 방해받지 않습니다.
이 정보는 매우 큰 파일을 다운로드할 때 유용할 수 있지만 일반적으로 방해가 되므로 `-s` 옵션을 지정하여 이 출력을 *자동*으로 만듭니다.

```{console curl_silent}
curl -s "https://en.wikipedia.org/wiki/List_of_windmills_in_Friesland" |
pup -n 'table.wikitable tr' # <1>
```
<1> 웹사이트 스크래핑에 유용한 도구인 `pup`[@pup]에 대해서는 [5장](#chapter-5-scrubbing-data)에서 자세히 설명합니다.

그리고 아시다시피 프리슬란트 주에만 234개의 풍차가 있습니다!


### 저장

`-O` 옵션을 추가하여 `curl`이 출력을 파일에 저장하도록 할 수 있습니다.
파일 이름은 URL의 마지막 부분을 기반으로 합니다.

```{console curl_save}
curl -s "https://en.wikipedia.org/wiki/List_of_windmills_in_Friesland" -O
l
```

해당 파일 이름이 마음에 들지 않으면 `-o` 옵션과 파일 이름을 함께 사용하거나 출력을 직접 파일로 리디렉션할 수 있습니다.

```{console curl_redirect}
curl -s "https://en.wikipedia.org/wiki/List_of_windmills_in_Friesland" > friesland.html
```


### 기타 프로토콜

총 `curl`은 [20개 이상의 프로토콜](https://ec.haxx.se/protocols/protocols-curl)을 지원합니다.
파일 전송 프로토콜을 의미하는 FTP 서버에서 다운로드하려면 `curl`을 동일한 방식으로 사용합니다.
여기서는 *ftp.gnu.org*에서 *welcome.msg* 파일을 다운로드합니다.

```{console curl_ftp}
curl -s "ftp://ftp.gnu.org/welcome.msg" | trim
```

지정된 URL이 디렉터리인 경우 `curl`은 해당 디렉터리의 내용을 나열합니다.
URL이 암호로 보호된 경우 `-u` 옵션을 사용하여 다음과 같이 사용자 이름과 암호를 지정할 수 있습니다.

또는 다양한 사전에 액세스하고 정의를 조회할 수 있는 DICT 프로토콜은 어떻습니까?
다음은 협업 국제 영어 사전에 따른 "windmill"의 정의입니다.

```{console curl_dict}
curl -s "dict://dict.org/d:windmill" | trim
```

그러나 인터넷에서 데이터를 다운로드할 때 프로토콜은 HTTP일 가능성이 가장 높으므로 URL은 *http://* 또는 *https://*로 시작합니다.


### 리디렉션 따르기

*http://bit.ly/* 또는 *http://t.co/*로 시작하는 것과 같은 단축 URL에 액세스하면 브라우저가 자동으로 올바른 위치로 리디렉션합니다.
그러나 `curl`을 사용하면 리디렉션되려면 `-L` 또는 `--location` 옵션을 지정해야 합니다.
그렇지 않으면 다음과 같은 결과가 나타날 수 있습니다.

```{console curl_bitly}
curl -s "https://bit.ly/2XBxvwK"
```

때로는 위에서 언급한 URL을 따를 때처럼 아무것도 반환되지 않습니다.

```{console curl_youtube}
curl -s "https://youtu.be/dQw4w9WgXcQ"
```

`-I` 또는 `--head` 옵션을 지정하면 `curl`은 응답의 HTTP 헤더만 가져오므로 서버에서 반환된 상태 코드 및 기타 정보를 검사할 수 있습니다.

```{console curl_head}
curl -sI "https://youtu.be/dQw4w9WgXcQ" | trim
```

첫 번째 줄은 프로토콜과 HTTP 상태 코드를 보여주며 이 경우 303입니다.
이 URL이 리디렉션되는 위치도 볼 수 있습니다.
헤더를 검사하고 상태 코드를 가져오는 것은 `curl`이 예상한 결과를 제공하지 않는 경우 유용한 디버깅 도구입니다.
다른 일반적인 HTTP 상태 코드에는 404(찾을 수 없음) 및 403(금지됨)이 포함됩니다.
위키백과에는 [모든 HTTP 상태 코드](http://en.wikipedia.org/wiki/List_of_HTTP_status_codes)를 나열하는 페이지가 있습니다.

요약하자면 `curl`은 인터넷에서 데이터를 다운로드하기 위한 유용한 명령줄 도구입니다.
가장 일반적인 세 가지 옵션은 진행률 표시기를 자동 모드로 설정하는 `-s`, 사용자 이름과 암호를 지정하는 `-u`, 자동으로 리디렉션을 따르는 `-L`입니다.
자세한 내용(그리고 머리를 복잡하게 만드는 내용)은 해당 man 페이지를 참조하십시오.

```{console curl_man}
man curl | trim 20
```


## 파일 압축 해제

원본 데이터 세트가 매우 크거나 많은 파일 모음인 경우 파일은 압축된 아카이브일 수 있습니다.
반복되는 값이 많은 데이터 세트(예: 텍스트 파일의 단어 또는 JSON 파일의 키)는 특히 압축에 적합합니다.

압축 아카이브의 일반적인 파일 확장자는 *.tar.gz*, *.zip*, *.rar*입니다.
이러한 파일의 압축을 풀려면 각각 명령줄 도구 `tar`, `unzip` [@unzip], `unrar` [@unrar]를 사용합니다.
(흔하지는 않지만 다른 도구가 필요한 파일 확장자가 몇 개 더 있습니다.)

예를 들어 *tar.gz*("gzipped tarball"로 발음)를 살펴보겠습니다.
*logs.tar.gz*라는 아카이브의 압축을 풀려면 다음 명령을 사용합니다.

```{console tar_x}
tar -xzf logs.tar.gz # <1> #! enter=FALSE
```
<1> 여기서처럼 이 세 가지 짧은 옵션을 결합하는 것이 일반적이지만 `-x -z -f`와 같이 개별적으로 지정할 수도 있습니다.
실제로 많은 명령줄 도구에서 단일 문자로 구성된 옵션을 결합할 수 있습니다.

실제로 `tar`는 많은 명령줄 인수로 악명이 높습니다.
이 경우 세 가지 옵션 `-x`, `-z`, `-f`는 `tar`가 아카이브에서 파일을 *추출*하고, 압축 해제 알고리즘으로 *gzip*을 사용하고, 파일 *logs.tar.gz*를 사용하도록 지정합니다.

```{console tar_cancel, include=FALSE}
C-C #! literal=FALSE
```

그러나 이 아카이브에 아직 익숙하지 않으므로 먼저 내용을 검사하는 것이 좋습니다.
이는 `-x` 옵션 대신 `-t` 옵션을 사용하여 수행할 수 있습니다.

```{console tar_t}
tar -tzf logs.tar.gz | trim
```

이 아카이브에는 많은 파일이 포함되어 있고 디렉터리 내부에 있지 않은 것 같습니다.
현재 디렉터리를 깨끗하게 유지하려면 먼저 `mkdir`을 사용하여 새 디렉터리를 만들고 `-C` 옵션을 사용하여 해당 파일을 거기에 추출하는 것이 좋습니다.

```{console tar_mkdir}
mkdir logs
tar -xzf logs.tar.gz -C logs
```

파일 수와 일부 내용을 확인해 보겠습니다.

```{console tar_verify}
ls logs | wc -l
cat logs/* | trim
```

훌륭합니다.
이제 이 로그 파일을 정제하고 탐색하고 싶다는 것을 이해하지만 이는 [5장](#chapter-5-scrubbing-data)과 [7장](#chapter-7-exploring-data)에서 나중에 다룰 내용입니다.

시간이 지나면 이러한 옵션에 익숙해지겠지만 편리할 수 있는 대안 옵션을 보여 드리고자 합니다.
다양한 명령줄 도구와 해당 옵션을 기억하는 대신 다양한 형식을 압축 해제하는 `unpack` [@unpack]이라는 편리한 스크립트가 있습니다.
`unpack`은 압축을 풀려는 파일의 확장자를 보고 적절한 명령줄 도구를 호출합니다.
이제 이 동일한 파일의 압축을 풀려면 다음을 실행합니다.

```{console unpack}
unpack logs.tar.gz #! enter=FALSE
```

```{console unpack_cancel, include=FALSE}
C-C #! literal=FALSE
```


## Microsoft Excel 스프레드시트를 CSV로 변환

많은 사람들에게 Microsoft Excel은 작은 데이터 세트로 작업하고 계산을 수행하는 직관적인 방법을 제공합니다.
결과적으로 많은 데이터가 Microsoft Excel 스프레드시트에 포함되어 있습니다.
이러한 스프레드시트는 파일 이름의 확장자에 따라 독점적인 이진 형식(*.xls*) 또는 압축된 XML 파일 모음(*.xlsx*)으로 저장됩니다.
두 경우 모두 대부분의 명령줄 도구에서 데이터를 즉시 사용할 수 없습니다.
이러한 귀중한 데이터 세트가 이러한 방식으로 저장되어 있기 때문에 사용할 수 없다면 안타까울 것입니다.

특히 명령줄을 처음 사용하는 경우 Microsoft Excel 또는 LibreOffice Calc와 같은 오픈 소스 변형에서 스프레드시트를 열고 수동으로 CSV로 내보내어 스프레드시트를 CSV로 변환하고 싶을 수 있습니다.
이것은 일회성 솔루션으로 작동하지만 여러 파일로 확장하기 어렵고 자동화할 수 없다는 단점이 있습니다.
또한 서버에서 작업하는 경우 이러한 응용 프로그램을 사용할 수 없을 가능성이 있습니다.
저를 믿으십시오. 익숙해질 것입니다.

다행히 Microsoft Excel 스프레드시트를 CSV 파일로 변환하는 `in2csv`라는 명령줄 도구가 있습니다.
CSV는 쉼표로 구분된 값을 의미합니다.
CSV는 공식적인 사양이 없기 때문에 작업하기 까다로울 수 있습니다.
<!-- TODO: MUST: check and possibly fix reference? -->
Yakov Shafranovich는 다음 세 가지 사항에 따라 CSV 형식을 정의합니다.[@shafranovich2005common]

1.  각 레코드는 줄 바꿈(`␊`)으로 구분된 별도의 줄에 있습니다. 예를 들어 십대 돌연변이 닌자 거북이에 대한 중요한 정보가 포함된 다음 CSV 파일을 살펴보십시오.

```{console csv_1}
bat -A tmnt-basic.csv # <1>
```
<1> `-A` 옵션을 사용하면 `bat`이 공백, 탭 및 줄 바꿈과 같은 인쇄할 수 없는 모든 문자를 표시합니다.

2.  파일의 마지막 레코드에는 끝 줄 바꿈이 있을 수도 있고 없을 수도 있습니다. 예를 들면 다음과 같습니다.

```{console csv_2}
bat -A tmnt-missing-newline.csv
```

3.  파일의 첫 번째 줄에 일반 레코드 줄과 동일한 형식의 헤더가 있을 수 있습니다. 이 헤더에는 파일의 필드에 해당하는 이름이 포함되며 파일의 나머지 레코드와 동일한 수의 필드를 포함해야 합니다. 예를 들면 다음과 같습니다.

```{console csv_3}
bat -A tmnt-with-header.csv
```

보시다시피 CSV는 기본적으로 가독성이 좋지 않습니다.
데이터를 `csvlook` [@csvlook]이라는 도구로 파이프하면 보기 좋게 테이블 형식으로 지정합니다.
CSV 데이터에 *tmnt-missing-newline.csv*와 같이 헤더가 없는 경우 `-H` 옵션을 추가해야 합니다. 그렇지 않으면 첫 번째 줄이 헤더로 해석됩니다.

```{console csvlook}
csvlook tmnt-with-header.csv
csvlook tmnt-basic.csv
csvlook -H tmnt-missing-newline.csv # <1>
```
<1> `-H` 옵션은 CSV 파일에 헤더가 없음을 지정합니다.

연간 네덜란드 마라톤 라디오 프로그램 [Top 2000](https://www.top2000nl.com)에 따른 2000개의 가장 인기 있는 노래가 포함된 스프레드시트를 사용하여 `in2csv`를 시연해 보겠습니다.
데이터를 추출하려면 다음과 같이 `in2csv`를 호출합니다.

```{console in2csv_save, remove=c("openpyxl", "silence", "behavior", "numpy", "guidance", "release")}
curl "https://www.nporadio2.nl/data/download/TOP-2000-2020.xlsx" > top2000.xlsx
in2csv top2000.xlsx | tee top2000.csv | trim
```

대니 베라는 누구인가요? 가장 인기 있는 노래는 당연히 *보헤미안 랩소디*여야 합니다.
음, 적어도 퀸은 Top 2000에 여러 번 등장하므로 불평할 수는 없습니다.

```{console cvsgrep, callouts=1}
csvgrep top2000.csv --columns ARTIEST --regex '^Queen$' | csvlook -I
```
<1> `--regex` 옵션 뒤의 값은 정규 표현식(또는 *regex*)입니다. 패턴을 정의하기 위한 특수 구문입니다. 여기서는 "Queen"과 정확히 일치하는 아티스트만 일치시키고 싶으므로 캐럿(`^`)과 달러 기호(`$`)를 사용하여 *`ARTIEST`* 열 값의 시작과 끝을 일치시킵니다.

참고로 `in2csv`, `csvgrep`, `csvlook` 도구는 CSV 데이터로 작업하기 위한 명령줄 도구 모음인 CSVkit의 일부입니다.

파일 형식은 이 경우 확장자 *.xlsx*에 의해 자동으로 결정됩니다.
데이터를 `in2csv`로 파이프하는 경우 형식을 명시적으로 지정해야 합니다.

스프레드시트에는 여러 워크시트가 포함될 수 있습니다.
`in2csv`는 기본적으로 첫 번째 워크시트를 추출합니다.
다른 워크시트를 추출하려면 워크시트 이름을 `--sheet` 옵션에 전달해야 합니다.
워크시트 이름이 무엇인지 확실하지 않은 경우 모든 워크시트 이름을 인쇄하는 `--names` 옵션을 사용할 수 있습니다.
여기서 *top2000.xlsx*에는 *Blad1*(*Sheet1*의 네덜란드어)이라는 하나의 시트만 있음을 알 수 있습니다.

```{console in2csv_names, remove=c("openpyxl", "silence", "behavior", "numpy", "guidance", "release")}
in2csv --names top2000.xlsx
```


## 관계형 데이터베이스 쿼리

많은 회사가 관계형 데이터베이스에 데이터를 저장합니다.
스프레드시트와 마찬가지로 명령줄에서 해당 데이터를 얻을 수 있다면 좋을 것입니다.

관계형 데이터베이스의 예로는 MySQL, PostgreSQL, SQLite가 있습니다.
이러한 데이터베이스는 모두 약간 다른 방식으로 인터페이스합니다.
일부는 명령줄 도구나 명령줄 인터페이스를 제공하지만 다른 일부는 그렇지 않습니다.
또한 사용법과 출력에 있어서 일관성이 없습니다.

다행히 CSVkit 제품군의 일부인 `sql2csv`라는 명령줄 도구가 있습니다.
MySQL, Oracle, PostgreSQL, SQLite, Microsoft SQL Server, Sybase를 포함한 다양한 데이터베이스에서 공통 인터페이스를 통해 작동합니다.
`sql2csv`의 출력은 이름에서 알 수 있듯이 CSV 형식입니다.

관계형 데이터베이스에서 *`SELECT`* 쿼리를 실행하여 데이터를 얻을 수 있습니다.
(`sql2csv`는 *`INSERT`*, *`UPDATE`*, *`DELETE`* 쿼리도 지원하지만 이 장의 목적은 아닙니다.)

`sql2csv`에는 두 가지 인수가 필요합니다. 데이터베이스 URL을 지정하는 `--db`(일반적인 형식은 `dialect+driver://username:password@host:port/database`)와 *`SELECT`* 쿼리가 포함된 `--query`입니다.
예를 들어 R[^3]의 표준 데이터 세트가 포함된 SQLite 데이터베이스가 주어지면 *mtcars* 테이블에서 모든 행을 선택하고 *mpg* 열을 기준으로 정렬할 수 있습니다.

```{console sql2csv}
sql2csv --db 'sqlite:///r-datasets.db' \
--query 'SELECT row_names AS car, mpg FROM mtcars ORDER BY mpg' | csvlook
```

이 SQLite 데이터베이스는 로컬 파일이므로 사용자 이름, 암호 또는 호스트를 지정할 필요가 없습니다.
고용주의 데이터베이스를 쿼리하려면 물론 액세스 방법을 알아야 하고 그렇게 할 수 있는 권한이 있어야 합니다.


## 웹 API 호출

이전 섹션에서는 인터넷에서 파일을 다운로드하는 방법을 설명했습니다.
인터넷에서 데이터를 가져오는 또 다른 방법은 *애플리케이션 프로그래밍 인터페이스*를 의미하는 웹 API를 사용하는 것입니다.
제공되는 API의 수가 점점 더 빠르게 증가하고 있으며 이는 데이터 과학자에게 많은 흥미로운 데이터를 의미합니다.

웹 API는 웹사이트와 같이 멋진 레이아웃으로 표시되도록 만들어지지 않았습니다.
대신 대부분의 웹 API는 JSON 또는 XML과 같은 구조화된 형식으로 데이터를 반환합니다.
구조화된 형식의 데이터를 사용하면 `jq`와 같은 다른 도구에서 데이터를 쉽게 처리할 수 있다는 장점이 있습니다.
예를 들어 조지 R.R. 마틴의 가상 세계에 대한 많은 정보가 포함된 얼음과 불의 API는 왕좌의 게임 책과 TV 쇼가 진행되는 곳이며 다음 JSON 구조로 데이터를 반환합니다.

```{console curl_api, callouts="died"}
curl -s "https://anapioficeandfire.com/api/characters/583" | jq '.'
```
<1> 스포일러 경고: 이 데이터는 완전히 최신 상태가 아닙니다.

데이터는 멋진 방식으로 표시하기 위해 명령줄 도구 `jq`로 파이프됩니다.
`jq`에는 [5장](#chapter-5-scrubbing-data)과 [7장](#chapter-7-exploring-data)에서 살펴볼 더 많은 정제 및 탐색 가능성이 있습니다.


### 인증

일부 웹 API는 출력을 사용하기 전에 인증(즉, 신원을 증명)해야 합니다.
이를 수행하는 방법에는 여러 가지가 있습니다.
일부 웹 API는 API 키를 사용하는 반면 다른 웹 API는 OAuth 프로토콜을 사용합니다.
헤드라인과 뉴스 기사의 독립적인 출처인 News API는 훌륭한 예입니다.
API 키 없이 이 API에 액세스하려고 하면 어떻게 되는지 살펴보겠습니다.

```{console curl_nokey}
curl -s "http://newsapi.org/v2/everything?q=linux" | jq .
```

음, 예상했던 일입니다.
참고로 물음표 뒤의 부분은 쿼리 매개변수를 전달하는 곳입니다.
API 키도 지정해야 하는 곳입니다.
제 자신의 API 키는 비밀로 유지하고 싶으므로 명령 대체를 사용하여 */data/.secret/newsapi.org_apikey* 파일을 읽어 아래에 삽입합니다.

```{console curl_auth}
curl -s "http://newsapi.org/v2/everything?q=linux&apiKey=$(< /data/.secret/newsapi.org_apikey)" |
jq '.' | trim 30
```
[News API 웹사이트](https://newsapi.org)에서 자신만의 API 키를 얻을 수 있습니다.


### 스트리밍 API

일부 웹 API는 스트리밍 방식으로 데이터를 반환합니다.
즉, 연결하면 연결이 닫힐 때까지 데이터가 계속 들어옵니다.
잘 알려진 예로는 전 세계에서 전송되는 모든 트윗을 지속적으로 스트리밍하는 트위터 "파이어호스"가 있습니다.
다행히 대부분의 명령줄 도구도 스트리밍 방식으로 작동합니다.


```{console wikimedia_cp, include=FALSE}
 cp /data/.cache/wikimedia-stream-sample .
```

예를 들어 위키미디어의 스트리밍 API 중 하나에서 10초 샘플을 가져와 보겠습니다.

```{console wikimedia_curl}
curl -s "https://stream.wikimedia.org/v2/stream/recentchange" |
sample -s 10 > wikimedia-stream-sample #! enter=FALSE
```

```{console wikimedia_cancel, include=FALSE}
C-C #! literal=FALSE
```

이 특정 API는 위키백과 및 위키미디어의 다른 속성에 적용된 모든 변경 사항을 반환합니다.
명령줄 도구 `sample`은 10초 후에 연결을 닫는 데 사용됩니다.
**`Ctrl-C`**를 눌러 인터럽트를 보내 수동으로 연결을 닫을 수도 있습니다.
출력은 *wikimedia-stream-sample* 파일에 저장됩니다.
`trim`을 사용하여 살펴보겠습니다.


```{console wikimedia_trim}
< wikimedia-stream-sample trim
```

약간의 `sed`와 `jq`를 사용하면 이 데이터를 정제하여 영어 버전의 위키백과에서 발생하는 변경 사항을 엿볼 수 있습니다.

```{console wikimedia_jq, callouts=c("sed", "jq")}
< wikimedia-stream-sample sed -n 's/^data: //p' |
jq 'select(.type == "edit" and .server_name == "en.wikipedia.org") | .title'
```
<1> 이 `sed` 표현식은 *`data: `*로 시작하는 줄만 인쇄하고 세미콜론 뒤의 부분(JSON임)을 인쇄합니다.
<2> 이 `jq` 표현식은 특정 *`type`*과 *`server_name`*을 갖는 JSON 객체의 *`title`* 키를 인쇄합니다.

스트리밍에 대해 말하자면 `telnet`[@telnet]을 사용하여 *스타워즈: 에피소드 IV – 새로운 희망*을 무료로 스트리밍할 수 있다는 것을 알고 계셨습니까?

```{console telnet_setup, include=FALSE}
 cd /data/.cache/
 alias telnet=echo
 print -S "telnet towel.blinkenlights.nl"
```

```{console telnet_cmd}
telnet towel.blinkenlights.nl#! enter=FALSE
```

그리고 잠시 후 한 솔로가 먼저 쐈다는 것을 알 수 있습니다!

```{console telnet_cancel, include=FALSE}
C-C#! literal=FALSE
```

```{console, remove=1}
 cat towel.blinkenlights.nl
```

물론 좋은 데이터 소스는 아니지만 기계 학습 모델을 훈련하는 동안 오래된 고전을 즐기는 데 아무런 문제가 없습니다[^starwars].


## 요약

축하합니다. OSEMN 모델의 첫 번째 단계를 완료했습니다.
다운로드에서 관계형 데이터베이스 쿼리에 이르기까지 다양한 데이터 획득 방법을 배웠습니다.
다음 장인 막간 장에서는 자신만의 명령줄 도구를 만드는 방법을 알려 드리겠습니다.
데이터 정제에 대해 배우고 싶어서 기다릴 수 없다면 이 장을 건너뛰고 [5장](#chapter-5-scrubbing-data)(OSEMN 모델의 두 번째 단계)으로 이동하십시오.


## 추가 탐색을 위해

- 연습할 데이터 세트를 찾고 계십니까? GitHub 저장소 [*Awesome Public Datasets*](https://github.com/awesomedata/awesome-public-datasets)에는 공개적으로 사용 가능한 수백 개의 고품질 데이터 세트가 나열되어 있습니다.
- 아니면 API로 연습하고 싶으십니까? GitHub 저장소 [*Public APIs*](https://github.com/public-apis/public-apis)에는 많은 무료 API가 나열되어 있습니다. [City Bikes](http://api.citybik.es/v2/)와 [The One API](https://the-one-api.dev/)는 제가 가장 좋아하는 API 중 하나입니다.
- 관계형 데이터베이스에서 데이터를 얻기 위해 SQL 쿼리를 작성하는 것은 중요한 기술입니다. Ben Forta의 책 *SQL in 10 Minutes a Day*의 처음 15개 강의에서는 *`SELECT`* 문과 해당 필터링, 그룹화 및 정렬 기능을 가르칩니다.

[^3]: [GitHub](https://github.com/r-dbi/RSQLite/blob/master/inst/db/datasets.sqlite)에서 사용 가능합니다.
[^starwars]: 누군가 아카이브 메모리에서 삭제했기 때문에 서버에 연결할 수 없는 경우 언제든지 YouTube에서 [`telnet` 세션 녹화](https://www.youtube.com/results?search_query=towel.blinkenlights.nl)를 즐길 수 있습니다.
