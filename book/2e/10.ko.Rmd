---
suppress-bibliography: true
---

```{r console_start, include=FALSE}
console_start()
```

```{console setup_history, include=FALSE}
 export CHAPTER="10"
 export HISTFILE=/history/history_${CHAPTER}
 rm -f $HISTFILE
```


# 다국어 데이터 과학 {#chapter-10-polyglot-data-science}

다국어 사용자는 여러 언어를 구사하는 사람입니다.
제가 보기에 다국어 데이터 과학자는 데이터를 획득, 정제, 탐색 및 모델링하기 위해 여러 프로그래밍 언어, 도구 및 기술을 사용하는 사람입니다.

명령줄은 다국어 접근 방식을 장려합니다.
명령줄은 도구가 Unix 철학을 따르는 한 어떤 프로그래밍 언어로 작성되었는지 신경 쓰지 않습니다.
[4장](#chapter-4-creating-command-line-tools)에서 Bash, Python, R로 명령줄 도구를 만들 때 이를 매우 분명하게 보았습니다.
또한 CSV 파일에서 직접 SQL 쿼리를 실행하고 명령줄에서 R 표현식을 실행했습니다.
요컨대, 우리는 이미 완전히 인식하지 못한 채 다국어 데이터 과학을 수행해 왔습니다!

이 장에서는 이를 뒤집어 더 나아가겠습니다.
다양한 프로그래밍 언어와 환경에서 명령줄을 활용하는 방법을 보여 드리겠습니다.
솔직히 말해서, 우리는 전체 데이터 과학 경력을 명령줄에서 보내지는 않을 것입니다.
저 같은 경우 데이터를 분석할 때 종종 RStudio IDE를 사용하고 무언가를 구현할 때는 종종 Python을 사용합니다.
작업을 완료하는 데 도움이 되는 모든 것을 사용합니다.

다른 응용 프로그램으로 전환할 필요 없이 명령줄이 종종 손이 닿는 곳에 있다는 것을 아는 것은 위안이 됩니다.
별도의 응용 프로그램으로 전환하고 작업 흐름을 중단하지 않고도 명령을 빠르게 실행할 수 있습니다.
예를 들어 `curl`로 파일 다운로드, `head`로 데이터 조각 검사, `git`으로 백업 만들기, `make`로 웹사이트 컴파일 등이 있습니다.
일반적으로 많은 코드가 필요하거나 명령줄 없이는 전혀 수행할 수 없는 작업입니다.


## 개요

이 장에서는 다음을 수행하는 방법을 배웁니다.

* JupyterLab 및 RStudio IDE 내에서 터미널 실행
* Python 및 R에서 임의의 명령줄 도구와 상호 작용
* Apache Spark에서 셸 명령을 사용하여 데이터 변환

이 장은 다음 파일로 시작합니다.

```{console}
cd /data/ch10
l
```

이러한 파일을 가져오는 지침은 [2장](#chapter-2-getting-started)에 있습니다.
다른 모든 파일은 명령줄 도구를 사용하여 다운로드하거나 생성됩니다.

## Jupyter

Project Jupyter는 2014년 IPython 프로젝트에서 파생된 오픈 소스 프로젝트로, 모든 프로그래밍 언어에서 대화형 데이터 과학 및 과학 컴퓨팅을 지원하도록 발전했습니다.
Jupyter는 Python, R, Julia, Scala를 포함하여 40개 이상의 프로그래밍 언어를 지원합니다.
이 섹션에서는 Python에 중점을 둘 것입니다.

이 프로젝트에는 JupyterLab, Jupyter Notebook, Jupyter Console이 포함됩니다.
대화형 방식으로 Python으로 작업하는 가장 기본적인 방법이므로 Jupyter Console부터 시작하겠습니다.
다음은 명령줄을 활용하는 몇 가지 방법을 보여주는 Jupyter Console 세션입니다.

```{console remove=list(1, "1R"), callouts=c("date", "open", "int", "import", "curl", "rm -v", "upper")}
cat /data/.cache/jupyter-console#!expect_prompt=FALSE
C-C#!literal=FALSE
```
<1> `date` 또는 Python 패키지를 설치하기 위한 `pip`와 같은 임의의 셸 명령 및 파이프라인을 실행할 수 있습니다.
<2> *alice.txt*의 줄 수를 세는 이 Python 코드 줄을 아래 `wc` 호출과 비교하십시오.
<3> 표준 출력이 문자열 목록으로 반환되므로 *total_lines* 값을 사용하려면 첫 번째 항목을 가져와 정수로 변환해야 합니다.
<4> 이 셀과 다음 셀을 비교하여 아래 `curl` 호출로 파일을 다운로드하십시오.
<5> 중괄호를 사용하여 Python 변수를 셸 명령의 일부로 사용할 수 있습니다.
<6> 리터럴 중괄호를 사용하려면 두 번 입력하십시오.
<7> Python 변수를 표준 입력으로 사용하는 것은 가능하지만 보시다시피 매우 까다로워집니다.

Jupyter Notebook은 본질적으로 Jupyter Console의 브라우저 기반 버전입니다.
느낌표와 bash 매직을 포함하여 명령줄을 활용하는 동일한 방법을 지원합니다.
가장 큰 차이점은 노트북에는 코드뿐만 아니라 마크업된 텍스트, 방정식, 데이터 시각화도 포함될 수 있다는 것입니다.
이러한 이유로 데이터 과학자들 사이에서 매우 인기가 있습니다.
Jupyter Notebook은 별도의 프로젝트 및 환경이지만 노트북으로 작업하기 위해 JupyterLab을 사용하고 싶습니다. 더 완벽한 IDE를 제공하기 때문입니다.

그림 \@ref(fig:jupyterlab)은 파일 탐색기(왼쪽), 코드 편집기(가운데), 노트북(오른쪽), 터미널(아래)을 보여주는 JupyterLab 스크린샷입니다. 후자의 세 가지 모두 명령줄을 활용하는 방법을 보여줍니다.
코드는 다음 섹션에서 다시 다룰 내용입니다.
이 특정 노트북은 방금 논의한 콘솔 세션과 매우 유사합니다.
터미널은 명령줄 도구를 실행할 수 있는 완전한 셸을 제공합니다.
이 터미널, 코드, 노트북 간에는 상호 작용이 불가능하다는 점에 유의하십시오.
따라서 이 터미널은 별도의 터미널 응용 프로그램을 열어두는 것과 실제로 다르지 않지만 Docker 컨테이너 내부나 원격 서버에서 작업할 때는 여전히 유용합니다.

```{r jupyterlab, echo=FALSE, fig.cap="파일 탐색기, 코드 편집기, 노트북, 터미널이 있는 JupyterLab", fig.align="center"}
knitr::include_graphics("images/screenshot_jupyterlab.png")
```


```{block2, type="rmdcaution"}
스크린샷의 이 노트북에는 여러 줄 Bash 스크립트를 작성할 수 있는 소위 `%%bash` 매직을 사용하는 셀도 포함되어 있습니다.
Python 변수를 사용하기가 훨씬 더 어렵기 때문에 이 접근 방식은 권장하지 않습니다.
별도의 파일에 Bash 스크립트를 만들고 느낌표(`!`)를 사용하여 실행하는 것이 좋습니다.
```


## Python

`subprocess` 모듈을 사용하면 Python에서 명령줄 도구를 실행하고 해당 표준 입력 및 출력에 연결할 수 있습니다.
이 모듈은 이전 `os.system()` 함수보다 권장됩니다.
기본적으로 셸에서 실행되지 않지만 `run()` 함수의 `shell` 인수를 사용하여 변경할 수 있습니다.

```{console, callouts=c("run", "open", "split", "grep", "stdout")}
bat count.py
```
<1> 명령줄을 활용하는 권장 방법은 `subprocess` 모듈의 `run()` 함수를 사용하는 것입니다.
<2> 파일 *filename* 열기
<3> 전체 텍스트를 단어로 분할
<4> 명령줄 도구 `grep` 실행, 여기서 *words*는 표준 입력으로 전달됩니다.
<5> 표준 출력은 하나의 긴 문자열로 제공됩니다. 여기서 각 줄 바꿈 문자를 기준으로 분할하여 *pattern*의 발생 횟수를 계산합니다.

이 명령줄 도구는 다음과 같이 사용됩니다.

```{console}
./count.py alice.txt alice
```

15행의 `run` 호출의 첫 번째 인수는 문자열 목록이며, 여기서 첫 번째 항목은 명령줄 도구의 이름이고 나머지 항목은 인수입니다.
이는 단일 문자열을 전달하는 것과는 다릅니다.
이는 또한 리디렉션 및 파이핑과 같은 다른 셸 구문을 사용할 수 없음을 의미합니다.


## R

R에서는 명령줄을 활용하는 여러 가지 방법이 있습니다.

아래 예에서는 R 세션을 시작하고 `system2()` 함수를 사용하여 *이상한 나라의 앨리스* 책에서 문자열 *alice*의 발생 횟수를 계산합니다.

```{r, include=FALSE}
# set_prompt(engine$session, prompts$R)
```

```{console, callouts=c("readLines", "strsplit", "system2", "length")}
R --quiet
lines <- readLines("alice.txt")
head(lines)
words <- unlist(strsplit(lines, " "))
head(words)
alice <- system2("grep", c("-i", "alice"), input = words, stdout = TRUE)
head(alice)
length(alice)
```
<1> 파일 *alice.txt* 읽기
<2> 텍스트를 단어로 분할
<3> 명령줄 도구 `grep`을 호출하여 문자열 *alice*와 일치하는 줄만 유지합니다. 문자 벡터 *words*는 표준 입력으로 전달됩니다.
<4> 문자 벡터 *alice*의 요소 수 계산

`system2()`의 단점은 명령줄 도구에 표준 입력으로 전달하기 전에 문자 벡터를 먼저 파일에 쓴다는 것입니다.
많은 데이터와 많은 호출을 처리할 때 문제가 될 수 있습니다.

명명된 파이프를 사용하는 것이 좋습니다. 데이터가 디스크에 기록되지 않아 훨씬 효율적이기 때문입니다.
이는 `pipe()` 및 `fifo()` 함수로 수행할 수 있습니다.
이를 제안해 준 Jim Hester에게 감사합니다.
아래 코드는 이를 보여줍니다.

```{console callouts=c("fifo", "grep", "writeLines", "readLines", "close")}
out_con <- fifo("out", "w+")
in_con <- pipe("grep b > out")
writeLines(c("foo", "bar"), in_con)
readLines(out_con)
close(out_con); close(in_con); unlink("out")
```
<1> 함수 `fifo()`는 *out*이라는 특수 FIFO(선입선출) 파일을 만듭니다. 이것은 파이프 연결(stdin 및 stdout과 같음)에 대한 참조일 뿐입니다. 데이터는 실제로 디스크에 기록되지 않습니다.
<2> 도구 `grep`은 *b*를 포함하는 줄만 유지하고 명명된 파이프 *out*에 씁니다.
<3> 셸 명령의 표준 입력에 두 값을 씁니다.
<4> `grep`에서 생성된 표준 출력을 문자 벡터로 읽습니다.
<5> 연결을 정리하고 특수 파일을 삭제합니다.


<!-- check out the processx package https://processx.r-lib.org/. experimental at the time of writing. but seems very promising to working with connections in a more robust manner. -->


<!-- # all four options are executed in a shell.  -->
<!-- # no stdin and no stdout: system -->
<!-- # stdin but no stdout: writeLines(pipe) -->
<!-- # stdout but no stdin: readLines(pipe) -->
<!-- readLines(pipe("")) -->
<!-- # both stdin and stdout: fifo, readLines, writeLines -->
<!-- first-in first-out special file, named pipe -->
<!-- When processes are -->
<!-- exchanging data via the FIFO, the kernel passes all data -->
<!-- internally without writing it to the filesystem. -->


`magrittr` 패키지의 파이프 연산자(`%>%`)를 사용하여 여러 셸 명령을 함께 연결하는 도우미 함수 `sh()`를 작성했습니다.


```{console}
library(magrittr)

sh <- function(.data, command) {#! expect_prompt=FALSE
  temp_file <- tempfile()#! expect_prompt=FALSE
  out_con <- fifo(temp_file, "w+")#! expect_prompt=FALSE
  in_con <- pipe(paste0(command, " > ", temp_file))#! expect_prompt=FALSE
  writeLines(as.character(.data), in_con)#! expect_prompt=FALSE
  result <- readLines(out_con)#! expect_prompt=FALSE
  close(out_con)#! expect_prompt=FALSE
  close(in_con)#! expect_prompt=FALSE
  unlink(temp_file)#! expect_prompt=FALSE
  result#! expect_prompt=FALSE
}

lines <- readLines("alice.txt")
words <- unlist(strsplit(lines, " "))

sh(words, "grep -i alice") %>%#! expect_prompt=FALSE
  sh("wc -l") %>%#! expect_prompt=FALSE
  sh("cowsay") %>%#! expect_prompt=FALSE
  cli::cat_boxx()

q("no")#! expect_prompt=FALSE
```

```{r, include=FALSE}
# set_prompt(engine$session, prompts$bash)
```


## RStudio

RStudio IDE는 R로 작업하기 위한 가장 인기 있는 환경이라고 할 수 있습니다.
RStudio를 열면 먼저 콘솔 탭이 표시됩니다.

```{r rstudio-console, echo=FALSE, fig.cap="콘솔 탭이 열린 RStudio IDE", fig.align="center"}
knitr::include_graphics("images/screenshot_rstudio_console.png")
```

터미널 탭은 콘솔 탭 바로 옆에 있습니다.
완전한 셸을 제공합니다.

```{r rstudio-terminal, echo=FALSE, fig.cap="터미널 탭이 열린 RStudio IDE", fig.align="center"}
knitr::include_graphics("images/screenshot_rstudio_terminal.png")
```

JupyterLab과 마찬가지로 이 터미널은 콘솔이나 R 스크립트에 연결되어 있지 않습니다.


## Apache Spark

Apache Spark는 클러스터 컴퓨팅 프레임워크입니다.
데이터를 메모리에 담을 수 없을 때 사용하는 800파운드 고릴라입니다.
Spark 자체는 Scala로 작성되었지만 [PySpark](https://spark.apache.org/docs/latest/api/python/index.html)를 사용하여 Python에서, [SparkR](https://spark.apache.org/docs/latest/sparkr.html) 또는 [sparklyr](https://spark.rstudio.com/)를 사용하여 R에서 상호 작용할 수도 있습니다.

데이터 처리 및 기계 학습 파이프라인은 일련의 변환과 하나의 최종 작업으로 정의됩니다.
이러한 변환 중 하나는 `pipe()` 변환으로, Bash 또는 Perl 스크립트와 같은 셸 명령을 통해 전체 데이터 세트를 실행할 수 있습니다.
데이터 세트의 항목은 표준 입력에 기록되고 표준 출력은 문자열의 RDD로 반환됩니다.

아래 세션에서는 Spark 셸을 시작하고 *이상한 나라의 앨리스* 책에서 *alice* 문자열의 발생 횟수를 다시 계산합니다.

```{console, remove=list("alias", 2, 4, "cache"), callouts=c("textFile", "flatMap", "grep", "wc", "res3", "toInt", "res5")}
alias spark-shell=echo
spark-shell --master local[6]#!enter=FALSE
C-C#!literal=FALSE
cat /data/.cache/spark
```
<1> *alice.txt*를 읽어 각 줄이 요소가 되도록 합니다.
<2> 각 요소를 공백으로 분할합니다. 즉, 각 줄이 단어로 분할됩니다.
<3> 각 파티션을 `grep`으로 파이프하여 *alice* 문자열과 일치하는 요소만 유지합니다.
<4> 각 파티션을 `wc`로 파이프하여 요소 수를 계산합니다.
<5> 각 파티션에 대해 하나의 개수가 있습니다.
<6> 모든 개수를 합하여 최종 개수를 얻습니다. 요소는 먼저 문자열에서 정수로 변환해야 합니다.
<7> 위 단계를 하나의 명령으로 결합합니다.

```{block2, type="rmdtip"}
`pipe()` 변환은 PySpark, SparkR, sparklyr에서도 사용할 수 있습니다.
```

<!-- https://stackoverflow.com/questions/54239583/question-about-rdd-pipe-operator-on-apache-spark -->

파이프라인에서 사용자 지정 명령줄 도구를 사용하려면 클러스터의 모든 노드(실행기라고 함)에 해당 도구가 있는지 확인해야 합니다.
이를 수행하는 한 가지 방법은 `spark-submit`을 사용하여 Spark 응용 프로그램을 제출할 때 `--files` 옵션으로 파일 이름을 지정하는 것입니다.

Matei Zaharia와 Bill Chambers(Apache Spark의 원래 작성자)는 그들의 책 *Spark: The Definitive Guide*에서 "[t]he `pipe` method is probably one of Spark's more interesting methods."라고 언급합니다.
정말 대단한 칭찬입니다!
Apache Spark 개발자들이 50년 된 기술을 활용할 수 있는 기능을 추가했다는 것은 환상적이라고 생각합니다.


<!-- ### Notable mentions -->

<!-- - Julia: Blog post with an introduction: https://blog.leahhanson.us/post/julia/julia-commands.html -->
<!-- - Visual Studio Code https://code.visualstudio.com/docs/editor/integrated-terminal -->
<!-- - Emacs -->
<!-- - VIM (using ! command) -->
<!-- - OS: Guake, -->
<!-- - OS: iTerm2:    https://www.sharmaprakash.com.np/guake-like-dropdown-terminal-in-mac/ -->
<!-- https://github.com/shelljs/shelljs -->
<!-- https://amoffat.github.io/sh/ -->
<!-- https://plumbum.readthedocs.io/en/latest/ -->
<!-- <\!-- ## Other Combinations -\-> -->

<!-- - reticulate -->
<!-- - Rpy2 -->
<!-- - sparkr -->
<!-- - sparklyr -->


<!-- TODO: MUST: Write Summary or Conclusion -->
<!-- # Summary -->
<!-- TODO: MUST: Talk about other combinations between languages. This is already possible with this approah, but there are tighter integrations. -->

## 요약

이 장에서는 프로그래밍 언어 및 기타 환경을 포함한 다른 상황에서 명령줄을 활용하는 여러 가지 방법을 배웠습니다.
명령줄이 진공 상태에 존재하지 않는다는 것을 깨닫는 것이 중요합니다.
가장 중요한 것은 때로는 조합하여 작업을 안정적으로 완료하는 도구를 사용하는 것입니다.

이제 네 가지 OSEMN 장과 네 가지 막간 장을 모두 다루었으므로 마지막 장에서 마무리하고 결론을 내릴 차례입니다.


## 추가 탐색을 위해

- 명령줄을 사용하지 않고 두 프로그래밍 언어를 직접 통합하는 방법도 있습니다. 예를 들어 R의 [`reticulate` 패키지](https://rstudio.github.io/reticulate/)를 사용하면 Python과 직접 인터페이스할 수 있습니다.
