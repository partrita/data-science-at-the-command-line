---
suppress-bibliography: true
---






# 병렬 파이프라인 {#chapter-8-parallel-pipelines}

<!-- #TODO: SHOULD: Dicuss progress bar -->

이전 장들에서 우리는 전체 작업을 한 번에 처리하는 명령어와 파이프라인을 다루었습니다.
하지만 실제로는 동일한 명령어 또는 파이프라인을 여러 번 실행해야 하는 작업에 직면하게 될 수도 있습니다.
예를 들어 다음과 같은 상황입니다.

- 수백 개의 웹 페이지 스크래핑하기
- 수십 개의 API를 호출하고 그 출력을 변환하기
- 다양한 매개변수 값에 대해 분류기(classifier) 훈련하기
- 데이터셋의 모든 특성 쌍에 대해 산점도 생성하기

위의 예시들에는 모두 특정한 형태의 반복 작업이 포함되어 있습니다.
여러분은 선호하는 스크립트나 프로그래밍 언어에서 `for` 루프나 `while` 루프를 통해 이를 처리했을 것입니다.
커맨드 라인에서 가장 먼저 하고 싶은 일은 아마 **`위`** 방향키를 눌러 이전 명령어를 불러온 뒤, 필요한 경우 수정하고 **`엔터`**를 눌러 다시 실행하는 것일지도 모릅니다.
두세 번 정도는 괜찮겠지만, 이를 수십 번 한다고 상상해 보십시오.
그러한 접근 방식은 금세 번거롭고 효율적이지 않으며 오류가 발생하기 쉬워집니다.
다행인 점은 이러한 루프를 커맨드 라인에서도 작성할 수 있다는 것입니다.
이것이 바로 이번 장에서 다룰 주제입니다.

때로는 빠른 명령어를 하나씩 차례대로(*순차적으로*) 반복하는 것만으로도 충분합니다.
하지만 데이터 집약적인 작업에 처했을 때, 여러 개의 코어(혹은 여러 대의 컴퓨터)가 있다면 이를 활용할 수 있으면 좋을 것입니다.
여러 코어나 장비를 사용하면 전체 실행 시간을 크게 단축할 수 있습니다.
이번 장에서는 바로 이러한 작업을 처리해 주는 `parallel` [@parallel]이라는 매우 강력한 도구를 소개해 드리겠습니다. 이 도구를 사용하면 숫자, 텍스트 줄(lines), 파일과 같은 다양한 인자(arguments) 범위에 대해 명령어 또는 파이프라인을 적용할 수 있습니다.
게다가 이름에서 알 수 있듯이, 명령어를 *병렬*로 실행할 수 있게 해줍니다.


## 개요

이번 인터메조(intermezzo) 장에서는 명령어와 파이프라인을 여러 번 실행해야 하는 작업의 속도를 높이는 여러 가지 접근 방식을 다룹니다.
저의 주된 목표는 `parallel`의 유연함과 강력함을 보여드리는 것입니다.
이 도구는 이 책에서 다루는 다른 어떤 도구와도 결합될 수 있으므로, 데이터 과학을 위해 커맨드 라인을 사용하는 여러분의 방식을 긍정적으로 바꿔놓을 것입니다.
이번 장에서 여러분은 다음 내용을 배우게 됩니다.

- 숫자, 텍스트 줄, 파일 범위에 대해 명령어를 순차적으로 실행하기
- 큰 작업을 여러 개의 작은 작업으로 쪼개기
- 파이프라인을 병렬로 실행하기
- 파이프라인을 여러 대의 컴퓨터로 분산하기

이번 장은 다음 파일들로 시작합니다.


``` console
cd /data/ch08
l
```

이 파일들을 가져오는 방법은 [2장](#chapter-2-getting-started)에 설명되어 있습니다.
그 외의 파일들은 커맨드 라인 도구를 사용하여 다운로드하거나 생성한 것들입니다.


## 순차 처리(Serial Processing)

병렬화에 대해 깊이 파고들기 전에, 순차적인 방식으로 루프를 실행하는 것에 대해 짧게 다루어 보겠습니다.
이 기능을 알아두는 가치가 있는 이유는 항상 사용 가능한 기능이고, 구문이 다른 프로그래밍 언어의 루프와 매우 비슷하며, 무엇보다 나중에 `parallel`의 소중함을 정말로 깨닫게 해주기 때문입니다.

이번 장의 서론에서 제공한 예시들로부터 루프를 돌릴 세 가지 유형의 항목을 추출할 수 있습니다. 바로 숫자, 텍스트 줄, 그리고 파일입니다.
이 세 가지 유형에 대해 다음 세 개의 하위 섹션에서 각각 논의하겠습니다.


### 숫자에 대해 루프 돌리기

0에서 100 사이의 모든 짝수의 제곱을 계산해야 한다고 상상해 봅시다. `bc` [@bc]라는 도구가 있는데, 이는 수식을 파이프로 전달할 수 있는 '기본 계산기(basic calculator)'입니다.
4의 제곱을 계산하는 명령어는 다음과 같습니다.


``` console
echo "4^2" | bc
```

일회성 계산이라면 이것으로 충분합니다.
하지만 서론에서 언급했듯이, **`위`** 방향키를 누르고 숫자를 바꾼 뒤 **`엔터`**를 누르는 짓을 50번이나 반복하는 것은 제정신이 아닐 것입니다!
이런 경우에는 `for` 루프를 사용하여 쉘이 대신 고된 일을 하도록 만드는 것이 좋습니다.


``` console
for i in {0..100..2}  #<1>
do
echo "$i^2" | bc      #<2>
done | trim
```
<1> Z 쉘에는 중괄호 확장(brace expansion)이라는 기능이 있어, *`{0..100..2}`*를 공백으로 구분된 목록인 *`0 2 4 … 98 100`*으로 변환합니다. 변수 *`i`*에는 첫 번째 반복에서 "0", 두 번째 반복에서 "1" [역주: 여기서는 2씩 증가하므로 "2"] 등의 값이 할당됩니다.
<2> 이 변수의 값은 앞에 달러 기호(*`$`*)를 붙여서 사용할 수 있습니다. 쉘은 `echo`가 실행되기 전에 *`$i`*를 그 값으로 교체합니다. *`do`*와 *`done`* 사이에는 하나 이상의 명령어가 올 수 있다는 점에 유의하십시오.

비록 구문이 여러분이 선호하는 프로그래밍 언어에 비해 조금 이상해 보일 수 있지만, 쉘에서 항상 사용할 수 있는 기능이므로 기억해 둘 가치가 있습니다.
잠시 후에 명령어를 반복하는 더 좋고 유연한 방법을 소개해 드리겠습니다.


### 텍스트 줄(Lines)에 대해 루프 돌리기

두 번째로 루프를 돌릴 수 있는 항목 유형은 텍스트 줄입니다.
이 줄들은 파일에서 올 수도 있고 표준 입력에서 올 수도 있습니다.
이 항목들은 숫자, 날짜, 이메일 주소 등 무엇이든 포함할 수 있으므로 매우 일반적인 접근 방식입니다.

모든 연락처에 이메일을 보내고 싶다고 상상해 봅시다.
먼저 무료 [Random User Generator API](https://randomuser.me)를 사용하여 가짜 사용자 데이터를 생성해 보겠습니다.


``` console
curl -s "https://randomuser.me/api/1.2/?results=5&seed=dsatcl2e" > users.json
< users.json jq -r '.results[].email' > emails
bat emails
```

`while` 루프를 사용하여 *emails* 파일의 줄들에 대해 루프를 돌릴 수 있습니다.


``` console
while read line                         #<1>
do
echo "Sending invitation to ${line}."   #<2>
done < emails                           #<3>
```
<1> 이 경우에는 Z 쉘이 입력이 몇 줄로 구성되어 있는지 미리 알 수 없으므로 `while` 루프를 사용해야 합니다.
<2> 이 경우 *line* 변수 주위의 중괄호는 필수적이지 않지만 (변수 이름에 마침표가 포함될 수 없으므로), 그래도 중괄호를 사용하는 것은 좋은 습관입니다.
<3> 이 리다이렉션은 `while` 앞에 올 수도 있습니다.

표준 입력 장치인 */dev/stdin*을 지정하여 상호작용적으로 `while` 루프에 입력을 제공할 수도 있습니다. 작업이 끝나면 **`Ctrl-D`**를 누르십시오.


``` console
while read line; do echo "You typed: ${line}."; done < /dev/stdin#! expect_prompt=FALSE
one#! expect_prompt=FALSE
two#! expect_prompt=FALSE
three#! expect_prompt=FALSE
C-D#! literal=FALSE, expect_prompt=TRUE
```

하지만 이 방식은 **`엔터`**를 누르는 즉시 해당 입력 줄에 대해 *`do`*와 *`done`* 사이의 명령어가 바로 실행된다는 단점이 있습니다. 되돌릴 수 없습니다.


### 파일에 대해 루프 돌리기

이 섹션에서는 우리가 자주 루프를 돌려야 하는 세 번째 항목인 파일에 대해 논의합니다.

특수 문자를 처리하려면 `ls` [@ls] 대신 글로빙(globbing, 즉 경로명 확장)을 사용하십시오.


``` console
for chapter in /data/*
do
echo "Processing Chapter ${chapter}."
done
```

중괄호 확장과 마찬가지로, *`/data/*`* 표현식은 `for` 루프에 의해 처리되기 전에 Z 쉘에 의해 목록으로 먼저 확장됩니다.

파일 목록을 작성하는 더 정교한 대안은 `find` [@find]입니다. 이 도구는 다음과 같은 특징이 있습니다.

- 하위 디렉터리까지 탐색할 수 있음
- 크기, 접근 시간, 권한 등 속성에 대한 정교한 검색 가능
- 공백이나 줄바꿈과 같은 특수 문자 처리 가능

예를 들어, 다음 `find` 실행은 */data* 디렉터리 아래에 있는 파일 중 확장자가 *csv*이고 크기가 2킬로바이트 미만인 모든 파일을 나열합니다.


``` console
find /data -type f -name '*.csv' -size -2k
```


## 병렬 처리(Parallel Processing)

여기에 있는 것처럼 매우 오래 실행되는 도구가 있다고 가정해 봅시다.


``` console
bat slow.sh
```
<1> `ts` [@ts]는 타임스탬프를 추가합니다.
<2> 마법 변수 *`RANDOM`*은 0에서 32767 사이의 의사 난수 개수를 반환하는 내부 Bash 함수를 호출합니다. 그 정수를 5로 나눈 나머지에 1을 더하면 *duration*이 1에서 5 사이가 되도록 보장합니다.
<3> `sleep`은 주어진 초 동안 실행을 일시 중지합니다.

이 프로세스는 아마 모든 가용 리소스를 다 쓰지는 않을 것입니다.
그런데 이 명령어를 아주 많이 실행해야 하는 상황이 생겼다고 해봅시다.
예를 들어, 일련의 파일 전체를 다운로드해야 하는 경우입니다.

병렬화하는 순진한 방법은 명령어를 백그라운드에서 실행하는 것입니다.
`slow.sh`를 세 번 실행해 보겠습니다.


``` console
for i in {A..C}; do
./slow.sh $i &
done#! hold=7
```
<1> 앰퍼샌드(`&`)는 명령어를 백그라운드로 보내어, `for` 루프가 즉시 다음 반복으로 계속 진행할 수 있게 합니다.
<2> 이 줄은 Z 쉘이 부여한 작업 번호와 프로세스 ID를 보여주며, 이는 더 세밀한 작업 제어에 사용될 수 있습니다. 이 주제는 강력하지만 이 책의 범위를 벗어납니다.

\BeginKnitrBlock{rmdnote}<div class="rmdnote">모든 것이 병렬화될 수 있는 것은 아니라는 점을 명심하십시오.
API 호출 횟수가 제한되어 있을 수도 있고, 어떤 명령어는 한 번에 하나의 인스턴스만 실행 가능할 수도 있습니다.</div>\EndKnitrBlock{rmdnote}

\@ref(fig:diagram-parallel-processing)은 개념적인 수준에서 순차 처리, 순진한 병렬 처리, 그리고 GNU Parallel을 사용한 병렬 처리 사이의 차이를 동시에 실행되는 프로세스 수와 전체 실행 시간 측면에서 보여줍니다.



이 순진한 접근 방식에는 두 가지 문제가 있습니다.
첫째, 동시에 실행되는 프로세스의 수를 제어할 방법이 없습니다.
너무 많은 작업을 한꺼번에 시작하면 CPU, 메모리, 디스크 액세스, 네트워크 대역폭과 같은 동일한 리소스를 놓고 서로 경쟁하게 될 수 있습니다.
이는 오히려 전체 실행 시간을 늘리는 결과를 초래할 수 있습니다.
둘째, 어떤 출력이 어떤 입력에 속하는지 구분하기 어렵습니다.
이제 더 나은 접근 방식을 살펴보겠습니다.


### GNU Parallel 소개

명령어와 파이프라인을 병렬화하고 분산할 수 있게 해주는 커맨드 라인 도구인 `parallel`을 소개하겠습니다.
이 도구의 장점은 기존 도구들을 있는 그대로 사용할 수 있다는 것입니다. 도구들을 수정할 필요가 없습니다.

\BeginKnitrBlock{rmdcaution}<div class="rmdcaution">커맨드 라인 도구 중에 이름이 `parallel`인 것이 두 개 있다는 점에 유의하십시오.
Docker 이미지를 사용하고 있다면 이미 올바른 도구가 설치되어 있습니다.
그렇지 않다면 `parallel --version`을 실행하여 올바른 도구인지 확인할 수 있습니다.
출력에 "GNU parallel"이라고 표시되어야 합니다.</div>\EndKnitrBlock{rmdcaution}

`parallel`의 세부 사항을 살펴보기 전에, 이전의 `for` 루프를 얼마나 쉽게 대체할 수 있는지 보여드리는 짧은 예시를 보십시오.


``` console
seq 0 2 100 | parallel "echo {}^2 | bc" | trim
```

이것이 `parallel`의 가장 단순한 형태입니다. 루프를 돌릴 항목들은 표준 입력을 통해 전달되며, `parallel`이 실행해야 할 명령어 외에는 다른 인자가 없습니다.
`parallel`이 어떻게 입력을 프로세스들에 동시에 분산하고 그 출력들을 수집하는지에 대한 그림은 \@ref(fig:diagram-parallel-output)를 참조하십시오.



보시다시피 기본적으로 `for` 루프 역할을 합니다.
이전 섹션의 `for` 루프를 대체하는 또 다른 예시입니다.


``` console
parallel --jobs 2 ./slow.sh ::: {A..C}
```

여기서는 `--jobs` 옵션을 사용하여 `parallel`이 동시에 최대 두 개의 작업을 실행하도록 지정했습니다. `slow.sh`에 전달할 인자들은 표준 입력 대신 명령문의 인자로 지정되었습니다 (`:::` 사용).

`parallel`은 무려 159개의 서로 다른 옵션을 제공하여 매우 방대한 기능을 갖추고 있습니다.
(아마도 너무 많을지도 모릅니다.)
다행히 효과적으로 사용하기 위해 알아야 할 옵션은 몇 개 되지 않습니다.
덜 일반적인 옵션을 사용해야 할 경우 매뉴얼 페이지가 아주 유용합니다.


### 입력 지정하기

`parallel`에서 가장 중요한 인자는 모든 입력에 대해 실행하고 싶은 명령어 또는 파이프라인입니다.
여기서 질문은, 입력 항목이 명령어의 어느 위치에 삽입되어야 하느냐는 것입니다.
아무것도 지정하지 않으면 입력 항목이 파이프라인의 맨 끝에 추가됩니다.


``` console
seq 3 | parallel cowsay#! enter=FALSE
C-C#! literal=FALSE
parallel --jobs 1 --keep-order cowsay ::: 1 2 3
```

위의 명령어는 다음과 같이 실행하는 것과 같습니다.


``` console
cowsay 1 > /dev/null #<1>
cowsay 2 > /dev/null
cowsay 3 > /dev/null
```
<1> 출력이 이전과 같으므로 출력을 억제하기 위해 */dev/null*로 리다이렉션했습니다.

이 방식이 작동할 때도 많지만, 플레이스홀더(placeholders)를 사용하여 명령어의 어느 부분에 입력 항목이 삽입되어야 하는지 명시하는 것이 좋습니다.
이 경우 입력 줄 전체(숫자 하나)를 한 번에 사용하고 싶으므로 하나의 플레이스홀더만 있으면 됩니다.
플레이스홀더, 즉 입력 항목을 넣을 위치는 한 쌍의 중괄호(`{}`)로 지정합니다.


``` console
seq 3 | parallel cowsay {} > /dev/null
```


\BeginKnitrBlock{rmdnote}<div class="rmdnote">`parallel`에 입력을 제공하는 다른 방법들도 있습니다.
저는 (이번 장에서 계속하는 것처럼) 입력을 파이프로 연결하는 방식을 선호하는데, 이는 대부분의 커맨드 라인 도구가 파이프라인으로 연결되는 방식이기 때문입니다.
다른 방식들은 다른 곳에서는 보기 힘든 구문을 포함하기도 합니다.
그렇긴 하지만, 여러 목록의 모든 가능한 조합에 대해 루프를 돌리는 것과 같은 추가 기능을 제공하므로, 더 자세히 알고 싶다면 `parallel`의 매뉴얼 페이지를 읽어보시기 바랍니다.</div>\EndKnitrBlock{rmdnote}

입력 항목이 파일 이름인 경우, 파일 이름의 일부만 사용할 수 있는 몇 가지 수식어(modifiers)가 있습니다.
예를 들어 `{/}`를 사용하면 파일 이름의 베이스 이름(basename)만 사용됩니다.


``` console
find /data/ch03 -type f | parallel echo '{#}\) \"{}\" has basename \"{/}\"'
```
<1> 괄호(`)`)나 따옴표(`"`) 같은 문자는 쉘에서 특별한 의미를 갖습니다. 이를 문자 그대로 사용하려면 앞에 백슬래시(`\`)를 붙입니다. 이를 이스케이프(escaping)라고 합니다.

입력 줄이 구분자로 나뉜 여러 부분으로 구성된 경우 플레이스홀더에 번호를 추가할 수 있습니다. 예를 들어 다음과 같습니다.


``` console
touch input.csv
< input.csv parallel --colsep , "mv {2} {1}" > /dev/null#! enter=FALSE
C-C#! literal=FALSE
```

여기에 똑같은 플레이스홀더 수식어를 적용할 수 있습니다.
동일한 입력 항목을 재사용하는 것도 가능합니다.
만약 `parallel`의 입력이 헤더가 있는 CSV 파일이라면, 열 이름을 플레이스홀더로 사용할 수 있습니다.


``` console
< input.csv parallel -C, --header : "invite {name} {email}"#! enter=FALSE
C-C#! literal=FALSE
```

\BeginKnitrBlock{rmdtip}<div class="rmdtip">플레이스홀더가 올바르게 설정되었는지 궁금하다면 `--dry-run` 옵션을 추가해 보십시오.
명령어를 실제로 실행하는 대신, `parallel`은 실행될 모든 명령어를 마치 실제로 실행될 것처럼 출력해 줍니다.</div>\EndKnitrBlock{rmdtip}


### 동시 실행 작업 수 제어하기

기본적으로 `parallel`은 CPU 코어당 하나의 작업을 실행합니다.
동시에 실행되는 작업 수는 `--jobs` 또는 `-j` 옵션으로 제어할 수 있습니다.
숫자를 지정하면 해당 숫자만큼의 작업이 동시에 실행됩니다.
숫자 앞에 플러스 기호(`+`)를 붙이면 CPU 코어 수에 해당 숫자를 더한 만큼의 작업을 실행합니다. 마이너스 기호(`-`)를 붙이면 CPU 코어 수에서 해당 숫자를 뺀 만큼의 작업을 실행합니다.
여기서 CPU 코어 수는 `N`을 의미합니다.
퍼센트를 지정할 수도 있으며, 기본값은 CPU 코어 수의 100%입니다.
동시에 실행할 최적의 작업 수는 여러분이 실행하고 있는 실제 명령어에 따라 달라집니다.


``` console
seq 5 | parallel -j0 "echo Hi {}"
```


``` console
seq 5 | parallel -j200% "echo Hi {}"
```

`-j1`을 지정하면 명령어가 한 번에 하나씩 순차적으로 실행됩니다. 비록 도구 이름의 의미를 무색하게 만들긴 하지만, 여전히 유용할 때가 있습니다. 예를 들어, 한 번에 하나의 연결만 허용하는 API에 접근해야 할 때입니다. `-j0`을 지정하면 `parallel`은 가능한 한 많은 작업을 병렬로 실행합니다. 이는 루프 끝에 앰퍼샌드를 붙여서 실행하는 것과 비슷합니다. 이 방식은 권장되지 않습니다.


### 로깅과 출력

각 명령어의 출력을 저장하고 싶을 때, 아마 다음과 같은 방식을 생각할 수도 있습니다.


``` console
seq 5 | parallel "echo \"Hi {}\" > hi-{}.txt"
```

이렇게 하면 출력이 각각의 개별 파일로 저장됩니다.
혹은 모든 내용을 하나의 큰 파일에 저장하고 싶다면 다음과 같이 할 수 있습니다.


``` console
seq 5 | parallel "echo Hi {}" >> one-big-file.txt
```

하지만 `parallel`은 출력을 개별 파일로 저장해 주는 `--results` 옵션을 제공합니다.
각 작업에 대해 `parallel`은 세 가지 파일을 생성합니다. 작업 번호를 담은 *seq*, 작업에 의해 생성된 표준 출력을 담은 *stdout*, 그리고 작업 중에 발생한 오류를 담은 *stderr*입니다.
이 세 파일은 입력 값에 기반한 하위 디렉터리에 배치됩니다.

`parallel`은 여전히 모든 출력을 인쇄하는데, 이 경우에는 불필요합니다.
다음과 같이 표준 입력과 표준 출력 모두를 넘길 수 있습니다.


``` console
seq 10 | parallel --results outdir "curl 'https://anapioficeandfire.com/api/characters/{}' | jq -r '.aliases[0]'" 2>/dev/null 1>&2
tree outdir | trim
```

`--results` 옵션이 어떻게 작동하는지에 대한 그림은 \@ref(fig:diagram-parallel-results)를 참조하십시오.



여러 작업을 병렬로 실행할 때, 작업이 실행되는 순서는 입력 순서와 일치하지 않을 수 있습니다.
따라서 작업의 출력도 섞이게 됩니다.
순서를 동일하게 유지하려면 `--keep-order` 또는 `-k` 옵션을 지정하십시오.

가끔 어떤 입력이 어떤 출력을 생성했는지 기록하는 것이 유용할 때가 있습니다.
`parallel`은 `--tag` 옵션을 통해 출력에 '태그'를 달 수 있게 해주는데, 이는 각 줄의 앞에 입력 항목을 붙여줍니다.


``` console
seq 5 | parallel --tag "echo 'sqrt({})' | bc -l"
parallel --tag --keep-order "echo '{1}*{2}' | bc -l" ::: 3 4 ::: 5 6 7
```


### 병렬 도구 만들기

본 장의 서두에서 사용했던 `bc` 도구는 그 자체로는 병렬로 작동하지 않습니다.
하지만 `parallel`을 사용하여 병렬화할 수 있습니다.
Docker 이미지에는 `pbc` [@pbc]라는 도구가 포함되어 있습니다.
해당 코드는 다음과 같습니다.


``` console
bat $(which pbc)
```

이 도구를 사용하면 본 장 서두에서 사용한 코드를 단순화할 수 있습니다.
또한 쉼표로 구분된 값들을 동시에 처리할 수 있습니다.


``` console
seq 100 | pbc '{1}^2' | trim
paste -d, <(seq 4) <(seq 4) <(seq 4) | pbc 'sqrt({1}+{2})^{3}'
```


## 분산 처리(Distributed Processing)

가끔은 모든 코어를 다 쓴다 하더라도 로컬 머신이 제공할 수 있는 것보다 더 많은 컴퓨팅 성능이 필요할 때가 있습니다.
다행히 `parallel`은 원격 머신의 성능도 활용할 수 있게 해주어, 파이프라인의 속도를 비약적으로 높여줍니다.

대단한 점은 원격 머신에 `parallel`이 반드시 설치되어 있을 필요가 없다는 것입니다.
원격 머신에 *보안 쉘(Secure Shell)* 프로토콜(즉, SSH)로 접속할 수만 있으면 됩니다. `parallel`은 이 SSH를 사용하여 파이프라인을 분산시킵니다.
(`parallel`이 원격 머신에 설치되어 있다면 각 원격 머신에서 몇 개의 코어를 고용할지 결정하는 데 도움이 되므로 설치되어 있는 것이 좋습니다. 이에 대해서는 나중에 더 자세히 다루겠습니다.)

먼저, 실행 중인 AWS EC2 인스턴스 목록을 가져오겠습니다.
원격 머신이 없더라도 걱정하지 마십시오. `parallel`에게 어떤 원격 머신을 사용할지 알려주는 `--slf hostnames` 옵션이 나올 때마다 이를 `--sshlogin :`으로 바꾸면 됩니다.
이렇게 하면 이 섹션의 예제들을 여전히 따라 할 수 있습니다.

사용할 원격 머신들을 파악했다면, 다음 세 가지 방식의 분산 처리를 고려해 볼 것입니다.

- 원격 머신에서 일반 명령어 실행하기
- 로컬 데이터를 직접 원격 머신들로 분산하기
- 원격 머신으로 파일을 보내고, 처리한 뒤, 결과를 가져오기


### 실행 중인 AWS EC2 인스턴스 목록 가져오기

이 섹션에서는 원격 머신의 호스트네임을 한 줄에 하나씩 담고 있는 *hostnames*라는 파일을 만들 것입니다.
여기서는 아마존 웹 서비스(AWS)를 예로 들겠습니다.
여러분에게 AWS 계정이 있고 인스턴스를 시작하는 방법을 알고 있다고 가정합니다.
다른 클라우드 컴퓨팅 서비스(예: 구글 클라우드 플랫폼 또는 마이크로소프트 애저)를 사용 중이거나 독자적인 서버를 보유하고 있다면, 다음 섹션으로 넘어가기 전에 직접 *hostnames* 파일을 만드시기 바랍니다.

AWS API의 커맨드 라인 인터페이스인 `aws` [@aws]를 사용하여 실행 중인 AWS EC2 인스턴스 목록을 가져올 수 있습니다.
`aws`를 사용하면 온라인 AWS 관리 콘솔에서 할 수 있는 거의 모든 일을 할 수 있습니다.

`aws ec2 describe-instances` 명령어는 모든 EC2 인스턴스에 대한 방대한 정보를 JSON 형식으로 반환합니다 (자세한 내용은 [온라인 문서](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/ec2/describe-instances.html)를 참조하십시오).
`jq`를 사용하여 관련 필드만 추출할 수 있습니다.


``` console
aws ec2 describe-instances | jq '.Reservations[].Instances[] | {public_dns: .PublicDnsName, state: .State.Name}'#! enter=FALSE
C-C#!literal=FALSE
echo '{' &&
echo '  "state": "running",' &&
echo '  "public_dns": "ec2-54-88-122-140.compute-1.amazonaws.com"' &&
echo '}' &&
echo '{' &&
echo '  "state": "stopped",' &&
echo '  "public_dns": null' &&
echo '}' &&
```

EC2 인스턴스의 가능한 상태는 *`pending`*, *`running`*, *`shutting-down`*, *`terminated`*, *`stopping`*, 그리고 *`stopped`*입니다.
실행 중인(running) 인스턴스에만 파이프라인을 분산할 수 있으므로, 다음과 같이 실행 중이 아닌 인스턴스들은 걸러냅니다.


``` console
aws ec2 describe-instances | jq -r '.Reservations[].Instances[] | select(.State.Name=="running") | .PublicDnsName' | tee hostnames#! enter=FALSE
C-C#! literal=FALSE
echo 'ec2-54-88-122-140.compute-1.amazonaws.com' &&
echo 'ec2-54-88-89-208.compute-1.amazonaws.com'
```

(`-r` 또는 `--raw-output` 옵션이 없으면 호스트네임 주위에 큰따옴표가 붙게 됩니다.)
출력은 *hostnames*에 저장하여 나중에 `parallel`에 전달할 수 있게 합니다.

언급했듯이 `parallel`은 원격 머신에 접속하기 위해 `ssh` [@ssh]를 고용합니다.
매번 자격 증명을 입력하지 않고 EC2 인스턴스에 접속하고 싶다면, *\~/.ssh/config* 파일에 다음과 같은 내용을 추가할 수 있습니다.




``` console
bat ~/.ssh/config
```

여러분이 어떤 배포판을 실행 중이냐에 따라 사용자 이름은 *`ubuntu`*가 아닐 수 있습니다.


### 원격 머신에서 명령어 실행하기

분산 처리의 첫 번째 방식은 원격 머신에서 일반 명령어를 실행하는 것입니다.
우선 각 EC2 인스턴스에서 `hostname` [@hostname] 도구를 실행하여 `parallel`이 제대로 작동하는지 확인해 봅시다.


``` console
parallel --nonall --sshloginfile hostnames hostname#! enter=FALSE
C-C#! literal=FALSE
echo 'ip-172-31-23-204\nip-172-31-23-205'
```

여기서 `--sshloginfile` 또는 `--slf` 옵션은 *hostnames* 파일을 참조하는 데 사용됩니다.
`--nonall` 옵션은 `parallel`에게 매개변수를 사용하지 않고 *hostnames* 파일에 있는 모든 원격 머신에서 동일한 명령어를 한 번씩 실행하라고 지시합니다.
기억하십시오. 활용할 원격 머신이 없다면 `--slf hostnames`를 `--sshlogin :`으로 바꾸어 명령어가 로컬 머신에서 실행되도록 할 수 있습니다.


``` console
parallel --nonall --sshlogin : hostname#! enter=FALSE
C-C#! literal=FALSE
echo 'data-science-toolbox'
```

모든 원격 머신에서 동일한 명령어를 한 번씩 실행하는 데는 머신당 하나의 코어만 있으면 됩니다. 만약 `parallel`에 전달된 인자 목록을 분산시키려 한다면 잠재적으로 하나 이상의 코어를 사용할 수 있습니다. 코어의 수가 명시적으로 지정되지 않았다면, `parallel`은 이를 스스로 파악하려고 시도할 것입니다.


``` console
alias fake=echo
seq 2 | parallel --slf hostnames echo 2>&1#! enter=FALSE
C-C#! literal=FALSE
fake 'bash: parallel: command not found' &&
fake -n 'parallel: Warning: Could not figure out number of cpus on' &&
fake ' ec2-54-88-122-140.compute-1.amazonaws.com (). Using 1.' &&
fake '1' &&
fake '2'
```

이 경우에는 두 대의 원격 머신 중 한 대에만 `parallel`이 설치되어 있습니다.
그중 하나에서 `parallel`을 찾을 수 없다는 경고 메시지가 뜹니다.
그 결과, `parallel`은 코어의 수를 파악할 수 없어 기본값으로 코어 하나만 사용하게 됩니다.
이런 경고 메시지를 받으면 다음 네 가지 중 하나를 할 수 있습니다.

- 걱정하지 말고, 머신당 코어 하나만 쓰는 것에 만족하기
- `--jobs` 또는 `-j` 옵션을 통해 각 머신에 대한 작업 수 지정하기
- *hostnames* 파일에서 각 호스트네임 앞에 예를 들어 *2/* 라고 적어 머신당 사용할 코어 수 지정하기 (이 경우 2개의 코어)
- 패키지 관리자를 사용하여 `parallel` 설치하기. 예를 들어 원격 머신들이 모두 우분투(Ubuntu)를 실행 중이라면 다음과 같습니다.


``` console
parallel --nonall --slf hostnames "sudo apt-get install -y parallel"#! enter=FALSE
C-C#! literal=FALSE
```


### 로컬 데이터를 원격 머신들로 분산하기

분산 처리의 두 번째 방식은 로컬 데이터를 직접 원격 머신들로 분산하는 것입니다.
여러 대의 원격 머신을 사용하여 처리하고 싶은 아주 큰 데이터셋이 하나 있다고 상상해 봅시다.
간단하게 1부터 1000까지의 모든 정수를 더해 보겠습니다.
먼저, `hostname`과 `wc`를 사용하여 입력이 실제로 분산되고 있는지, 그리고 각 원격 머신이 받은 입력의 길이(줄 수)는 얼마인지 출력해 확인해 보겠습니다.


``` console
seq 1000 | parallel -N100 --pipe --slf hostnames "(hostname; wc -l) | paste -sd:"#! enter=FALSE
C-C#! literal=FALSE
echo 'ip-172-31-23-204:100' &&
echo 'ip-172-31-23-205:100' &&
echo 'ip-172-31-23-205:100' &&
echo 'ip-172-31-23-204:100' &&
echo 'ip-172-31-23-205:100' &&
echo 'ip-172-31-23-204:100' &&
echo 'ip-172-31-23-205:100' &&
echo 'ip-172-31-23-204:100' &&
echo 'ip-172-31-23-205:100' &&
echo 'ip-172-31-23-204:100'
```

훌륭합니다. 1000개의 숫자가 100개씩 묶여서 (`-N100`으로 지정) 골고루 분산되는 것을 볼 수 있습니다.
이제 이 모든 숫자들을 더할 준비가 되었습니다.


``` console
seq 1000 | parallel -N100 --pipe --slf hostnames "paste -sd+ | bc" | paste -sd+ | bc#! enter=FALSE
C-C#! literal=FALSE
echo '500500'
```

여기서는 원격 머신들로부터 돌려받은 10개의 합계를 즉시 다시 합산했습니다.
`parallel` 없이 계산했을 때와 결과가 같은지 확인해 봅시다.


``` console
seq 1000 | paste -sd+ | bc
```

네, 잘 작동합니다.
원격 머신에서 실행하고 싶은 더 큰 파이프라인이 있다면, 이를 별도의 스크립트에 담아 `parallel`을 통해 업로드할 수도 있습니다.
`add`라는 아주 간단한 커맨드 라인 도구를 만들어 이를 시연해 보겠습니다.


``` console
echo '#!/usr/bin/env bash' > add
echo 'paste -sd+ | bc' >> add
bat add
chmod u+x add
seq 1000 | ./add
```

`--basefile` 옵션을 사용하면 `parallel`은 작업을 실행하기 전에 *add* 파일을 모든 원격 머신에 먼저 업로드합니다.


``` console
seq 1000 |
parallel -N100 --basefile add --pipe --slf hostnames './add' |
./add #! enter=FALSE
C-C#! literal=FALSE
echo '500500'
```

1000개의 숫자를 합산하는 것은 물룬 장난감 수준의 예제일 뿐입니다.
게다가 이를 로컬에서 수행하는 것이 훨씬 더 빨랐을 것입니다.
그래도 `parallel`이 얼마나 믿기 힘들 정도로 강력할 수 있는지 이 예제를 통해 명확해졌기를 바랍니다.


### 원격 머신에서 파일 처리하기

분산 처리의 세 번째 방식은 원격 머신으로 파일을 보내고, 처리한 뒤, 결과를 가져오는 것입니다.
뉴욕시의 각 구(borough)별로 311 서비스 호출이 얼마나 자주 발생하는지 집계하고 싶다고 가정해 봅시다.
아직 로컬 머신에 해당 데이터가 없으므로, 먼저 무료 [NYC Open Data API](https://data.cityofnewyork.us/)에서 데이터를 가져오겠습니다.


``` console
seq 0 100 900 | parallel  "curl -sL 'http://data.cityofnewyork.us/resource/erm2-nwe9.json?\$limit=100&\$offset={}' | jq -c '.[]' | gzip > nyc-{#}.json.gz"
```

이제 압축된 JSON 데이터를 포함하는 10개의 파일이 생겼습니다.


``` console
l nyc*json.gz
```

`jq -c '.[]'`가 JSON 객체 배열을 평탄화(flatten)하여 한 줄에 하나의 객체가 오도록 하며, 파일당 총 100줄이 되도록 한다는 점에 유의하십시오.
`zcat` [@zcat]을 사용하면 압축된 파일의 내용을 직접 출력할 수 있습니다.


``` console
zcat nyc-1.json.gz | trim
```

JSON의 한 줄이 어떻게 생겼는지 확인해 봅시다.


``` console
zcat nyc-1.json.gz | head -n 1
```

로컬 머신에서 각 구별 서비스 호출 총 횟수를 구하려면 다음과 같은 명령어를 실행할 것입니다.


``` console
zcat nyc*json.gz |
jq -r '.borough' |
tr '[A-Z] ' '[a-z]_' |
sort | uniq -c | sort -nr |
awk '{print $2","$1}' |
header -a borough,count |
csvlook
```
<1> `zcat`을 사용하여 모든 압축 파일을 확장합니다.
<2> 각 호출에 대해 `jq`를 사용하여 구의 이름을 추출합니다.
<3> 구 이름을 소문자로 변환하고 공백을 언더스코어(`_`)로 바꿉니다 (`awk`가 기본적으로 공백을 기준으로 분리하기 때문입니다).
<4> `sort`와 `uniq`를 사용하여 각 구의 발생 횟수를 집계합니다.
<5> 두 열의 순서를 바꾸고 `awk`를 사용하여 쉼표로 구분합니다.
<6> `header`를 사용하여 헤더를 추가합니다.

잠시, 여러분의 머신이 너무 느려서 이 파이프라인을 로컬에서 도저히 수행할 수 없다고 상상해 봅시다.
`parallel`을 사용하여 로컬 파일을 원격 머신들로 분산하고, 원격 머신에서 처리를 수행한 뒤 그 결과를 가져올 수 있습니다.


``` console
ls *.json.gz |
parallel -v --basefile jq \
--trc {.}.csv \
--slf hostnames \
"zcat {} | ./jq -r '.borough' | tr '[A-Z] ' '[a-z]_' | sort | uniq -c | awk '{print \$2\",\"\$1}' > {.}.csv"#! enter=FALSE
C-C#! literal=FALSE
```
<1> 파일 목록을 출력하고 이를 `parallel`로 파이프합니다.
<2> `jq` 바이너리를 각 원격 머신으로 전송합니다. 다행히 `jq`는 의존성이 없습니다. 이 파일은 `--trc` 옵션(이는 `--cleanup` 옵션을 포함합니다)을 지정했으므로 나중에 원격 머신에서 제거됩니다. 파이프라인에서 `jq` 대신 `./jq`를 사용하는 점에 유의하십시오. 파이프라인이 검색 경로(search path)에 있을지도 모르는 버전이 아니라 업로드된 버전을 사용해야 하기 때문입니다.
<3> 커맨드 라인 인자 `--trc {.}.csv`는 `--transfer --return {.}.csv --cleanup`의 약어입니다. (치환 문자열 *`{.}`*은 입력 파일명에서 마지막 확장자를 제외한 것으로 치환됩니다.) 여기서 이 옵션은 JSON 파일이 원격 머신으로 전송되고, CSV 파일이 로컬 머신으로 반환되며, 각 작업이 끝난 후 원격 머신에서 두 파일이 모두 삭제됨을 의미합니다.
<4> 호스트네임 목록을 지정합니다. 로컬에서 직접 시도해보고 싶다면 `--slf hostnames` 대신 `--sshlogin :`을 지정할 수도 있습니다.
<5> `awk` 표현식의 이스케이프 처리에 유의하십시오. 따옴표 처리는 가끔 까다로울 수 있습니다. 여기서는 달러 기호와 큰따옴표가 이스케이프되었습니다. 따옴표 처리가 너무 혼란스러워진다면, `add` 도구에서 했던 것처럼 파이프라인을 별도의 커맨드 라인 도구로 만드는 것을 기억하십시오.



이 과정 중에 원격 머신 중 하나에서 `ls`를 실행해 본다면, `parallel`이 실제로 바이너리 `jq`, JSON 파일, 그리고 CSV 파일을 전송(하고 정리)하는 것을 볼 수 있을 것입니다.


``` console
ssh $(head -n 1 hostnames) ls#! enter=FALSE
C-C#! literal=FALSE
echo 'nyc-1.json.csv' &&
echo 'nyc-1.json.gz' &&
echo 'jq' &&
```

각 CSV 파일은 다음과 같이 생겼습니다.


``` console
cat nyc-1.json.csv #! enter=FALSE
C-C#! literal=FALSE
echo 'bronx,3' &&
echo 'brooklyn,5' &&
echo 'manhattan,24' &&
echo 'queens,3' &&
echo 'staten_island,2'
```

`rush` [@rush]와 tidyverse를 사용하여 각 CSV 파일의 합계를 집계할 수 있습니다.


``` console
cat nyc*csv | header -a borough,count |
rush run -t 'group_by(df, borough) %>% summarize(count = sum(count))' - |
csvsort -rc count | csvlook
```

또는 결과 집계에 SQL을 사용하고 싶다면, [5장](#chapter-5-scrubbing-data)에서 다룬 `csvsql`을 사용할 수 있습니다.


``` console
cat nyc*csv | header -a borough,count |
csvsql --query 'SELECT borough, SUM(count) AS count FROM stdin GROUP BY borough ORDER BY count DESC' |
csvlook
```


## 요약

데이터 과학자로서 여러분은 데이터, 때로는 아주 많은 양의 데이터를 다룹니다.
이는 가끔 명령어를 여러 번 실행하거나 데이터 집약적인 명령어를 여러 코어에 분산해야 함을 의미합니다.
이번 장에서 저는 명령어를 병렬화하는 것이 얼마나 쉬운지 보여드렸습니다.
`parallel`은 일반적인 커맨드 라인 도구의 속도를 높이고 이를 분산하는 매우 강력하고 유연한 도구입니다.
`parallel`은 아주 많은 기능을 제공하며 이번 장에서 저는 그 겉핥기 정도만 할 수 있었습니다.
다음 장에서는 OSEMN 모델의 네 번째 단계인 데이터 모델링을 다루겠습니다.


## 더 읽어보기

- `parallel`과 그 주요 옵션들에 대해 기본적인 이해를 마쳤다면, [온라인 튜토리얼](https://www.gnu.org/software/parallel/parallel_tutorial.html)을 살펴보실 것을 권장합니다. 입력을 지정하는 다양한 방법, 모든 작업의 로그 유지, 타임아웃, 재개(resume), 그리고 작업 재시도(retry) 방법 등을 배울 수 있습니다. `parallel`의 제작자인 Ole Tange는 이 튜토리얼에서 이렇게 말합니다. "여러분의 커맨드 라인이 당신을 아주 좋아하게 될 것입니다(Your command line will love you for it)."
