[{"path":"index.html","id":"환영합니다","chapter":"환영합니다","heading":"환영합니다","text":"Jeroen Janssens가 집필하고 2021년 10월 O’Reilly Media에서 출판한 커맨드 라인에서 시작하는 데이터 과학 2판 웹사이트에 오신 것을 환영합니다. 이 웹사이트는 무료로 이용할 수 있습니다. 콘텐츠는 Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License 라이선스를 따릅니다. 종이책은 Amazon에서 주문하실 수 있습니다.Jeroen에게 직접 배우고 싶으신가요? Jeroen은 자신의 회사인 Data Science Workshops를 통해 커맨드 라인 데이터 과학 및 Python, R, 머신러닝과 같은 관련 주제에 대한 사내 교육을 제공합니다. 자세한 정보는 Data Science Workshops를 방문하세요.","code":""},{"path":"index.html","id":"책-소개","chapter":"환영합니다","heading":"책 소개","text":"이 개정판 가이드는 커맨드 라인의 유연성이 어떻게 여러분을 더 효율적이고 생산적인 데이터 과학자로 만들어 줄 수 있는지 보여줍니다. 작지만 강력한 커맨드 라인 도구들을 결합하여 데이터를 빠르게 획득, 정제, 탐색 및 모델링하는 방법을 배우게 됩니다. 시작을 돕기 위해 저자 Jeroen Janssens는 Windows, macOS, Linux 어디서든 유용한 100개 이상의 유닉스 파워 도구가 포함된 Docker 이미지를 제공합니다.커맨드 라인이 왜 민첩하고 확장 가능하며 유연한 기술인지 빠르게 깨닫게 될 것입니다. Python이나 R로 데이터를 처리하는 데 익숙하더라도 커맨드 라인의 힘을 활용하여 데이터 과학 워크플로우를 크게 개선하는 방법을 배울 수 있습니다. 이 책은 데이터 과학자, 분석가, 엔지니어, 시스템 관리자 및 연구자에게 이상적입니다.웹사이트, API, 데이터베이스 및 스프레드시트에서 데이터 획득텍스트, CSV, HTML, XML 및 JSON 파일에 대한 정제(scrub) 작업 수행데이터 탐색, 기술 통계 계산 및 시각화 생성데이터 과학 워크플로우 관리한 줄 명령어(one-liners)와 기존 Python 또는 R 코드로 나만의 도구 만들기데이터 집약적인 파이프라인의 병렬화 및 분산 처리차원 축소, 회귀 및 분류 알고리즘으로 데이터 모델링Python, Jupyter, R, RStudio 및 Apache Spark에서 커맨드 라인 활용","code":""},{"path":"index.html","id":"추천사","chapter":"환영합니다","heading":"추천사","text":"\n전통적인 컴퓨터 및 데이터 과학 커리큘럼은 커맨드 라인을 현대적이고 필수적인 도구가 아닌 구시대의 유물로 오해하는 경우가 많습니다. 저는 경력을 쌓고 난 후에야 지저분한 데이터셋을 탐색하고 재현 가능한 데이터 파이프라인을 만드는 데 있어 커맨드 라인의 우아함과 강력함을 깨달았습니다. 커맨드 라인에서 시작하는 데이터 과학 1판은 제가 초보였을 때 가장 포괄적이고 명확한 참고서 중 하나였으며, 이제 2판을 통해 다시 새로운 도구와 응용법을 배우고 있습니다.\n\nDan Nguyen, 데이터 과학자, 전 ProPublica 뉴스 앱 개발자, 전 스탠포드 대학교 전문 저널리즘 객원 교수\n\n각자 하나의 일을 잘 수행하는 단순한 도구들을 파이프로 연결하는 유닉스 철학이 커맨드 라인에 담겨 있습니다. Jeroen은 이러한 철학을 데이터 과학 업무에 어떻게 접목시키는지 숙련되게 설명하며, 커맨드 라인이 단순히 파일 입출력의 세계가 아니라 데이터 조작, 탐색, 심지어 모델링의 세계임을 보여줍니다.\n\nChris H. Wiggins, 컬럼비아 대학교 응용 물리학 및 응용 수학과 부교수, New York Times 수석 데이터 과학자\n\n이 책은 일반적인 데이터 과학 태스크를 일관된 워크플로우로 통합하는 방법을 설명합니다. 단순히 문제를 분해하는 전술뿐만 아니라, 해결책의 조각들을 조립하는 전략에 대해서도 다룹니다.\n\nJohn D. Cook, 응용 수학, 통계 및 기술 컴퓨팅 컨설턴트\n\n많은 이야기들이 들리겠지만, 대부분의 실무 데이터 과학은 여전히 평면 파일(flat files)에서 유도된 흥미로운 시각화와 통찰에 집중되어 있습니다. Jeroen의 책은 이러한 현실을 파고들어, 검증된 커맨드 라인 도구들이 어떻게 데이터 과학용으로 재탄생할 수 있는지 보여줌으로써 데이터 실무자들의 복잡성을 줄여줍니다.\n\nPaige Bailey, Microsoft GitHub 코드 인텔리전스 수석 제품 매니저\n\n데이터를 R, Python 또는 데이터베이스로 가져오기 전에도 커맨드 라인에서 수많은 데이터 작업을 얼마나 빠르게 수행할 수 있는지 놀랍습니다. sed와 awk 같은 오래된 기술들은 여전히 믿을 수 없을 정도로 강력하고 다재다능합니다. 커맨드 라인에서 시작하는 데이터 과학을 읽기 전까지 저는 이 도구들에 대해 들어보기만 했지 그 진정한 위력을 본 적이 없었습니다. Jeroen 덕분에 이제 대용량 데이터를 다루는 비밀 무기를 갖게 된 기분입니다.\n\nJared Lander, Lander Analytics 수석 데이터 과학자, New York Open Statistical Programming Meetup 운영자, R Everyone 저자\n\n커맨드 라인은 모든 데이터 과학자의 도구 상자에 있어야 할 필수 도구이며, 이를 잘 활용하면 데이터에 대한 질문을 실시간 통찰로 쉽게 전환할 수 있습니다. Jeroen은 복잡한 문제에 대한 단순한 해결책을 찾기 위해 단일 목적의 도구들을 함께 연결하는 기본 유닉스 철학을 설명할 뿐만 아니라, 데이터 정제, 분석, 시각화 및 모델링을 위한 새로운 커맨드 라인 도구들을 소개합니다.\n\nJake Hofman, Microsoft Research 수석 연구원, 컬럼비아 대학교 응용 수학과 겸임 조교수\n","code":""},{"path":"index.html","id":"헌사","chapter":"환영합니다","heading":"헌사","text":"다시 한번 아내 Esther에게. 당신의 지속적인 격려와 지원, 그리고 인내가 없었다면 이 2판은 분명 /dev/null로 사라졌을 것입니다.","code":""},{"path":"index.html","id":"저자-소개","chapter":"환영합니다","heading":"저자 소개","text":"Jeroen Janssens는 독립 데이터 과학 컨설턴트이자 강사입니다. 데이터 시각화, 머신러닝 모델 구현, 그리고 Python, R, JavaScript, Bash를 사용한 솔루션 구축을 즐깁니다. Jeroen은 교육 및 코칭 회사인 Data Science Workshops를 운영하며 공개 워크숍, 사내 교육, 영감 세션, 해커톤 및 미트업을 조직합니다. 이전에는 Jheronimus Academy Data Science의 조교수였으며, 암스테르담의 Elsevier와 뉴욕의 여러 스타트업에서 데이터 과학자로 근무했습니다. Jeroen은 Tilburg 대학교에서 머신러닝 전공 박사 학위를, Maastricht 대학교에서 인공지능 전공 석사 학위를 취득했습니다. 현재 네덜란드 로테르담에서 아내, 두 아이와 함께 살고 있습니다.\nTwitter(1(https://twitter.com/jeroenhjanssens)), GitHub, LinkedIn에서 Jeroen을 만날 수 있습니다.","code":""},{"path":"index.html","id":"판권-정보","chapter":"환영합니다","heading":"판권 정보","text":"커맨드 라인에서 시작하는 데이터 과학의 표지에 있는 동물은 주름목코뿔새(Rhytidoceros undulatus)입니다. 동남아시아 본토와 인도 북동부, 부탄의 숲에서 발견되는 이 종은 ’bar-pouched wreathed hornbill’로도 알려져 있습니다. 코뿔새(Hornbills)라는 이름은 부리 윗부분에 형성되는 투구(casques) 모양 구조물에서 유래되었습니다. 이 속이 빈 각질 구조의 뚜렷한 목적은 밝혀지지 않았지만, 종 구성원 간의 인식 수단, 울음소리 증폭기, 또는 수컷의 투구가 암컷보다 더 크기 때문에 성별 인식 수단으로 쓰일 수 있습니다. 주름목코뿔새는 근연종인 ’plain-pouched hornbill’과 외형이 비슷하지만, 목 아래쪽의 어두운 줄무늬로 구분할 수 있습니다.주름목코뿔새는 최대 400마리까지 무리를 지어 잠을 자지만, 짝짓기는 일부일처제로 평생을 함께합니다. 수컷의 도움을 받아 암컷은 배설물과 진흙으로 나무 구멍을 막고 그 안에서 알을 낳고 품습니다. 수컷은 부리가 겨우 들어갈 정도의 틈을 통해 암컷과 새끼에게 최장 4개월 동안 먹이를 나릅니다. 암컷과 새끼가 둥지를 떠나면 육식 위주의 식단은 주로 과일로 바뀝니다. 코뿔새 커플은 같은 둥지를 최장 9년 동안 사용하는 것으로 알려져 있습니다.O’Reilly 표지에 등장하는 많은 동물들은 멸종 위기종입니다. 이들 모두는 세상에 중요한 존재들입니다.컬러 일러스트레이션은 Braukhaus의 Lexicon 흑백 판화를 기초로 Karen Montgomery가 그렸습니다. 표지 서체는 Gilroy Semibold와 Guardian Sans입니다. 본문 및 제목 서체는 Source Sans Pro이며, 코드 서체는 Fira Mono입니다.","code":""},{"path":"추천의-글.html","id":"추천의-글","chapter":"추천의 글","heading":"추천의 글","text":"그것은 첫눈에 반한 사랑이었습니다.제가 유닉스를 처음 맛본 것은 1981년이나 1982년쯤이었을 것입니다.\n단일 명령과 복잡한 프로그램에 동일한 언어를 사용하는 커맨드 라인 쉘은 제 세상을 바꾸어 놓았고, 저는 결코 뒤를 돌아보지 않았습니다.저는 컴퓨팅의 즐거움을 발견한 작가였고, 정규 표현식은 저를 매료시킨 입문용 마약과도 같았습니다.\nHP의 RTE 운영체제에 있는 텍스트 에디터에서 정규 표현식을 처음 써봤지만, 유닉스와 그 철학—작고 서로 협력하는 도구들을 커맨드 라인 쉘이라는 접착제로 묶는 방식—을 접하고 나서야 비로소 그 진정한 위력을 이해하게 되었습니다.ed, ex, vi (현재의 vim), 그리고 emacs의 정규 표현식은 강력했지만, ex 스크립트가 독립되어 유닉스 스트림 에디터인 sed가 되고, 다시 프로그래밍된 동작을 정규 표현식에 바인딩할 수 있게 해주는 AWK가 되는 과정을 지켜보면서 깨달음을 얻었습니다. 또한 쉘 스크립트를 통해 기존 도구뿐만 아니라 직접 만든 새로운 도구들로 파이프라인을 구축할 수 있다는 사실을 알게 되었을 때 비로소 저는 유닉스의 본질을 이해하게 되었습니다.\n프로그래밍은 컴퓨터와 대화하는 방식이며, 컴퓨터에게 무엇을 하라고 지시하는 방법입니다. 단순히 한 번만 시키는 것이 아니라 지속되는 방식으로, 인간의 언어처럼 반복 가능한 구조를 가지면서도 동사와 목적어를 다양하게 바꿀 수 있는 방식으로 말입니다.초보자였을 때, 다른 형태의 프로그래밍은 모든 것을 완벽하게 맞춰야 하는 정교한 주문이나 엄격히 따라야 할 조리법처럼 느껴졌습니다. 마치 선생님이 제가 쓴 에세이를 채점하기를 기다리는 기분이었죠.\n쉘 프로그래밍에는 컴파일이나 기다림이 없었습니다.\n그것은 마치 친구와 대화하는 것과 더 비슷했습니다.\n친구가 이해하지 못하면 다시 시도해보기 쉬웠습니다.\n게다가 할 말이 간단하다면 단어 하나로도 의사 표현이 가능했습니다.\n이미 수많은 상황에 맞는 단어들이 준비되어 있었고, 없다면 새로운 단어를 쉽게 만들 수도 있었습니다.\n그리고 배운 단어와 직접 만든 단어들을 엮어 점차 복잡한 문장과 단락을 만들고, 결국에는 설득력 있는 에세이를 작성할 수 있었습니다.거의 모든 다른 프로그래밍 언어가 쉘과 그 관련 도구들보다 강력할지 모르지만, 적어도 제게는 프로그래밍적 사고방식으로 들어가는 더 쉬운 길을 제공하는 것도, 업무를 도와달라고 요청하는 기계와의 일상적인 대화를 위한 더 나은 환경을 제공하는 것도 없습니다.\nAWK의 창시자 중 한 명이자 명저인 Unix Programming Environment의 공동 저자인 브라이언 커니핸(Brian Kernighan)은 2019년 렉스 프리드먼(Lex Fridman)과의 인터뷰에서 “[유닉스는] 프로그램을 작성하기 정말 쉬운 환경이 되도록 의도되었습니다.”라고 말했습니다. [00:23:10]\n그는 데이터를 탐색할 때 왜 여전히 파이썬 프로그램을 작성하기보다 AWK를 자주 사용하는지 설명하며 덧붙였습니다.\n“큰 프로그램에는 적합하지 않지만, 무언가 안에서 특정한 것들을 보고 싶을 때 수행하는 이런 작은 작업들에는 정말 기막히게 잘 돌아갑니다.” [00:37:01]커맨드 라인에서 시작하는 데이터 과학에서 Jeroen Janssens는 유닉스/리눅스 방식의 커맨드 라인이 오늘날에도 얼마나 강력한지 보여줍니다.\nJeroen이 이미 책에서 다루지 않았다면, 저는 여기서 왜 커맨드 라인이 데이터 과학에서 흔히 마주치는 작업들과 이토록 잘 어울리는 강력한 조합인지에 대해 에세이를 썼을 것입니다.\n하지만 그는 이미 책의 시작 부분에서 이를 설명하고 있습니다.\n그래서 저는 이 말만 전하겠습니다. 커맨드 라인을 더 많이 사용할수록, 그것이 업무의 상당 부분을 처리하는 가장 쉬운 방법임을 깨닫고 더 자주 돌아오게 될 것입니다.\n쉘 입문자이든, 쉘 프로그래밍이 데이터 과학에 얼마나 적합한지 깊이 생각해보지 않은 분이든, 이 책은 여러분이 소중히 여기게 될 책이 될 것입니다.\nJeroen은 훌륭한 스승이며, 그가 다루는 내용은 귀중합니다.\n—팀 오라일리(Tim O’Reilly)\n2021년 5월\n","code":""},{"path":"서문.html","id":"서문","chapter":"서문","heading":"서문","text":"데이터 과학은 일하기에 정말 흥미로운 분야입니다.\n또한 여전히 상대적으로 젊은 분야이기도 하죠.\n불행하게도 많은 사람들과 기업들이 데이터 과학이 제기하는 문제들을 해결하려면 새로운 기술이 필요하다고 믿습니다.\n하지만 이 책에서 보여주듯이, 많은 일들이 커맨드 라인을 사용함으로써 해결될 수 있으며, 때로는 훨씬 더 효율적인 방식으로 이루어집니다.박사 과정 동안 저는 서서히 마이크로소프트 윈도우에서 리눅스로 사용 환경을 옮겼습니다.\n이 전환이 처음에는 조금 두려웠기 때문에, 두 운영체제를 나란히 설치하는 방식(듀얼 부트라고 알려진)으로 시작했습니다.\n결국 윈도우와 리눅스를 왔다 갔다 하려는 욕구는 사라졌고, 어느 시점에는 나만의 맞춤형 리눅스 머신을 처음부터 빌드할 수 있는 아치 리눅스(Arch Linux)를 만지작거리기도 했습니다.\n주어지는 것은 오직 커맨드 라인뿐이며, 그것으로 무엇을 할지는 여러분에게 달려 있습니다.\n필요에 의해 저는 빠르게 커맨드 라인 사용에 익숙해졌습니다.\n결국 여가 시간이 더 소중해지면서, 사용하기 쉽고 커뮤니티가 큰 우분투(Ubuntu)라는 리눅스 배포판에 정착하게 되었습니다.\n하지만 여전히 제가 대부분의 시간을 보내는 곳은 커맨드 라인입니다.사실 제가 커맨드 라인이 단순히 소프트웨어를 설치하고, 시스템을 설정하고, 파일을 검색하는 용도만이 아니라는 것을 깨달은 지는 그리 오래되지 않았습니다.\n저는 cut, sort, sed와 같은 도구들을 배우기 시작했습니다.\n이것들은 데이터를 입력받아 무언가를 수행하고 결과를 출력하는 커맨드 라인 도구들의 예시입니다.\n우분투에는 이런 도구들이 꽤 많이 포함되어 있습니다.\n이 작은 도구들을 결합했을 때의 잠재력을 이해하고 나자, 저는 완전히 매료되었습니다.박사 학위를 받은 후 데이터 과학자가 되었을 때, 저는 가능한 한 이 방식을 데이터 과학에 사용하고 싶었습니다.\nxml2json, jq, json2csv와 같은 몇 가지 새로운 오픈 소스 커맨드 라인 도구들 덕분에 웹사이트 스크래핑이나 대량의 JSON 데이터 처리와 같은 작업에도 커맨드 라인을 사용할 수 있었습니다.2013년 9월, 저는 데이터 과학을 위한 7가지 커맨드 라인 도구라는 제목의 블로그 포스트를 작성하기로 결심했습니다.\n놀랍게도 이 블로그 포스트는 꽤 많은 관심을 받았고, 다른 커맨드 라인 도구들에 대한 수많은 제안을 받았습니다.\n저는 이 블로그 포스트를 책으로 만들 수 있을지 궁금해지기 시작했습니다.\n약 10개월 후, 많은 유능한 분들의 도움(감사의 글 참조)으로 그 대답이 ’예’가 되어 기뻤습니다.제가 이런 개인적인 이야기를 공유하는 것은 이 책이 어떻게 탄생했는지 여러분이 알아야 한다고 생각해서라기보다, 저 역시 커맨드 라인을 배워야만 했다는 사실을 알려드리고 싶기 때문입니다.\n커맨드 라인은 그래픽 사용자 인터페이스(GUI)를 사용하는 것과 매우 다르기 때문에 처음에는 무섭게 느껴질 수 있습니다.\n하지만 제가 배울 수 있었다면 여러분도 할 수 있습니다.\n현재 사용하는 운영체제가 무엇이든, 현재 데이터를 어떻게 다루든 관계없이 이 책을 읽고 나면 커맨드 라인에서 데이터 과학을 할 수 있게 될 것입니다.\n이미 커맨드 라인에 익숙하거나 쉘 스크립트로 꿈을 꿀 정도인 분들이라도, 다음 데이터 과학 프로젝트에 사용할 만한 흥미로운 팁이나 도구를 몇 가지 발견하게 될 것입니다.","code":""},{"path":"서문.html","id":"이-책에서-기대할-수-있는-것","chapter":"서문","heading":"이 책에서 기대할 수 있는 것","text":"이 책에서 우리는 데이터를 획득(obtain), 정제(scrub), 탐색(explore), 모델링(model)할 것입니다. 그것도 아주 많이요.\n이 책은 이러한 데이터 과학 작업 자체를 어떻게 하면 더 잘 할 수 있는지에 대한 책은 아닙니다.\n예를 들어 어떤 통계 검정을 언제 적용해야 하는지나 데이터를 어떻게 가장 잘 시각화할 것인지를 논하는 훌륭한 리소스들은 이미 많이 있습니다.\n대신, 이 실무 중심의 책은 커맨드 라인에서 이러한 데이터 과학 작업을 수행하는 방법을 가르쳐줌으로써 여러분을 더 효율적이고 생산적이게 만드는 것을 목표로 합니다.이 책에서는 90개가 넘는 커맨드 라인 도구를 다루지만, 가장 중요한 것은 도구 그 자체가 아닙니다.\n어떤 도구들은 아주 오랫동안 존재해 왔고, 어떤 도구들은 더 나은 것으로 대체될 것입니다.\n여러분이 이 글을 읽는 동안에도 새로운 커맨드 라인 도구들이 만들어지고 있습니다.\n지난 몇 년 동안 저는 놀라운 커맨드 라인 도구들을 많이 발견했습니다.\n안타깝게도 그중 일부는 너무 늦게 발견되어 이 책에 포함되지 못했습니다.\n요컨대, 커맨드 라인 도구는 생겨나고 사라집니다.\n하지만 그것으로 괜찮습니다.가장 중요한 것은 도구, 파이프, 그리고 데이터를 다루는 그 밑바탕의 아이디어입니다.\n대부분의 커맨드 라인 도구는 한 가지 일을 하고 그것을 아주 잘 수행합니다.\n이것은 책의 여러 곳에서 등장하는 유닉스 철학의 일부이기도 합니다.\n일단 커맨드 라인에 익숙해지고, 도구들을 결합하는 방법을 알고, 심지어 새로운 도구를 만들 수 있게 된다면, 여러분은 매우 귀중한 기술을 갖게 된 것입니다.","code":""},{"path":"서문.html","id":"판의-변경-사항","chapter":"서문","heading":"2판의 변경 사항","text":"기술로서의 커맨드 라인과 그 작동 방식은 시대를 초월하지만, 1판에서 다루었던 일부 도구들은 더 새로운 도구들로 대체되거나(csvkit은 대개 xsv로 대체되었습니다), 개발자에 의해 방치되었거나(drake), 혹은 최적이 아닌 선택이었던 것들(weka)이 있었습니다.\n2014년 10월 1판이 출간된 이후 저는 스스로의 경험이나 독자들의 유용한 피드백을 통해 많은 것을 배웠습니다.\n이 책이 두 주제의 교차점에 있는 틈새 분야를 다루고 있음에도 불구하고, 거의 매일 제가 받는 수많은 긍정적인 메시지들이 증명하듯 데이터 과학 커뮤니티의 꾸준한 관심이 이어지고 있습니다.\n1판을 업데이트함으로써 저는 이 책이 앞으로 최소 5년 동안은 유효한 가치를 지니기를 바랍니다.\n다음은 제가 수정한 주요 변경 사항 목록입니다.가능한 한 csvkit을 xsv로 교체했습니다. xsv는 CSV 파일을 다루는 훨씬 빠른 대안입니다.2.2절과 3.2절에서 VirtualBox 이미지를 Docker 이미지로 대체했습니다. Docker는 격리된 환경을 실행하는 데 있어 VirtualBox보다 빠르고 가벼운 방식입니다.HTML을 다루기 위해 scrape 대신 pup을 사용합니다. scrape은 제가 직접 만든 파이썬 도구입니다. pup은 훨씬 빠르고 기능이 많으며 설치가 더 쉽습니다.6장을 처음부터 다시 썼습니다. 프로젝트 관리를 위해 drake 대신 make를 사용합니다. drake는 더 이상 유지보수되지 않으며 make는 훨씬 성숙하고 개발자들에게 매우 인기가 있습니다.Rio를 rush로 대체했습니다. Rio는 제가 직접 만든 투박한 Bash 스크립트였습니다. rush는 커맨드 라인에서 R을 사용하는 훨씬 더 안정적이고 유연한 방식인 R 패키지입니다.9장에서 Weka와 BigML을 Vowpal Wabbit(vw)으로 대체했습니다. Weka는 오래되었고 커맨드 라인에서 사용하는 방식이 투박합니다. BigML은 제가 더 이상 의존하고 싶지 않은 상용 API입니다. Vowpal Wabbit은 Yahoo!에서 개발되어 현재 Microsoft에서 관리하는 매우 성숙한 머신러닝 도구입니다.10장은 파이썬, R, Apache Spark 등 기존 워크플로우에 커맨드 라인을 통합하는 방법을 다루는 완전히 새로운 장입니다. 1판에서는 커맨드 라인이 기존 워크플로우에 쉽게 통합될 수 있다는 점만 언급하고 깊이 다루지 못했습니다. 이 장이 그 부분을 해결해 줍니다.","code":""},{"path":"서문.html","id":"이-책을-읽는-방법","chapter":"서문","heading":"이 책을 읽는 방법","text":"일반적으로 이 책을 순서대로 읽으시길 권장합니다.\n한번 개념이나 커맨드 라인 도구가 소개되면, 이후의 장에서 그것을 활용할 가능성이 높습니다.\n예를 들어 9장에서는 8장에서 광범위하게 소개한 parallel을 집중적으로 사용합니다.데이터 과학은 프로그래밍, 데이터 시각화, 머신러닝 등 수많은 다른 분야와 교차하는 넓은 분야입니다.\n그 결과, 이 책은 많은 흥미로운 주제들을 다루지만 안타깝게도 모든 주제를 심도 있게 다루지는 못합니다.\n책 곳곳의 각 장 끝에는 더 깊게 탐구할 수 있는 제안들이 있습니다.\n책의 내용을 따라가기 위해 이 자료들을 반드시 읽어야 하는 것은 아니지만, 관심이 있다면 배울 것이 훨씬 더 많다는 것을 알게 될 것입니다.","code":""},{"path":"서문.html","id":"이-책의-대상-독자","chapter":"서문","heading":"이 책의 대상 독자","text":"이 책은 여러분에 대해 단 한 가지 가정만 합니다. 바로 여러분이 데이터를 다루는 분들이라는 점입니다.\n현재 어떤 프로그래밍 언어나 통계 컴퓨팅 환경을 사용하고 있는지는 중요하지 않습니다.\n이 책은 필요한 모든 개념을 기초부터 설명합니다.또한 여러분의 운영체제가 마이크로소프트 윈도우인지, macOS인지, 혹은 어떤 종류의 리눅스인지도 중요하지 않습니다.\n이 책은 설치하기 쉬운 가상 환경인 Docker 이미지를 함께 제공합니다.\n이를 통해 커맨드 라인 도구를 실행하고 이 책이 작성된 것과 동일한 환경에서 코드 예제를 따라 해볼 수 있습니다.\n수많은 커맨드 라인 도구와 그 의존성들을 어떻게 설치할지 고민하며 시간을 낭비할 필요가 없습니다.이 책에는 Bash, 파이썬, R로 작성된 일부 코드가 포함되어 있어 프로그래밍 경험이 있다면 도움이 되겠지만, 예제를 따라가는 데 있어 반드시 필요한 것은 아닙니다.","code":""},{"path":"서문.html","id":"이-책에-사용된-표기-규칙","chapter":"서문","heading":"이 책에 사용된 표기 규칙","text":"이 책에서는 다음과 같은 서체 규칙을 사용합니다.이탤릭(Italic)새로운 용어, URL, 디렉터리 이름, 파일 이름을 나타냅니다.\n새로운 용어, URL, 디렉터리 이름, 파일 이름을 나타냅니다.고정 폭(Constant width)코드와 명령어를 나타낼 때, 그리고 본문 내에서 커맨드 라인 도구와 옵션을 언급할 때 사용합니다.\n코드와 명령어를 나타낼 때, 그리고 본문 내에서 커맨드 라인 도구와 옵션을 언급할 때 사용합니다.고정 폭 굵게(Constant width bold)사용자가 직접 입력해야 하는 명령어 나 텍스트를 나타냅니다.\n사용자가 직접 입력해야 하는 명령어 나 텍스트를 나타냅니다.","code":""},{"path":"서문.html","id":"감사의-글","chapter":"서문","heading":"감사의 글","text":"","code":""},{"path":"서문.html","id":"판을-위한-감사의-글-2021","chapter":"서문","heading":"2판을 위한 감사의 글 (2021)","text":"1판이 출간된 지 7년이 지났습니다.\n그동안, 특히 지난 13개월 동안 많은 분들이 저를 도와주셨습니다.\n그분들이 없었다면 저는 결코 2판을 쓸 수 없었을 것입니다.저는 이번에도 O’Reilly에서 세 분의 멋진 에디터들과 함께하게 된 행운을 누렸습니다.\nSarah “Embrace deadline” Grey, Jess “Pedal metal” Haberman, 그리고 Kate “Let go” Galloway에게 감사의 마음을 전합니다. 그들의 미들 네임(별명)이 모든 것을 말해줍니다. 그들의 믿을 수 없는 도움 덕분에 마감 시한을 받아들이고(embrace deadlines), 중요한 순간에 가속 페달을 밟고(put pedal metal), 마침내 원고를 손에서 놓을(let go) 수 있었습니다.\n또한 O’Reilly와의 협업을 이토록 즐겁게 만들어준 그들의 동료들—Angela Rufino, Arthur Johnson, Cassandra Furtado, David Futato, Helen Monroe, Karen Montgomery, Kate Dullea, Kristen Brown, Marie Beaugureau, Marsee Henon, Nick Adams, Regina Wilkinson, Shannon Cutt, Shannon Turlington, Yasmina Greco—에게도 감사를 표합니다.코드를 실행하고 그 결과를 다시 붙여넣는 자동화된 프로세스(R Markdown과 Docker 덕분)가 있음에도 불구하고, 제가 저지른 실수의 숫자는 정말 놀라울 정도였습니다.\n이 숫자를 대폭 줄여주신 Aaditya Maruthi, Brian Eoff, Caitlin Hudon, Julia Silge, Mike Dewar, 그리고 Shane Reustle에게 감사드립니다.\n물론 남아 있는 모든 실수는 저의 책임입니다.Marc Canaleta는 특별한 감사를 받을 자격이 있습니다.\n1판이 나온 직후인 2014년 10월, Marc는 바르셀로나에 있는 Social Point 팀을 위해 “커맨드 라인에서 시작하는 데이터 과학”을 주제로 한 하루짜리 워크숍에 저를 초대했습니다.\n우리 둘 다 그토록 많은 워크숍이 이어지게 될 줄은 몰랐습니다.\n그 경험은 결국 제가 직접 회사인 Data Science Workshops를 차리는 원동력이 되었습니다.\n교육을 할 때마다 저는 새로운 것을 배웁니다.\n아마 학생들은 모르겠지만, 모든 학생 한 명 한 명이 이 책에 각기 다른 방식으로 영향을 미쳤습니다.\n그분들에게 감사의 인사를 전합니다.\n저는 아주 오랫동안 학생들을 가르치고 싶습니다.매혹적인 대화, 멋진 제안, 그리고 열정적인 풀 리퀘스트들.\n다음에 열거된 아낌없이 베풀어준 분들의 모든 기여에 깊이 감사드립니다.\nAdam Johnson, Andre Manook, Andrea Borruso, Andres Lowrie, Andrew Berisha, Andrew Gallant, Andrew Sanchez, Anicet Ebou, Anthony Egerton, Ben Isenhart, [.keep-together]#Chris Wiggins#, Chrys Wu, Dan Nguyen, Darryl Amatsetam, Dmitriy Rozhkov, Doug Needham, Edgar Manukyan, Erik Swan, Felienne Hermans, George Kampolis, Giel van Lankveld, Greg Wilson, Hay Kranen, Ioannis Cherouvim, Jake Hofman, Jannes Muenchow, Jared Lander, Jay Roaf, Jeffrey Perkel, Jim Hester, Joachim Hagege, Joel Grus, John Cook, John Sandall, Joost Helberg, Joost van Dijk, Joyce Robbins, Julian Hatwell, Karlo Guidoni, Karthik Ram, Lissa Hyacinth, Longhow Lam, Lui Pillmann, Lukas Schmid, Luke Reding, Maarten van Gompel, Martin Braun, Max Schelker, Max Shron, Nathan Furnal, Noah Chase, Oscar Chic, Paige Bailey, Peter Saalbrink, Rich Pauloo, Richard Groot, Rico Huijbers, Rob Doherty, Robbert van Vlijmen, Russell Scudder, Sylvain Lapoix, TJ Lavelle, Tan Long, Thomas Stone, Tim O’Reilly, Vincent Warmerdam, Yihui Xie.책 전체에 걸쳐, 특히 각주와 부록에는 수백 개의 이름이 등장합니다.\n이 이름들은 이 책의 근간이 된 수많은 도구, 서적 및 기타 리소스들의 저자들입니다.\n그 작업이 50년 전의 것이든 50일 전의 것이든, 그분들의 노고에 무한한 감사를 드립니다.무엇보다도, 무엇이 진정으로 중요한지 매일 일깨워주는 아내 Esther와 딸 Florien, 그리고 아들 Olivier에게 감사를 전합니다.\n약속컨대 3판을 쓰기 전까지는 몇 년의 시간 여유가 있을 것입니다.","code":""},{"path":"서문.html","id":"판을-위한-감사의-글-2014","chapter":"서문","heading":"1판을 위한 감사의 글 (2014)","text":"가장 먼저 저의 2013년 9월 블로그 포스트인 데이터 과학을 위한 7가지 커맨드 라인 도구가 책으로 확장될 수 있다고 믿어준 Mike Dewar와 Mike Loukides에게 감사를 전합니다.다양한 초안을 읽고, 모든 명령어를 세심하게 테스트하며 귀중한 피드백을 준 기술 검토자 Mike Dewar, Brian Eoff, Shane Reustle에게 특별한 감사를 전합니다.\n여러분의 노력으로 책이 크게 향상되었습니다. 남아 있는 오역이나 오류는 전적으로 저의 책임입니다.Ann Spencer, Julie Steele, 그리고 Marie Beaugureau라는 세 분의 놀라운 에디터와 함께 일할 수 있는 특권을 누렸습니다.\n저를 잘 이끌어 주시고 O’Reilly의 유능한 많은 분들과 훌륭한 가교 역할을 해주셔서 감사합니다.\n그분들 중에는 Laura Baldwin, Huguette Barriere, Sophia DeMartini, Yasmina Greco, Rachel James, Ben Lorica, Mike Loukides, Christopher Pappas가 포함됩니다.\n무대 뒤에서 일하느라 제가 직접 만나보지 못한 다른 많은 분들도 있습니다.\n그분들 모두 덕분에 O’Reilly와 함께 일하는 것이 진정한 즐거움이었습니다.이 책은 80개가 넘는 커맨드 라인 도구를 다룹니다. 말할 필요도 없이, 이 도구들이 없었다면 이 책은 존재조차 할 수 없었을 것입니다.\n따라서 이 도구들을 만들고 기여해주신 모든 저자분들께 깊은 감사를 드립니다. 전체 저자 목록은 안타깝게도 여기에 다 나열하기에 너무 길어 부록에 언급했습니다.\n특히 Aaron Crow, Jehiah Czebotar, Christoph Groskopf, Dima Kogan, Sergey Lisitsyn, Francisco J. Martin, 그리고 Ole Tange에게 그들의 놀라운 도구들에 대해 도움을 주셔서 감사를 전합니다.박사 과정 동안 저를 지도해주신 Eric Postma와 Jaap van den Herik 교수님께도 특별히 감사의 인사를 드립니다.\n5년이 넘는 시간 동안 교수님들은 제게 많은 교훈을 주셨습니다.\n기술 서적을 쓰는 것이 박사 학위 논문을 쓰는 것과는 꽤 다르지만, 그 교훈들 중 많은 부분이 지난 9개월 동안 매우 큰 도움이 되었습니다.마지막으로 YPlan 동료들, 친구들, 가족들, 그리고 무엇보다 적절한 순간에 저를 커맨드 라인에서 떼어내 준 아내 Esther에게 고마움을 전합니다.","code":""},{"path":"chapter-1-introduction.html","id":"chapter-1-introduction","chapter":"제 1 서론","heading":"제 1 서론","text":"이 책은 커맨드 라인에서 데이터 과학을 수행하는 것에 관한 책입니다.\n저의 목표는 커맨드 라인의 힘을 활용하는 방법을 가르쳐드림으로써 여러분을 더 효율적이고 생산적인 데이터 과학자로 만드는 것입니다.제목에 데이터 과학과 커맨드 라인이라는 두 용어가 함께 쓰인 것에 대해 설명이 필요할 것 같습니다.\n어떻게 50년도 더 된 기술2이 이제 겨우 몇 년밖에 되지 않은 분야에 도움이 될 수 있을까요?오늘날 데이터 과학자들은 파이썬, R, Julia, Apache Spark 등 흥미롭고 압도적인 기술과 프로그래밍 언어들 중에서 선택할 수 있습니다.\n여러분은 이미 이들 중 하나 이상에 익숙할지도 모릅니다.\n그렇다면 왜 여전히 데이터 과학을 위해 커맨드 라인에 관심을 가져야 할까요?\n커맨드 라인이 다른 기술이나 프로그래밍 언어들이 제공하지 못하는 무엇을 가지고 있을까요?이것들은 모두 타당한 질문입니다.\n이 첫 번째 장에서 저는 다음과 같은 방식으로 그 답을 제시하고자 합니다.\n먼저, 이 책의 근간이 될 데이터 과학의 실무적인 정의를 제공하겠습니다.\n둘째로, 커맨드 라인이 가진 다섯 가지 중요한 장점을 나열하겠습니다.\n이 장을 마칠 때쯤이면 여러분도 데이터 과학을 위해 커맨드 라인을 배울 가치가 충분하다는 점에 동의하시게 될 것입니다.","code":""},{"path":"chapter-1-introduction.html","id":"데이터-과학은-osemnawesome하다","chapter":"제 1 서론","heading":"1.1 데이터 과학은 OSEMN(Awesome)하다","text":"데이터 과학 분야는 아직 초기 단계에 있으며, 따라서 데이터 과학이 무엇을 포함하는지에 대한 다양한 정의가 존재합니다.\n이 책 전체에 걸쳐 저는3 제시한 매우 실무적인 정의를 따를 것입니다.\n그들은 데이터 과학을 다음과 같은 다섯 단계로 정의합니다: (1) 데이터 획득(Obtaining), (2) 데이터 정제(Scrubbing), (3) 데이터 탐색(Exploring), (4) 데이터 모델링(Modeling), (5) 데이터 해석(Interpreting).\n이 다섯 단계의 앞글자를 따서 OSEMN 모델이라고 부릅니다(발음은 awesome과 같습니다).\n이 정의는 이 책의 뼈대 역할을 하는데, 아래에서 설명할 5단계 ’데이터 해석’을 제외한 각 단계가 별도의 장으로 구성되어 있기 때문입니다.비록 이 다섯 단계가 선형적이고 점진적인 방식으로 논의되지만, 실제로는 각 단계 사이를 오가거나 여러 단계를 동시에 수행하는 것이 매우 일반적입니다.\nFigure ??은 데이터 과학을 수행하는 것이 반복적이고 비선형적인 과정임을 보여줍니다.\n예를 들어, 데이터를 모델링하고 결과를 확인한 후, 데이터셋의 특징(features)을 조정하기 위해 정제 단계로 되돌아가기로 결정할 수도 있습니다.아래에서는 각 단계가 무엇을 의미하는지 설명하겠습니다.","code":""},{"path":"chapter-1-introduction.html","id":"데이터-획득-obtaining-data","chapter":"제 1 서론","heading":"1.1.1 데이터 획득 (Obtaining Data)","text":"데이터 없이는 데이터 과학을 할 수 없습니다.\n따라서 첫 번째 단계는 데이터를 얻는 것입니다.\n이미 데이터를 보유하고 있는 행운아가 아니라면, 다음과 같은 작업 중 하나 이상을 수행해야 할 수도 있습니다.다른 위치(예: 웹페이지나 서버)에서 데이터 다운로드데이터베이스나 API(예: MySQL이나 Twitter)에서 데이터 쿼리다른 파일(예: HTML 파일이나 스프레드시트)에서 데이터 추출데이터 직접 생성(예: 센서 데이터 읽기나 설문 조사 수행)3장에서는 커맨드 라인을 사용하여 데이터를 획득하는 여러 가지 방법을 다룹니다.\n획득한 데이터는 대부분 텍스트, CSV, JSON, HTML, XML 형식 중 하나일 것입니다.\n다음 단계는 이 데이터를 정제하는 것입니다.","code":""},{"path":"chapter-1-introduction.html","id":"데이터-정제-scrubbing-data","chapter":"제 1 서론","heading":"1.1.2 데이터 정제 (Scrubbing Data)","text":"획득한 데이터에 결측치, 불일치, 오류, 이상한 문자 또는 불필요한 열이 포함되어 있는 경우는 매우 흔합니다.\n이 경우, 데이터로 무언가 흥미로운 작업을 하기 전에 데이터를 정제(scrub)하거나 깨끗하게(clean) 만들어야 합니다.\n일반적인 정제 작업은 다음과 같습니다.행 필터링특정 열 추출값 치환단어 추출결측치 및 중복 처리데이터를 한 형식에서 다른 형식으로 변환데이터 과학자로서 우리는 흥미로운 시각화와 통찰력 있는 모델을 만드는 것(3, 4단계)을 좋아하지만, 대개는 필요한 데이터를 먼저 획득하고 정제하는 데(1, 2단계) 훨씬 많은 노력을 들입니다.\nData Jujitsu에서4 “어떤 데이터 프로젝트에서든 업무의 80%는 데이터를 청소하는 일이다”라고 언급했습니다.\n5장에서는 커맨드 라인이 이러한 데이터 정제 작업을 완수하는 데 어떻게 도움이 되는지 보여줍니다.","code":""},{"path":"chapter-1-introduction.html","id":"데이터-탐색-exploring-data","chapter":"제 1 서론","heading":"1.1.3 데이터 탐색 (Exploring Data)","text":"데이터를 정제했다면 이제 탐색할 준비가 되었습니다.\n탐색을 통해 여러분은 데이터를 진정으로 알게 되므로 이 단계부터 흥미진진해집니다.\n7장에서는 커맨드 라인을 사용하여 다음 작업을 수행하는 방법을 보여드립니다.데이터 살펴보기데이터에서 통계량 도출통찰력 있는 시각화 생성7장에서 소개되는 커맨드 라인 도구에는 csvstat5과 rush6가 포함됩니다.","code":""},{"path":"chapter-1-introduction.html","id":"데이터-모델링-modeling-data","chapter":"제 1 서론","heading":"1.1.4 데이터 모델링 (Modeling Data)","text":"데이터를 설명하거나 앞으로 일어날 일을 예측하고 싶다면, 데이터에 대한 통계 모델을 만들고 싶을 것입니다.\n모델을 만드는 기법으로는 클러스터링, 분류, 회귀, 차원 축소 등이 있습니다.\n커맨드 라인은 새로운 유형의 모델을 처음부터 프로그래밍하기에는 적합하지 않습니다.\n하지만 커맨드 라인에서 모델을 빌드하는 것은 매우 유용합니다.\n9장에서는 로컬에서 모델을 빌드하거나 API를 활용하여 클라우드에서 연산을 수행하는 여러 커맨드 라인 도구를 소개하겠습니다.","code":""},{"path":"chapter-1-introduction.html","id":"데이터-해석-interpreting-data","chapter":"제 1 서론","heading":"1.1.5 데이터 해석 (Interpreting Data)","text":"OSEMN 모델의 마지막이자 어쩌면 가장 중요한 단계는 데이터를 해석하는 것입니다.\n이 단계는 다음을 포함합니다.데이터에서 결론 도출결과의 의미 평가결과 공유 및 커뮤니케이션솔직히 말해서, 이 단계에서 컴퓨터의 역할은 거의 없으며 커맨드 라인도 딱히 개입할 여지가 없습니다.\n일단 이 단계에 도달했다면, 그다음은 여러분에게 달려 있습니다.\n이 장은 OSEMN 모델 중 유일하게 별도의 장이 없는 단계입니다.\n대신7 명저 Thinking Data를 참고하시길 권합니다.","code":""},{"path":"chapter-1-introduction.html","id":"간주intermezzo-장들","chapter":"제 1 서론","heading":"1.2 간주(Intermezzo) 장들","text":"OSEMN 단계들을 다루는 장들 외에도 네 개의 간주(Intermezzo) 장이 있습니다.\n각 장은 데이터 과학에 관한 보다 일반적인 주제와 이를 위해 커맨드 라인이 어떻게 사용되는지를 논의합니다.\n이 주제들은 데이터 과학 프로세스의 어떤 단계에도 적용될 수 있습니다.4장에서는 커맨드 라인을 위한 재사용 가능한 도구를 만드는 방법을 다룹니다.\n이런 개인용 도구는 커맨드 라인에 직접 입력했던 긴 명령어나 파이썬, R 등으로 작성했던 기존 코드에서 만들어질 수 있습니다.\n직접 도구를 만들 수 있게 되면 효율성과 생산성이 크게 향상됩니다.커맨드 라인은 데이터 과학을 위한 대화형 환경이기 때문에 전체 워크플로우를 추적하는 것이 어려울 수 있습니다.\n6장에서는 작업과 작업 간의 의존성 관점에서 데이터 과학 워크플로우를 정의할 수 있게 해주는 make라는 커맨드 라인 도구를 소개합니다.\n이 도구는 여러분뿐만 아니라 동료들의 워크플로우 재현성(reproducibility)을 높여줍니다.8장에서는 명령어와 도구들을 병렬로 실행하여 속도를 높이는 방법을 설명합니다.\nGNU Parallel8이라는 커맨드 라인 도구를 사용하면, 매우 큰 데이터셋에 도구들을 적용하고 여러 코어 또는 원격 머신에서 실행할 수 있습니다.10장에서는 R, RStudio, 파이썬, Jupyter Notebook, 심지어 Apache Spark와 같은 다른 환경과 프로그래밍 언어에서 커맨드 라인의 힘을 활용하는 방법을 다룹니다.","code":""},{"path":"chapter-1-introduction.html","id":"커맨드-라인이란-무엇인가","chapter":"제 1 서론","heading":"1.3 커맨드 라인이란 무엇인가?","text":"데이터 과학을 위해 왜 커맨드 라인을 사용해야 하는지 논의하기 전에, 커맨드 라인이 실제로 어떻게 생겼는지 잠시 살펴보겠습니다(이미 익숙하실 수도 있습니다).\nFigure ??과 Figure ??은 각각 macOS와 우분투에서 기본적으로 나타나는 커맨드 라인의 스크린샷입니다.\n우분투는 GNU/리눅스의 특정 배포판이며, 이 책에서 제가 사용할 환경입니다.두 스크린샷에 보이는 창을 터미널(terminal)이라고 부릅니다.\n이것은 쉘(shell)과 상호작용할 수 있게 해주는 프로그램입니다.\n그 쉘이 제가 입력한 명령어들을 실행합니다.\n2장에서 이 두 용어에 대해 더 자세히 설명하겠습니다.명령어를 입력하는 것은 그래픽 사용자 인터페이스(GUI)를 통해 컴퓨터와 상호작용하는 것과는 매우 다른 방식입니다.\n주로 마이크로소프트 엑셀 등으로 데이터를 처리하는 데 익숙하다면 이 방식이 처음에는 위협적으로 느껴질 수 있습니다.\n두려워하지 마세요.\n커맨드 라인 작업에 매우 빠르게 익숙해질 것이라는 제 말을 믿으셔도 좋습니다.이 책에서 제가 입력하는 명령어와 그 결과물은 텍스트로 표시됩니다.\n예를 들어, 두 스크린샷의 터미널 내용은 다음과 같이 보일 것입니다.또한 각 명령어 앞에 달러 표시($)가 붙어 있는 것을 보실 수 있습니다.\n이를 프롬프트(prompt)라고 부릅니다.\n스크린샷의 프롬프트는 사용자 이름, 날짜, 펭귄 등 더 많은 정보를 보여주었습니다.\n예제에서는 달러 표시만 보여주는 것이 관례인데, 그 이유는 프롬프트가 (1) 세션 중에 변경될 수 있고(디렉터리를 이동할 때), (2) 사용자가 커스터마이징할 수 있으며(예: 시간이나 현재 작업 중인 git9 브랜치를 표시할 수도 있음), (3) 명령어 자체와는 무관하기 때문입니다.다음 장에서는 필수적인 커맨드 라인 개념들에 대해 훨씬 더 많이 설명하겠습니다.\n이제 왜 데이터 과학을 위해 커맨드 라인을 배워야 하는지 먼저 알아볼 차례입니다.","code":"whoami\ndate\necho 'The command line is awesome!' | cowsay -f tux"},{"path":"chapter-1-introduction.html","id":"왜-커맨드-라인에서-데이터-과학을-하는가","chapter":"제 1 서론","heading":"1.4 왜 커맨드 라인에서 데이터 과학을 하는가?","text":"커맨드 라인은 여러분을 더 효율적이고 생산적인 데이터 과학자로 만들어 줄 수 있는 수많은 장점을 가지고 있습니다.\n그 장점들을 크게 묶어보자면 커맨드 라인은 기민하고(agile), 보완적이며(augmenting), 확장 가능하고(scalable), 유연하며(extensible), 어디에나 존재(ubiquitous)합니다.\n각 장점에 대해 아래에서 자세히 설명하겠습니다.","code":""},{"path":"chapter-1-introduction.html","id":"커맨드-라인은-기민하다-agile","chapter":"제 1 서론","heading":"1.4.1 커맨드 라인은 기민하다 (Agile)","text":"커맨드 라인의 첫 번째 장점은 기민하게 움직일 수 있게 해준다는 점입니다.\n데이터 과학은 매우 대화형이고 탐색적인 성격을 띠고 있으며, 여러분의 작업 환경도 이를 허용해야 합니다.\n커맨드 라인은 두 가지 수단을 통해 이를 달성합니다.첫째, 커맨드 라인은 소위 반복적 실행 환경(REPL: read-eval-print-loop)을 제공합니다.\n이는 명령어를 입력하고 Enter를 누르면 명령이 즉시 평가된다는 의미입니다.\nREPL은 스크립트, 대형 프로그램, Hadoop 작업 등에 수반되는 ‘편집-컴파일-실행-디버그’ 주기보다 데이터 과학을 수행하기에 훨씬 편리한 경우가 많습니다.\n명령어는 즉시 실행되고, 마음대로 중단할 수 있으며, 빠르게 수정할 수 있습니다.\n이러한 짧은 반복 주기는 데이터를 마음껏 다루어 볼 수 있게 해줍니다.둘째, 커맨드 라인은 파일 시스템과 매우 밀접해 있습니다.\n데이터가 데이터 과학의 주재료인 만큼, 데이터셋이 포함된 파일들을 쉽게 다룰 수 있는 것은 매우 중요합니다.\n커맨드 라인은 이를 위한 수많은 편리한 도구들을 제공합니다.","code":""},{"path":"chapter-1-introduction.html","id":"커맨드-라인은-보완적이다-augmenting","chapter":"제 1 서론","heading":"1.4.2 커맨드 라인은 보완적이다 (Augmenting)","text":"커맨드 라인은 다른 기술들과 잘 통합됩니다.\n여러분의 데이터 과학 워크플로우가 현재 어떤 기술(R, 파이썬, 엑셀 등)을 포함하고 있든, 제가 그 워크플로우를 버리라고 제안하는 것이 아님을 알아주셨으면 합니다.\n대신 커맨드 라인을 여러분이 현재 사용하는 기술들을 증폭시켜 주는 보완적인 기술로 생각해보세요.\n이는 세 가지 방식으로 이루어질 수 있습니다.첫째, 커맨드 라인은 서로 다른 많은 데이터 과학 도구들 사이의 접착제 역할을 할 수 있습니다.\n도구들을 연결하는 한 가지 방법은 첫 번째 도구의 출력을 두 번째 도구의 입력으로 연결하는 것입니다.\n2장에서 이것이 어떻게 작동하는지 설명합니다.둘째, 여러분의 작업 환경 내에서 작업의 일부를 커맨드 라인에 위임할 수 있습니다.\n예를 들어 파이썬, R, Apache Spark에서는 커맨드 라인 도구를 실행하고 그 결과를 캡처할 수 있습니다.\n10장에서 예제와 함께 이를 보여드립니다.셋째, 여러분의 코드(예: 파이썬이나 R 스크립트)를 재사용 가능한 커맨드 라인 도구로 변환할 수 있습니다.\n그렇게 되면 그 도구가 어떤 언어로 작성되었는지는 더 이상 중요하지 않게 됩니다.\n이제 커맨드 라인에서 직접 사용하거나, 앞서 언급한 대로 커맨드 라인과 통합되는 어떤 환경에서도 해당 도구를 사용할 수 있습니다.\n4장에서 그 방법을 설명합니다.결국 모든 기술에는 강점과 약점이 있으므로, 여러 가지를 알고 상황에 가장 적합한 것을 골라 쓰는 것이 좋습니다.\n때로는 그것이 R일 수도 있고, 때로는 커맨드 라인일 수도 있으며, 때로는 펜과 종이일 수도 있습니다.\n이 책을 마칠 때쯤이면 여러분은 언제 커맨드 라인을 사용해야 할지, 그리고 언제 즐겨 쓰는 프로그래밍 언어나 통계 컴퓨팅 환경을 계속 사용하는 것이 나을지에 대해 확고한 이해를 갖게 될 것입니다.","code":""},{"path":"chapter-1-introduction.html","id":"커맨드-라인은-확장-가능하다-scalable","chapter":"제 1 서론","heading":"1.4.3 커맨드 라인은 확장 가능하다 (Scalable)","text":"앞서 말씀드렸듯이 커맨드 라인에서 작업하는 것은 GUI를 사용하는 것과 매우 다릅니다.\n커맨드 라인에서는 타이핑을 통해 무언가를 하는 반면, GUI에서는 마우스로 가리키고 클릭함으로써 작업을 수행합니다.커맨드 라인에서 수동으로 입력하는 모든 것은 스크립트와 도구를 통해 자동화될 수 있습니다.\n덕분에 실수를 했거나, 입력 데이터가 바뀌었거나, 동료가 같은 분석을 수행하고 싶어 할 때 명령어를 다시 실행하기가 매우 쉬워집니다.\n게다가 명령어는 특정 간격으로, 원격 서버에서, 그리고 데이터의 수많은 조각에 대해 병렬로 실행될 수 있습니다(8장에서 더 자세히 다룹니다).커맨드 라인은 자동화가 가능하기 때문에 확장성과 반복성을 갖게 됩니다.\n가리키고 클릭하는 동작을 자동화하는 것은 쉽지 않으며, 이 때문에 GUI는 확장 가능하고 반복적인 데이터 과학을 하기에 덜 적합한 환경입니다.","code":""},{"path":"chapter-1-introduction.html","id":"커맨드-라인은-유연하다-extensible","chapter":"제 1 서론","heading":"1.4.4 커맨드 라인은 유연하다 (Extensible)","text":"커맨드 라인 자체는 50년도 더 전에 발명되었습니다.\n핵심 기능은 대체로 변하지 않았지만, 커맨드 라인의 핵심 일꾼들인 도구들은 매일같이 개발되고 있습니다.커맨드 라인 그 자체는 언어에 종속되지 않습니다(language-agnostic).\n덕분에 커맨드 라인 도구들은 수많은 서로 다른 프로그래밍 언어로 작성될 수 있습니다.\n오픈 소스 커뮤니티는 우리가 데이터 과학을 위해 사용할 수 있는 수많은 무료 고품질 커맨드 라인 도구들을 내놓고 있습니다.이러한 커맨드 라인 도구들은 서로 협력할 수 있으며, 이는 커맨드 라인을 매우 유연하게 만듭니다.\n또한 여러분만의 도구를 만들 수도 있어 커맨드 라인의 실질적인 기능을 확장할 수 있습니다.","code":""},{"path":"chapter-1-introduction.html","id":"커맨드-라인은-어디에나-존재한다-ubiquitous","chapter":"제 1 서론","heading":"1.4.5 커맨드 라인은 어디에나 존재한다 (Ubiquitous)","text":"커맨드 라인은 우분투 리눅스와 macOS를 포함한 모든 유닉스 계열 운영체제에 포함되어 있어 많은 곳에서 찾아볼 수 있습니다.\n게다가 전 세계 상위 500대 슈퍼컴퓨터의 100%가 리눅스를 사용하고 있습니다.10\n따라서 혹시라도 그런 슈퍼컴퓨터 중 하나를 만져볼 기회가 생긴다면(혹은 쥬라기 공원에서 문 잠금 장치가 작동하지 않는 상황에 처한다면), 커맨드 라인 사용법을 잘 알고 있는 편이 좋을 것입니다!하지만 리눅스는 슈퍼컴퓨터에서만 돌아가는 것이 아닙니다.\n서버, 노트북, 그리고 임베디드 시스템에서도 돌아갑니다.\n요즘 많은 기업들이 클라우드 컴퓨팅을 제공하며, 즉석에서 새로운 머신을 쉽게 실행할 수 있습니다.\n그런 머신(또는 일반적인 서버)에 접속하게 된다면, 거의 확실하게 커맨드 라인을 마주하게 될 것입니다.또한 커맨드 라인이 단순히 일시적인 유행이 아니라는 점에 주목하는 것도 중요합니다.\n이 기술은 50년 넘게 존재해 왔으며, 앞으로 50년 동안도 계속될 것이라고 확신합니다.\n따라서 (데이터 과학을 위해서든 일반적인 용도를 위해서든) 커맨드 라인 사용법을 배우는 것은 투자할 만한 가치가 있는 일입니다.","code":""},{"path":"chapter-1-introduction.html","id":"요약","chapter":"제 1 서론","heading":"1.5 요약","text":"이 장에서는 책 전체의 가이드로 사용할 데이터 과학을 위한 OSEMN 모델을 소개했습니다.\n유닉스 커맨드 라인에 대한 몇 가지 배경 지식을 제공했으며, 커맨드 라인이 데이터 과학을 수행하기에 적합한 환경임을 납득해 주셨기를 바랍니다.\n다음 장에서는 데이터셋과 도구들을 설치하고 기본적인 개념들을 설명하며 본격적으로 시작해보겠습니다.","code":""},{"path":"chapter-1-introduction.html","id":"더-읽을거리","chapter":"제 1 서론","heading":"1.6 더 읽을거리","text":"브라이언 커니핸(Brian W. Kernighan)의 저서 UNIX: History Memoir는 유닉스가 무엇인지, 어떻게 개발되었는지, 그리고 왜 중요한지에 대한 이야기를 들려줍니다.2018년 런던 Strata에서 저는 50 Reasons Learn Shell Data Science라는 제목으로 발표했습니다. 더 많은 설득이 필요하다면 슬라이드를 읽어보세요.Max Shron의 짧지만 알찬 책 Thinking Data는 ’어떻게’보다 ’왜’에 집중하여, 올바른 질문을 던지고 올바른 문제를 해결하는 데 도움이 될 데이터 과학 프로젝트 정의를 위한 프레임워크를 제공합니다.","code":""},{"path":"chapter-2-getting-started.html","id":"chapter-2-getting-started","chapter":"제 2 시작하기","heading":"제 2 시작하기","text":"이 장에서는 커맨드 라인에서 데이터 과학을 수행하기 위한 모든 전제 조건이 갖춰졌는지 확인할 것입니다.\n전제 조건은 세 부분으로 나뉩니다. (1) 이 책에서 사용하는 것과 동일한 데이터셋 확보, (2) 이 책 전체에서 사용하는 모든 커맨드 라인 도구가 포함된 적절한 환경 구축, (3) 커맨드 라인을 사용할 때 작용하는 필수적인 개념들에 대한 이해입니다.먼저 데이터셋을 다운로드하는 방법을 설명합니다.\n둘째로, 필요한 모든 커맨드 라인 도구를 포함하고 있는 우분투 리눅스 기반 가상 환경인 Docker 이미지를 설치하는 방법을 설명합니다.\n그다음에는 예제를 통해 필수적인 유닉스 개념들을 살펴볼 것입니다.이 장을 마칠 때쯤이면 여러분은 데이터 과학의 첫 번째 단계인 데이터 획득을 계속하는 데 필요한 모든 것을 갖추게 될 것입니다.","code":""},{"path":"chapter-2-getting-started.html","id":"데이터-확보하기","chapter":"제 2 시작하기","heading":"2.1 데이터 확보하기","text":"이 책에서 사용하는 데이터셋은 다음과 같이 다운로드할 수 있습니다.https://www.datascienceatthecommandline.com/2e/data.zip 에서 ZIP 파일을 다운로드합니다.새 디렉터리를 만듭니다. 이름은 원하는 대로 지어도 되지만, 커맨드 라인에서 작업하기 편하도록 소문자, 숫자, 하이픈(-) 또는 언더스코어(_)만 사용하는 것을 권장합니다. 예를 들어 dsatcl2e-data와 같이 지을 수 있습니다. 이 디렉터리의 위치를 기억해 두세요.ZIP 파일을 해당 디렉터리로 옮기고 압축을 풉니다.이제 이 디렉터리에는 각 장별로 하나의 하위 디렉터리가 들어 있게 됩니다.다음 절에서는 이 데이터를 다루기 위한 모든 커맨드 라인 도구가 포함된 환경을 설치하는 방법을 설명합니다.","code":""},{"path":"chapter-2-getting-started.html","id":"docker-image","chapter":"제 2 시작하기","heading":"2.2 Docker 이미지 설치하기","text":"이 책에서는 매우 다양한 커맨드 라인 도구를 사용합니다.\n유닉스에는 이미 많은 도구가 기본으로 설치되어 있고, 관련 도구들을 포함하는 많은 패키지들도 제공됩니다.\n이러한 패키지들을 직접 설치하는 것이 그리 어렵지는 않습니다.\n하지만 패키지로 제공되지 않아 수동으로 복잡하게 설치해야 하는 도구들도 사용하게 될 것입니다.\n일일이 설치 과정을 거치지 않고 필요한 도구들을 한꺼번에 갖추기 위해, 여러분이 윈도우, macOS, 리눅스 중 어떤 운영체제를 사용하든 이 책을 위해 특별히 제작된 Docker 이미지를 설치하시길 강력히 권장합니다.Docker 이미지는 하나 이상의 애플리케이션과 그에 필요한 모든 의존성을 하나로 묶은 것입니다.\nDocker 컨테이너는 이미지를 실행하는 격리된 환경입니다.\n이후에 수행할 것처럼 docker 커맨드 라인 도구를 사용하거나 Docker GUI를 통해 이미지와 컨테이너를 관리할 수 있습니다.\n어떤 면에서 Docker 컨테이너는 가상 머신과 비슷하지만, 자원을 훨씬 적게 사용합니다.\n이 장의 끝에서 Docker에 대해 더 배울 수 있는 자료들을 제안하겠습니다.Docker 이미지를 설치하려면 먼저 Docker 웹사이트에서 Docker를 다운로드하여 설치해야 합니다.\nDocker가 설치되면 터미널(또는 명령 프롬프트)에서 다음 명령어를 입력하여 Docker 이미지를 다운로드합니다(달러 표시는 입력하지 마세요).Docker 이미지는 다음과 같이 실행할 수 있습니다.이제 필요한 모든 커맨드 라인 도구가 설치된 격리된 환경(Docker 컨테이너라고 불립니다) 내부에 들어와 있습니다.\n다음 명령어를 입력했을 때 열정적인 소 한 마리가 나타난다면 모든 것이 제대로 작동하는 것입니다.컨테이너 안팎으로 데이터를 주고받으려면 볼륨(volume)을 추가하여 로컬 디렉터리를 컨테이너 내부의 디렉터리에 매핑할 수 있습니다.\n먼저 새로운 디렉터리를 만들고 해당 디렉터리로 이동한 뒤, macOS나 리눅스라면 다음 명령어를 실행하는 것을 추천합니다.윈도우에서 명령 프롬프트(cmd)를 사용 중이라면 다음과 같이 입력합니다.윈도우 PowerShell을 사용 중이라면 다음과 같이 입력합니다.위의 명령어에서 -v 옵션은 docker에게 현재 디렉터리를 컨테이너 내부의 /data 디렉터리로 매핑하도록 지시합니다. 따라서 이곳이 Docker 컨테이너 안팎으로 데이터를 주고받는 통로가 됩니다.작업을 마쳤다면 exit를 입력하여 Docker 컨테이너를 종료할 수 있습니다.","code":"docker pull datasciencetoolbox/dsatcl2e#! enter=FALSEdocker run --rm -it datasciencetoolbox/dsatcl2e#! enter=FALSEcowsay \"Let's moove\\!\"docker run --rm -it -v \"$(pwd)\":/data datasciencetoolbox/dsatcl2e#! enter=FALSEC:\\> docker run --rm -it -v \"%cd%\":/data datasciencetoolbox/dsatcl2ePS C:\\> docker run --rm -it -v ${PWD}:/data datasciencetoolbox/dsatcl2e"},{"path":"chapter-2-getting-started.html","id":"essential-concepts","chapter":"제 2 시작하기","heading":"2.3 필수 유닉스 개념","text":"1장에서 커맨드 라인이 무엇인지 잠시 보여드렸습니다.\n이제 여러분은 Docker 이미지를 실행하고 있으니, 본격적으로 시작할 수 있습니다.\n이 절에서는 커맨드 라인에서 데이터 과학을 편안하게 수행하기 위해 꼭 알아야 할 몇 가지 개념과 도구들을 다룹니다.\n지금까지 주로 그래픽 사용자 인터페이스(GUI)로 작업해 오셨다면 상당한 변화로 느껴질 수 있습니다.\n하지만 걱정하지 마세요. 기초부터 시작해서 점진적으로 더 고급 주제로 나아갈 것입니다.","code":""},{"path":"chapter-2-getting-started.html","id":"환경-the-environment","chapter":"제 2 시작하기","heading":"2.3.1 환경 (The Environment)","text":"여러분은 이제 막 새로운 환경에 로그인했습니다.\n무언가를 시작하기 전에, 이 환경에 대해 전반적으로 이해하는 것이 좋습니다.\n환경은 대략 네 개의 계층으로 정의되며, 위에서 아래 방향으로 간단히 살펴보겠습니다.커맨드 라인 도구 (Command-line tools)가장 먼저 여러분이 직접 다루게 될 커맨드 라인 도구들이 있습니다.\n이 도구들은 해당 명령어를 입력하여 사용합니다.\n커맨드 라인 도구에는 여러 유형이 있으며, 이에 대해서는 다음 절에서 자세히 다루겠습니다.\n도구의 예로는 ls11, cat12, jq13 등이 있습니다.\n가장 먼저 여러분이 직접 다루게 될 커맨드 라인 도구들이 있습니다.\n이 도구들은 해당 명령어를 입력하여 사용합니다.\n커맨드 라인 도구에는 여러 유형이 있으며, 이에 대해서는 다음 절에서 자세히 다루겠습니다.\n도구의 예로는 ls11, cat12, jq13 등이 있습니다.터미널 (Terminal)두 번째 계층인 터미널은 명령어를 입력하는 애플리케이션입니다. 책에서 다음과 같은 텍스트를 보게 된다면:\nseq 3\n터미널에 seq 3이라고 입력하고 Enter를 누르면 됩니다.\n(커맨드 라인 도구 seq14는 보시다시피 일련의 숫자들을 생성합니다.) 달러 표시는 입력하지 않습니다.\n그것은 단지 터미널에 입력할 수 있는 명령어임을 알려주는 표시일 뿐입니다.\n이 달러 표시를 프롬프트(prompt)라고 부릅니다.\nseq 3 아래에 있는 텍스트는 해당 명령어의 출력 결과입니다.\n두 번째 계층인 터미널은 명령어를 입력하는 애플리케이션입니다. 책에서 다음과 같은 텍스트를 보게 된다면:터미널에 seq 3이라고 입력하고 Enter를 누르면 됩니다.\n(커맨드 라인 도구 seq14는 보시다시피 일련의 숫자들을 생성합니다.) 달러 표시는 입력하지 않습니다.\n그것은 단지 터미널에 입력할 수 있는 명령어임을 알려주는 표시일 뿐입니다.\n이 달러 표시를 프롬프트(prompt)라고 부릅니다.\nseq 3 아래에 있는 텍스트는 해당 명령어의 출력 결과입니다.쉘 (Shell)세 번째 계층은 쉘입니다. 명령어를 입력하고 Enter를 누르면 터미널은 그 명령어를 쉘로 보냅니다. 쉘은 명령어를 해석하는 프로그램입니다. 저는 Z 쉘(Zsh)을 사용하지만, Bash나 Fish와 같은 다른 쉘들도 많이 있습니다.\n세 번째 계층은 쉘입니다. 명령어를 입력하고 Enter를 누르면 터미널은 그 명령어를 쉘로 보냅니다. 쉘은 명령어를 해석하는 프로그램입니다. 저는 Z 쉘(Zsh)을 사용하지만, Bash나 Fish와 같은 다른 쉘들도 많이 있습니다.운영체제 (Operating system)네 번째 계층은 운영체제이며, 우리의 경우에는 GNU/리눅스입니다. 리눅스는 커널(kernel)의 이름으로, 운영체제의 심장과 같습니다. 커널은 CPU, 디스크 등 하드웨어와 직접 소통합니다. 또한 커널은 우리의 커맨드 라인 도구들을 실행합니다. GNU(GNU’s UNIX의 약자)는 기본적인 도구들의 모음을 의미합니다. 우리가 사용하는 Docker 이미지는 우분투라는 특정 GNU/리눅스 배포판을 기반으로 합니다.\n네 번째 계층은 운영체제이며, 우리의 경우에는 GNU/리눅스입니다. 리눅스는 커널(kernel)의 이름으로, 운영체제의 심장과 같습니다. 커널은 CPU, 디스크 등 하드웨어와 직접 소통합니다. 또한 커널은 우리의 커맨드 라인 도구들을 실행합니다. GNU(GNU’s UNIX의 약자)는 기본적인 도구들의 모음을 의미합니다. 우리가 사용하는 Docker 이미지는 우분투라는 특정 GNU/리눅스 배포판을 기반으로 합니다.","code":"seq 3"},{"path":"chapter-2-getting-started.html","id":"커맨드-라인-도구-실행하기","chapter":"제 2 시작하기","heading":"2.3.2 커맨드 라인 도구 실행하기","text":"이제 환경에 대한 기본적인 이해가 생겼으니, 직접 명령어를 실행해 볼 차례입니다.\n터미널에 다음을 입력하고(달러 표시 제외) Enter를 누르세요.여러분은 방금 하나의 커맨드 라인 도구를 포함하는 명령어를 실행했습니다.\npwd15 도구는 현재 여러분이 위치한 디렉터리의 이름을 출력합니다.\n로그인했을 때의 기본 위치는 홈 디렉터리입니다.Z 쉘 내장 명령어인 cd 도구를 사용하면 다른 디렉터리로 이동할 수 있습니다.➊ /data/ch02 디렉터리로 이동합니다.\n➋ 현재 디렉터리를 출력합니다.\n➌ 상위 디렉터리로 이동합니다.\n➍ 다시 현재 디렉터리를 출력합니다.\n➎ 하위 디렉터리 ch02로 이동합니다.cd 뒤에 오는 부분은 이동하고자 하는 디렉터리를 지정합니다.\n명령어 뒤에 오는 값들을 커맨드 라인 인자(arguments) 또는 옵션(options)이라고 부릅니다.\n마침표 두 개(..)는 상위 디렉터리를 의미합니다.\n참고로 마침표 하나(.)는 현재 디렉터리를 의미합니다.\ncd .은 아무런 효과가 없겠지만, 마침표 하나가 다른 곳에서 쓰이는 것을 보게 될 것입니다.\n다른 명령어를 시도해 봅시다.여기서는 head16에 세 개의 커맨드 라인 인자를 전달했습니다.\n첫 번째는 옵션입니다. 여기서는 짧은 옵션인 -n을 사용했습니다.\n가끔 짧은 옵션은 긴 형태의 변형을 가지기도 하는데, 이 경우 --lines가 됩니다.\n두 번째는 해당 옵션에 속하는 값입니다.\n세 번째는 파일 이름입니다.\n이 명령어는 /data/ch02/movies.txt 파일의 처음 세 줄을 출력합니다.","code":"pwdcd /data/ch02\npwd\ncd ..\npwd\ncd ch02head -n 3 movies.txt"},{"path":"chapter-2-getting-started.html","id":"다섯-가지-유형의-커맨드-라인-도구","chapter":"제 2 시작하기","heading":"2.3.3 다섯 가지 유형의 커맨드 라인 도구","text":"저는 ’커맨드 라인 도구’라는 용어를 많이 사용하지만, 지금까지 실제로 그것이 무엇을 의미하는지는 설명하지 않았습니다.\n저는 이 용어를 커맨드 라인에서 실행할 수 있는 모든 것을 가리키는 포괄적인 용어로 사용합니다(?? 참고).\n내부적으로 각 커맨드 라인 도구는 다음 다섯 가지 유형 중 하나에 속합니다.바이너리 실행 파일 (Binary executable)쉘 내장 명령어 (Shell builtin)해석되는 스크립트 (Interpreted script)쉘 함수 (Shell function)별칭 (Alias)각 유형 간의 차이를 아는 것이 좋습니다.\nDocker 이미지에 미리 설치된 커맨드 라인 도구들은 대부분 처음 두 가지 유형(바이너리 실행 파일과 쉘 내장 명령어)으로 구성됩니다.\n나머지 세 유형(해석되는 스크립트, 쉘 함수, 별칭)은 우리의 데이터 과학 도구 상자를 더욱 확장하고, 우리가 더 효율적이고 생산적인 데이터 과학자가 될 수 있게 해줍니다.바이너리 실행 파일 (Binary Executable)바이너리 실행 파일은 고전적인 의미의 프로그램입니다. 바이너리 실행 파일은 소스 코드를 기계어로 컴파일하여 만들어집니다. 즉, 텍스트 에디터로 파일을 열어도 내용을 읽을 수 없습니다.\n바이너리 실행 파일은 고전적인 의미의 프로그램입니다. 바이너리 실행 파일은 소스 코드를 기계어로 컴파일하여 만들어집니다. 즉, 텍스트 에디터로 파일을 열어도 내용을 읽을 수 없습니다.쉘 내장 명령어 (Shell Builtin)쉘 내장 명령어는 쉘(우리 환경에서는 Z 쉘 또는 zsh)이 제공하는 커맨드 라인 도구입니다. cd와 pwd가 그 예입니다. 내장 명령어는 쉘마다 다를 수 있습니다. 바이너리 실행 파일과 마찬가지로 내용을 쉽게 검사하거나 변경할 수 없습니다.\n쉘 내장 명령어는 쉘(우리 환경에서는 Z 쉘 또는 zsh)이 제공하는 커맨드 라인 도구입니다. cd와 pwd가 그 예입니다. 내장 명령어는 쉘마다 다를 수 있습니다. 바이너리 실행 파일과 마찬가지로 내용을 쉽게 검사하거나 변경할 수 없습니다.해석되는 스크립트 (Interpreted Script)해석되는 스크립트는 바이너리 실행 파일에 의해 실행되는 텍스트 파일입니다. 파이썬, R, Bash 스크립트 등이 예입니다. 해석되는 스크립트의 큰 장점은 직접 읽고 수정할 수 있다는 것입니다. 아래의 스크립트가 파이썬에 의해 해석되는 이유는 확장자가 .py이기 때문이 아니라, 스크립트의 첫 번째 줄이 이를 실행해야 할 바이너리를 정의하고 있기 때문입니다.\nbat fac.py\n이 스크립트는 매개변수로 전달한 정수의 팩토리얼을 계산합니다. 커맨드 라인에서 다음과 같이 호출할 수 있습니다.\n./fac.py 5\n4장에서는 해석되는 스크립트를 사용하여 재사용 가능한 커맨드 라인 도구를 만드는 방법을 아주 자세히 다룰 것입니다.\n해석되는 스크립트는 바이너리 실행 파일에 의해 실행되는 텍스트 파일입니다. 파이썬, R, Bash 스크립트 등이 예입니다. 해석되는 스크립트의 큰 장점은 직접 읽고 수정할 수 있다는 것입니다. 아래의 스크립트가 파이썬에 의해 해석되는 이유는 확장자가 .py이기 때문이 아니라, 스크립트의 첫 번째 줄이 이를 실행해야 할 바이너리를 정의하고 있기 때문입니다.이 스크립트는 매개변수로 전달한 정수의 팩토리얼을 계산합니다. 커맨드 라인에서 다음과 같이 호출할 수 있습니다.4장에서는 해석되는 스크립트를 사용하여 재사용 가능한 커맨드 라인 도구를 만드는 방법을 아주 자세히 다룰 것입니다.쉘 함수 (Shell Function)쉘 함수는 우리 환경의 경우 zsh에 의해 실행되는 함수입니다. 스크립트와 유사한 기능을 제공하지만, 대개 스크립트보다는 크기가 작습니다(꼭 그래야 하는 것은 아니지만요). 또한 더 개인적인 용도로 쓰이는 경향이 있습니다. 다음 명령어는 fac이라는 함수를 정의하는데, 앞서 본 파이썬 스크립트와 마찬가지로 전달된 정수의 팩토리얼을 계산합니다. 이 함수는 seq로 숫자 목록을 생성하고, paste17를 사용해 숫자들 사이에 *를 넣어 한 줄로 만든 뒤, 이를 계산하여 결과를 출력하는 bc18로 전달함으로써 작동합니다.\nfac() { (echo 1; seq $1) | paste -s -d\\* - | bc; }\nfac 5\nZ 쉘의 설정 파일인 ~/.zshrc는 쉘 함수를 정의하기에 좋은 곳입니다. 그렇게 하면 함수를 항상 사용할 수 있습니다.\n쉘 함수는 우리 환경의 경우 zsh에 의해 실행되는 함수입니다. 스크립트와 유사한 기능을 제공하지만, 대개 스크립트보다는 크기가 작습니다(꼭 그래야 하는 것은 아니지만요). 또한 더 개인적인 용도로 쓰이는 경향이 있습니다. 다음 명령어는 fac이라는 함수를 정의하는데, 앞서 본 파이썬 스크립트와 마찬가지로 전달된 정수의 팩토리얼을 계산합니다. 이 함수는 seq로 숫자 목록을 생성하고, paste17를 사용해 숫자들 사이에 *를 넣어 한 줄로 만든 뒤, 이를 계산하여 결과를 출력하는 bc18로 전달함으로써 작동합니다.Z 쉘의 설정 파일인 ~/.zshrc는 쉘 함수를 정의하기에 좋은 곳입니다. 그렇게 하면 함수를 항상 사용할 수 있습니다.별칭 (Alias)별칭은 매크로와 같습니다. 어떤 명령어를 항상 같은 매개변수와 함께(혹은 명령어의 일부를) 실행하게 된다면, 시간을 아끼기 위해 별칭을 정의할 수 있습니다. 별칭은 특정 명령어를 계속 오타 낼 때도 매우 유용합니다(Chris Wiggins가 관리하는 유용한 별칭 목록을 참고하세요). 다음 명령어는 그러한 별칭을 정의합니다.\nalias l='ls --color -lhF --group-directories-first'\nalias les=less\n이제 커맨드 라인에 다음과 같이 입력하면, 쉘은 발견한 각 별칭을 해당 값으로 대체합니다.\ncd /data\nl\ncd ch02\n별칭은 매개변수를 허용하지 않으므로 쉘 함수보다 단순합니다. 앞서 정의한 fac 함수는 매개변수 때문에 별칭으로 정의할 수 없었습니다. 그럼에도 별칭은 수많은 키 입력을 줄여줍니다. 쉘 함수와 마찬가지로 별칭도 대개 홈 디렉터리에 있는 .zshrc 파일에 정의합니다. 현재 정의된 모든 별칭을 보려면 인자 없이 alias를 실행해 보세요. 무엇이 보이나요?\n별칭은 매크로와 같습니다. 어떤 명령어를 항상 같은 매개변수와 함께(혹은 명령어의 일부를) 실행하게 된다면, 시간을 아끼기 위해 별칭을 정의할 수 있습니다. 별칭은 특정 명령어를 계속 오타 낼 때도 매우 유용합니다(Chris Wiggins가 관리하는 유용한 별칭 목록을 참고하세요). 다음 명령어는 그러한 별칭을 정의합니다.이제 커맨드 라인에 다음과 같이 입력하면, 쉘은 발견한 각 별칭을 해당 값으로 대체합니다.별칭은 매개변수를 허용하지 않으므로 쉘 함수보다 단순합니다. 앞서 정의한 fac 함수는 매개변수 때문에 별칭으로 정의할 수 없었습니다. 그럼에도 별칭은 수많은 키 입력을 줄여줍니다. 쉘 함수와 마찬가지로 별칭도 대개 홈 디렉터리에 있는 .zshrc 파일에 정의합니다. 현재 정의된 모든 별칭을 보려면 인자 없이 alias를 실행해 보세요. 무엇이 보이나요?이 책에서는 마지막 세 가지 유형인 해석되는 스크립트, 쉘 함수, 별칭에 주로 집중할 것입니다.\n이들은 쉽게 변경할 수 있기 때문입니다.\n커맨드 라인 도구의 목적은 여러분의 삶을 편하게 만들고, 여러분을 더 생산적이고 효율적인 데이터 과학자로 만드는 것입니다.\n명령어 type(그 자체로 쉘 내장 명령어입니다)을 사용하여 커맨드 라인 도구의 유형을 확인할 수 있습니다.type은 pwd에 대해 세 개의 커맨드 라인 도구를 반환합니다.\n이 경우, 여러분이 pwd를 입력하면 목록에서 가장 먼저 보고된 도구가 사용됩니다.\n다음 절에서는 커맨드 라인 도구들을 조합하는 방법을 살펴보겠습니다.","code":"bat fac.py./fac.py 5fac() { (echo 1; seq $1) | paste -s -d\\* - | bc; }\nfac 5alias l='ls --color -lhF --group-directories-first'\nalias les=lesscd /data\nl\ncd ch02type -a pwd\ntype -a cd\ntype -a fac\ntype -a l"},{"path":"chapter-2-getting-started.html","id":"combining-command-line-tools","chapter":"제 2 시작하기","heading":"2.3.4 커맨드 라인 도구 조합하기","text":"대부분의 커맨드 라인 도구는 유닉스 철학19을 따르기 때문에, 한 가지 작업만 아주 잘하도록 설계되어 있습니다.\n예를 들어, grep20 도구는 행을 필터링할 수 있고, wc21는 행의 개수를 셀 수 있으며, sort22는 행을 정렬할 수 있습니다.\n커맨드 라인의 진정한 힘은 작지만 강력한 이러한 도구들을 조합하는 능력에서 나옵니다.이러한 힘은 도구들 사이의 통신 스트림(communication streams)을 관리함으로써 가능해집니다.\n각 도구는 세 가지 표준 통신 스트림을 가집니다: 표준 입력(standard input), 표준 출력(standard output), 표준 오류(standard error).\n이들은 종종 stdin, stdout, stderr로 줄여서 부릅니다.표준 출력과 표준 오류는 모두 기본적으로 터미널로 연결되어 있어, 일반적인 결과와 오류 메시지가 모두 화면에 출력됩니다.\nFigure ??는 pwd와 rev23를 예로 들어 이를 설명합니다.\nrev를 실행하면 아무 일도 일어나지 않는 것을 볼 수 있습니다.\n그 이유는 rev가 입력을 기다리고 있기 때문이며, 기본적으로는 키보드로 입력하는 값들이 입력값이 됩니다.\n문장을 하나 입력하고 Enter를 눌러 보세요.\nrev는 즉시 여러분의 입력을 거꾸로 뒤집어 응답할 것입니다.\n입력 전송을 중단하려면 Ctrl-D를 누르면 되며, 그러면 rev가 멈춥니다.실제로 여러분은 키보드를 입력 소스로 쓰기보다는, 다른 도구가 생성한 출력물이나 파일의 내용을 입력으로 사용하게 될 것입니다.\n예를 들어 curl을 사용하여 루이스 캐럴의 이상한 나라의 앨리스를 다운로드하고, 그 결과를 다음 도구로 파이프(pipe)할 수 있습니다.\n(curl에 대해서는 3장에서 더 자세히 다룰 것입니다.)\n도구 연결은 파이프 연산자(|)를 사용하여 수행합니다.우리는 curl의 출력을 grep으로 파이프하여 특정 패턴으로 행을 필터링할 수 있습니다.\n목차에 나열된 장(chapter) 목록을 보고 싶다고 가정해 봅시다.\n다음과 같이 curl과 grep을 조합할 수 있습니다.그리고 이 책에 몇 개의 장이 있는지 알고 싶다면, 개수를 세는 데 탁월한 wc를 사용할 수 있습니다.➊ -l 옵션은 wc가 입력받은 행의 수만 출력하도록 지정합니다. 기본적으로는 글자 수와 단어 수도 함께 반환합니다.파이프 연결은 자동화된 복사 및 붙여넣기라고 생각해도 좋습니다.\n파이프 연산자를 사용해 도구들을 조합하는 요령을 터득하고 나면, 그 활용 가능성이 무궁무진하다는 것을 깨닫게 될 것입니다.","code":"curl -s \"https://www.gutenberg.org/files/11/11-0.txt\" | grep \" CHAPTER\"curl -s \"https://www.gutenberg.org/files/11/11-0.txt\" |\ngrep \" CHAPTER\" |\nwc -l"},{"path":"chapter-2-getting-started.html","id":"입력과-출력-리다이렉션-redirecting-input-and-output","chapter":"제 2 시작하기","heading":"2.3.5 입력과 출력 리다이렉션 (Redirecting Input and Output)","text":"한 도구의 출력을 다른 도구로 파이프하는 것 외에도, 파일로 저장할 수 있습니다.\n전체 경로를 지정하지 않으면 파일은 현재 디렉터리에 저장됩니다.\n이것을 출력 리다이렉션(output redirection)이라고 하며, 다음과 같이 작동합니다.여기서는 grep의 출력을 /data/ch02 디렉터리에 chapters.txt라는 이름의 파일로 저장합니다.\n파일이 아직 없으면 새로 생성됩니다. 이미 파일이 존재한다면 그 내용은 덮어쓰여집니다.\nFigure ??는 출력 리다이렉션의 개념을 보여줍니다.\n표준 오류는 여전히 터미널로 연결되어 있음에 유의하세요.출력을 기존 파일 내용 뒤에 덧붙이려면 >>를 사용합니다.echo 도구는 지정한 값을 출력합니다.\n-n 옵션은 출력 끝에 줄바꿈 문자를 붙이지 않도록 지정합니다.출력을 파일에 저장하는 것은 중간 분석 결과를 보관했다가 나중에 분석을 이어가고자 할 때 유용합니다.\ngreeting.txt 파일의 내용을 다시 사용하려면, 파일을 읽어서 출력하는 cat을 사용하면 됩니다.➊ -w 옵션은 wc가 단어 수만 세도록 합니다.같은 결과를 작다 기호(<)를 사용해서도 얻을 수 있습니다.이 방식은 별도의 프로세스를 실행하지 않고 파일을 wc의 표준 입력으로 직접 전달합니다24.\nFigure ??는 이 두 가지 방식의 차이를 보여줍니다.\n최종 결과는 동일합니다.많은 커맨드 라인 도구들과 마찬가지로, wc는 하나 이상의 파일 이름을 인자로 받을 수 있습니다.\n예를 들어 다음과 같습니다.이 경우 wc는 각 파일의 이름도 함께 출력합니다.어떤 도구의 출력이든 /dev/null이라는 특수 파일로 리다이렉션하여 출력을 억제할 수 있습니다.\n저는 주로 오류 메시지를 보지 않기 위해 이 방법을 사용합니다(?? 참고).\n다음 명령어는 cat이 404.txt 파일을 찾을 수 없어 오류 메시지를 생성하게 합니다.다음과 같이 표준 오류를 /dev/null로 리다이렉션할 수 있습니다.➊ 2는 표준 오류를 의미합니다.같은 파일에서 읽고 동시에 같은 파일에 쓰는 것을 주의하세요.\n그렇게 하면 빈 파일만 남게 될 것입니다.\n그 이유는 출력이 리다이렉션되는 도구가 즉시 파일을 쓰기 모드로 열면서 내용을 비워버리기 때문입니다.\n이를 해결하는 방법은 두 가지가 있습니다. (1) 다른 파일에 쓴 다음 mv로 이름을 바꾸거나, (2) 입력을 모두 빨아들인 뒤 파일에 쓰는 sponge25 도구를 사용하는 것입니다.\nFigure ??는 이것이 어떻게 작동하는지 보여줍니다.예를 들어, dseq26로 dates.txt 파일을 생성하고 nl27을 사용해 행 번호를 붙이고 싶다고 가정해 봅시다.\n다음과 같이 실행하면 dates.txt 파일은 비어 있게 됩니다.대신 앞서 설명한 방법 중 하나를 사용할 수 있습니다.","code":"curl \"https://www.gutenberg.org/files/11/11-0.txt\" | grep \" CHAPTER\" > chapters.txt\ncat chapters.txtecho -n \"Hello\" > greeting.txt\necho \" World\" >> greeting.txtcat greeting.txt\ncat greeting.txt | wc -w< greeting.txt wc -wwc -w greeting.txt movies.txtcat movies.txt 404.txtcat movies.txt 404.txt 2> /dev/nulldseq 5 > dates.txt\n< dates.txt nl > dates.txt\nbat dates.txtdseq 5 > dates.txt\n< dates.txt nl > dates-nl.txt\nbat dates-nl.txt\ndseq 5 > dates.txt\n< dates.txt nl | sponge dates.txt\nbat dates.txt"},{"path":"chapter-2-getting-started.html","id":"파일과-디렉터리-다루기","chapter":"제 2 시작하기","heading":"2.3.6 파일과 디렉터리 다루기","text":"데이터 과학자로서 우리는 수많은 데이터와 소통하며, 그 데이터는 주로 파일에 저장됩니다.\n커맨드 라인에서 파일(그리고 파일이 담긴 디렉터리)을 다루는 법을 아는 것이 중요합니다.\nGUI에서 할 수 있는 모든 동작(그리고 그 이상의 것들)을 커맨드 라인 도구로 수행할 수 있습니다.\n이 절에서는 파일과 디렉터리를 나열하고, 생성하고, 이동하고, 복사하고, 이름을 바꾸고, 삭제하는 가장 중요한 도구들을 소개합니다.디렉터리의 내용을 나열하는 것은 ls로 할 수 있습니다.\n디렉터리를 지정하지 않으면 현재 디렉터리의 내용을 나열합니다.\n저는 주로 긴 목록 형식으로 표시하고 디렉터리를 파일보다 먼저 그룹화하는 것을 선호합니다.\n매번 옵션을 일일이 입력하는 대신, 별칭 l을 사용합니다.이미 >나 >>를 사용하여 출력을 리다이렉션함으로써 새 파일을 만드는 방법을 보셨습니다.\n파일을 다른 디렉터리로 옮겨야 한다면 mv28를 사용하면 됩니다.mv로 파일의 이름을 바꿀 수도 있습니다.디렉터리 전체의 이름을 바꾸거나 이동할 수도 있습니다.\n더 이상 필요 없는 파일은 rm29으로 삭제(또는 제거)합니다.디렉터리와 그 안의 모든 내용을 삭제하고 싶다면 재귀(recursive)를 의미하는 -r 옵션을 지정합니다.파일을 복사하려면 cp30를 사용합니다. 백업본을 만들 때 유용합니다.디렉터리는 mkdir31을 사용하여 생성할 수 있습니다.위의 모든 커맨드 라인 도구들은 -v 옵션(verbose)을 지원하여 어떤 작업이 진행되는지 출력해 줍니다.\n예를 들어 다음과 같습니다.mkdir을 제외한 모든 도구는 -옵션(interactive)을 지원하여, 작업을 수행하기 전에 확인 과정을 거칩니다.\n예를 들어 다음과 같습니다.","code":"ls /data/ch10\nalias l\nl /data/ch10mv hello.txt /data/ch02cd data\nmv hello.txt bye.txtrm bye.txtrm -r /data/ch02/oldcp server.log server.log.bakcd /data\nmkdir logs\nlmkdir -v backup\ncp -v * backuprm -i *#! expect_prompt=FALSE\nn#! enter=FALSE, expect_prompt=TRUE"},{"path":"chapter-2-getting-started.html","id":"출력-관리하기","chapter":"제 2 시작하기","heading":"2.3.7 출력 관리하기","text":"가끔 도구나 파이프라인이 책에 싣기에는 너무 많은 양의 출력을 생성할 때가 있습니다.\n출력을 수동으로 수정하는 대신, 헬퍼 도구로 파이프 연결을 하여 투명하게 처리하는 것을 선호합니다.\n여러분이 직접 할 때는 전체 내용을 보고 싶다면 굳이 이렇게 할 필요는 없습니다.제가 출력을 제어하기 위해 사용하는 도구들은 다음과 같습니다.trim은 출력의 높이(줄 수)와 너비(글자 수)를 제한하기 위해 자주 사용합니다.\n기본적으로 출력은 10줄과 터미널 너비로 잘립니다.\n음수를 전달하면 높이나 너비 제한을 해제할 수 있습니다.\n예를 들어 다음과 같습니다.출력을 다듬기 위해 사용하는 다른 도구로는 head, tail, fold, paste, column 등이 있습니다.\n부록에 각각의 예제가 실려 있습니다.쉼표로 구분된(CSV) 출력인 경우, 주로 csvlook으로 파이프하여 보기 좋은 표로 변환합니다.\n그냥 csvlook을 실행하면 전체 표가 보일 것입니다.\n저는 trim에 의해 표가 짧게 보이도록 csvlook을 재정의해 두었습니다.줄 번호와 신택스 하이라이팅(구문 강조)이 중요한 소스 코드 등의 내용을 보여줄 때는 bat을 사용합니다.\n예를 들어 소스 코드는 다음과 같습니다.파일의 공백, 탭, 줄바꿈 등을 명시적으로 가리키고 싶을 때는 -옵션을 추가하기도 합니다.중간 분석 결과를 파일에 기록하는 것이 유용할 때가 있습니다.\n이를 통해 파이프라인의 각 단계를 완료 후에 확인해 볼 수 있습니다.\n파이프라인 안 어디든 원하는 만큼 tee 도구를 삽입할 수 있습니다.\n저는 주로 최종 출력물의 일부를 확인하면서 동시에 전체 출력물을 파일에 저장하고 싶을 때 사용합니다(?? 참고).\n여기서는 전체 출력이 even.txt에 기록되고, 동시에 처음 5줄이 trim을 통해 출력됩니다.마지막으로, 커맨드 라인 도구에 의해 생성된 이미지(스크린샷과 다이어그램을 제외한 모든 이미지)를 삽입할 때는 display를 사용합니다.\n지금 display를 실행해 보면 작동하지 않을 것입니다.\n7장에서 커맨드 라인에서 생성된 이미지를 표시하기 위한 네 가지 옵션을 설명하겠습니다.","code":"cat /data/ch07/tips.csv | trim 5 25which csvlook\ncsvlook /data/ch07/tips.csvbat /data/ch04/stream.pyseq 0 2 100 | tee even.txt | trim 5"},{"path":"chapter-2-getting-started.html","id":"도와주세요-help","chapter":"제 2 시작하기","heading":"2.3.8 도와주세요! (Help!)","text":"커맨드 라인을 익히다 보면 도움이 필요할 때가 생깁니다.\n아무리 숙련된 사용자라도 가끔은 도움이 필요합니다.\n수많은 커맨드 라인 도구와 그 인자들을 모두 기억하는 것은 불가능하기 때문입니다.\n다행히 커맨드 라인에서는 도움을 얻을 수 있는 여러 방법을 제공합니다.도움을 얻는 가장 중요한 명령어는 아마도 manual의 줄임말인 man32일 것입니다.\n거의 모든 커맨드 라인 도구에 대한 정보를 담고 있습니다.\n만약 tar 도구의 옵션을 잊어버렸다면(저도 자주 잊어버립니다), 다음 명령어로 매뉴얼 페이지를 확인할 수 있습니다.모든 커맨드 라인 도구가 매뉴얼 페이지를 가지고 있는 것은 아닙니다.\ncd를 예로 들어 봅시다.cd와 같은 쉘 내장 명령어의 경우 zshbuiltins 매뉴얼 페이지를 참고할 수 있습니다./를 눌러 검색할 수 있고 q를 눌러 나갈 수 있습니다.\ncd에 해당하는 섹션을 찾아보세요.최신 커맨드 라인 도구들도 매뉴얼 페이지가 없는 경우가 많습니다.\n그럴 때는 해당 도구를 --help (또는 -h) 옵션과 함께 실행해 보는 것이 최선입니다.\n예를 들어 다음과 같습니다.--help 옵션을 지정하는 것은 cat과 같은 기존 도구들에서도 작동합니다.\n하지만 매뉴얼 페이지가 더 많은 정보를 제공하는 경우가 많습니다.\n이 세 가지 방법으로도 해결되지 않는다면 인터넷 검색을 활용하는 것도 전혀 문제없습니다.\n부록에 이 책에서 사용된 모든 커맨드 라인 도구의 목록이 있습니다.\n각 도구를 어떻게 설치하는지뿐만 아니라 어떻게 도움을 얻을 수 있는지도 나와 있습니다.매뉴얼 페이지는 내용이 너무 길고 읽기 힘들 수 있습니다.\ntldr33 도구는 커뮤니티에서 관리하는 도움말 페이지 모음으로, 전통적인 매뉴얼 페이지보다 훨씬 단순하고 접근하기 쉽도록 만들어졌습니다.\n다음은 tar에 대한 tldr 페이지 예시입니다.보시다시피 man처럼 수많은 옵션을 알파벳순으로 나열하는 대신, tldr은 실질적인 예시 목록을 보여주어 핵심을 바로 짚어줍니다.","code":"man tar | trim 20man cdman zshbuiltins | trimjq --help | trimtldr tar | trim 20"},{"path":"chapter-2-getting-started.html","id":"요약-1","chapter":"제 2 시작하기","heading":"2.4 요약","text":"이 장에서는 Docker 이미지를 설치하여 필요한 모든 커맨드 라인 도구를 확보하는 방법을 배웠습니다.\n또한 필수적인 커맨드 라인 개념들과 도움을 얻는 방법들도 살펴보았습니다.\n이제 모든 준비가 끝났으므로, 데이터 과학을 위한 OSEMN 모델의 첫 번째 단계인 데이터 획득을 시작할 준비가 되었습니다.","code":""},{"path":"chapter-2-getting-started.html","id":"더-읽을거리-1","chapter":"제 2 시작하기","heading":"2.5 더 읽을거리","text":"이 책의 부제는 Jerry Peek, Shelley Powers, Tim O’Reilly, Mike Loukides가 쓴 명저 Unix Power Tools에 경의를 표하는 의미를 담고 있습니다. 51개 장과 1000페이지 넘는 분량에 유닉스에 대해 알아야 할 거의 모든 것을 다루고 있습니다. 무게가 2kg 가까이 나가므로 전자책으로 보시는 것을 추천합니다.explainshell 웹사이트는 명령어나 명령어 시퀀스를 파싱하여 각 부분에 대해 짧은 설명을 제공합니다. 매뉴얼 페이지를 훑어보지 않고도 새로운 명령어와 옵션을 빠르게 이해하는 데 유용합니다.Docker는 정말 멋진 소프트웨어입니다. 이 장에서 Docker 이미지를 다운로드하고 컨테이너를 실행하는 법을 짧게 설명했지만, 여러분만의 Docker 이미지를 만드는 법을 배우는 것도 가치가 있을 것입니다. Sean Kane과 Karl Matthias가 쓴 Docker: & Running도 좋은 자료입니다.","code":""},{"path":"chapter-3-obtaining-data.html","id":"chapter-3-obtaining-data","chapter":"제 3 데이터 획득하기","heading":"제 3 데이터 획득하기","text":"이 장에서는 OSEMN 모델의 첫 번째 단계인 데이터 획득(Obtaining)을 다룹니다.\n데이터 없이는 데이터 과학을 할 수 없기 때문입니다.\n데이터 과학 문제를 해결하는 데 필요한 데이터가 이미 존재한다고 가정한다면, 여러분의 첫 번째 과제는 그 데이터를 여러분이 작업할 수 있는 형식으로 컴퓨터(혹은 가능하면 Docker 컨테이너 내부)에 가져오는 것입니다.유닉스 철학에 따르면 텍스트는 범용 인터페이스입니다.\n거의 모든 커맨드 라인 도구가 텍스트를 입력으로 받고, 텍스트를 출력으로 생성하거나 두 가지 모두를 수행합니다.\n이것이 커맨드 라인 도구들이 서로 아주 잘 어울려 작동할 수 있는 주요 원인입니다.\n하지만 앞으로 보겠지만, 텍스트 하나만 해도 여러 가지 형식이 있을 수 있습니다.데이터는 서버에서 다운로드하거나, 데이터베이스에 쿼리하거나, 웹 API에 연결하는 등 여러 가지 방법으로 얻을 수 있습니다.\n때로는 데이터가 압축된 형태이거나 마이크로소프트 엑셀 스프레드시트와 같은 바이너리 형식으로 오기도 합니다.\n이 장에서는 커맨드 라인에서 이를 해결하는 데 도움이 되는 여러 도구를 다룹니다: curl34, in2csv35, sql2csv36, tar37 등이 포함됩니다.","code":""},{"path":"chapter-3-obtaining-data.html","id":"개요","chapter":"제 3 데이터 획득하기","heading":"3.1 개요","text":"이 장에서 여러분은 다음 내용을 배우게 됩니다.로컬 파일을 Docker 이미지로 복사하기인터넷에서 데이터 다운로드하기파일 압축 풀기스프레드시트에서 데이터 추출하기관계형 데이터베이스 쿼리하기웹 API 호출하기이 장은 다음 파일들로 시작합니다.이 파일들을 얻는 방법은 2장에 설명되어 있습니다.\n그 외의 파일들은 커맨드 라인 도구를 사용하여 다운로드하거나 생성한 것들입니다.","code":"cd /data/ch03\nl"},{"path":"chapter-3-obtaining-data.html","id":"로컬-파일을-docker-컨테이너로-복사하기","chapter":"제 3 데이터 획득하기","heading":"3.2 로컬 파일을 Docker 컨테이너로 복사하기","text":"필요한 파일이 이미 여러분의 컴퓨터에 있는 경우가 흔합니다.\n이 절에서는 그 파일들을 Docker 컨테이너 안으로 가져오는 방법을 설명합니다.2장에서 말씀드렸듯이 Docker 컨테이너는 격리된 가상 환경입니다.\n다행히 한 가지 예외가 있는데, Docker 컨테이너 안팎으로 파일을 전송할 수 있다는 점입니다.\n여러분이 docker run을 실행했던 로컬 디렉터리가 Docker 컨테이너 내부의 한 디렉터리로 매핑됩니다.\n그 디렉터리는 /data라고 부릅니다.\n참고로 이곳은 홈 디렉터리(/home/dst)가 아닙니다.로컬 컴퓨터에 있는 파일을 커맨드 라인 도구로 다루고 싶다면, 해당 파일을 매핑된 디렉터리로 복사하거나 이동하기만 하면 됩니다.\n여러분의 ‘다운로드’ 디렉터리에 logs.csv라는 파일이 있다고 가정해 봅시다.윈도우를 사용 중이라면 명령 프롬프트나 PowerShell을 열고 다음 두 명령어를 실행합니다.리눅스나 macOS를 사용 중이라면 터미널을 열고 운영체제의 쉘에서(Docker 컨테이너 내부가 아님) 다음 명령어를 실행합니다.윈도우 파일 탐색기나 macOS의 Finder와 같은 그래픽 파일 관리자를 사용하여 파일을 해당 디렉터리로 드래그 앤 드롭할 수도 있습니다.","code":"> cd %UserProfile%\\Downloads\n> copy logs.csv MyDataScienceToolbox\\cp ~/Downloads/logs.csv ~/my-data-science-toolbox#! enter=FALSE"},{"path":"chapter-3-obtaining-data.html","id":"인터넷에서-다운로드하기","chapter":"제 3 데이터 획득하기","heading":"3.3 인터넷에서 다운로드하기","text":"인터넷은 의심할 여지 없이 흥미로운 데이터를 위한 가장 큰 자원 저장소입니다.\n커맨드 라인 도구 curl은 인터넷에서 데이터를 다운로드할 때 커맨드 라인의 맥가이버 칼과 같은 존재라고 할 수 있습니다.","code":""},{"path":"chapter-3-obtaining-data.html","id":"curl-소개","chapter":"제 3 데이터 획득하기","heading":"3.3.1 curl 소개","text":"URL(uniform resource locator) 주소로 접속하면 브라우저는 다운로드한 데이터를 해석합니다.\n예를 들어 브라우저는 HTML 파일을 렌더링하고, 비디오 파일을 자동으로 재생하며, PDF 파일을 보여줍니다.\n하지만 curl로 URL에 접근하면 데이터를 다운로드하여 기본적으로 표준 출력으로 인쇄합니다.\ncurl은 데이터를 해석하지 않지만, 다행히 다른 커맨드 라인 도구를 사용하여 데이터를 추가로 처리할 수 있습니다.curl을 실행하는 가장 쉬운 방법은 URL을 커맨드 라인 인자로 지정하는 것입니다.\n위키백과에서 문서를 하나 다운로드해 봅시다.➊ 기억하세요. trim은 출력 결과가 책에 잘 어울리게 들어가도록 하기 위해서만 사용합니다.보시다시피 curl은 위키백과 서버가 반환한 가공되지 않은(raw) HTML을 다운로드합니다. 아무런 해석 과정 없이 전체 내용이 즉시 표준 출력으로 인쇄됩니다.\nURL을 보면 이 문서가 네덜란드의 모든 풍차 목록을 담고 있을 것 같지만, 보아하니 풍차가 너무 많아서 각 주(province)마다 별도의 페이지가 있는 것 같습니다. 놀랍군요.기본적으로 curl은 다운로드 속도와 예상 완료 시간을 보여주는 진행 표시기(progress meter)를 출력합니다.\n이 출력은 표준 출력이 아닌 별도의 채널인 표준 오류로 전달되므로, 파이프라인에 다른 도구를 추가하더라도 방해되지 않습니다.\n매우 큰 파일을 다운로드할 때는 이 정보가 유용할 수 있지만, 대개는 번거롭게 느껴지므로 저는 -s 옵션을 지정하여 이 출력을 끕니다(silence).➊ 웹사이트 스크래핑에 유용한 도구인 pup38에 대해서는 5장에서 더 자세히 다루겠습니다.보세요, 프리슬란트 주에만 자그마치 234개의 풍차가 있다고 하네요!","code":"curl \"https://en.wikipedia.org/wiki/List_of_windmills_in_the_Netherlands\" |\ntrimcurl -s \"https://en.wikipedia.org/wiki/List_of_windmills_in_Friesland\" |\npup -n 'table.wikitable tr' ➊"},{"path":"chapter-3-obtaining-data.html","id":"파일로-저장하기","chapter":"제 3 데이터 획득하기","heading":"3.3.2 파일로 저장하기","text":"-O 옵션을 추가하여 curl이 출력을 파일로 저장하게 할 수 있습니다.\n파일 이름은 URL의 마지막 부분을 기반으로 결정됩니다.그 파일 이름이 마음에 들지 않는다면 -o 옵션과 함께 파일 이름을 지정하거나, 직접 출력을 파일로 리다이렉션할 수 있습니다.","code":"curl -s \"https://en.wikipedia.org/wiki/List_of_windmills_in_Friesland\" -O\nlcurl -s \"https://en.wikipedia.org/wiki/List_of_windmills_in_Friesland\" > friesland.html"},{"path":"chapter-3-obtaining-data.html","id":"다른-프로토콜들","chapter":"제 3 데이터 획득하기","heading":"3.3.3 다른 프로토콜들","text":"curl은 총 20개 이상의 프로토콜을 지원합니다.\nFTP(File Transfer Protocol) 서버에서 다운로드할 때도 동일한 방식으로 curl을 사용합니다.\n여기서는 ftp.gnu.org에서 welcome.msg 파일을 다운로드해 보겠습니다.지정된 URL이 디렉터리라면 curl은 해당 디렉터리의 내용을 나열합니다.\nURL이 비밀번호로 보호되어 있다면 -u 옵션을 사용하여 사용자 이름과 비밀번호를 지정할 수 있습니다.각종 사전과 용어 정의에 접근할 수 있게 해주는 DICT 프로토콜은 어떨까요?\nCollaborative International Dictionary English에 따른 “windmill”의 정의는 다음과 같습니다.하지만 인터넷에서 데이터를 다운로드할 때 사용하는 프로토콜은 대부분 HTTP일 것이므로, URL은 http:// 또는 *https://*로 시작할 것입니다.","code":"curl -s \"ftp://ftp.gnu.org/welcome.msg\" | trimcurl -s \"dict://dict.org/d:windmill\" | trim"},{"path":"chapter-3-obtaining-data.html","id":"리다이렉션-따라가기-following-redirects","chapter":"제 3 데이터 획득하기","heading":"3.3.4 리다이렉션 따라가기 (Following Redirects)","text":"http://bit.ly/ 또는 *http://t.co/*로 시작하는 단축 URL에 접근하면 브라우저는 자동으로 올바른 위치로 리다이렉션해 줍니다.\n하지만 curl에서는 리다이렉션되도록 -L 또는 --location 옵션을 지정해야 합니다.\n그렇지 않으면 다음과 같은 결과를 얻게 될 수도 있습니다.때로는 우리가 방금 사용한 URL처럼 아무런 응답도 받지 못할 때가 있습니다.-또는 --head 옵션을 지정하면 curl은 응답의 HTTP 헤더라고 불리는 부분만 가져옵니다. 이를 통해 서버가 반환한 상태 코드(status code)와 기타 정보를 확인할 수 있습니다.첫 번째 줄은 프로토콜과 HTTP 상태 코드를 보여주는데, 이 경우 303입니다.\n또한 이 URL이 리다이렉션되는 위치(location)도 확인할 수 있습니다.\ncurl이 예상한 결과를 주지 않을 때 헤더를 검사하고 상태 코드를 확인하는 것은 유용한 디버깅 방법입니다.\n다른 흔한 HTTP 상태 코드로는 404(찾을 수 없음)와 403(금지됨)이 있습니다.\n위키백과에는 모든 HTTP 상태 코드를 나열한 페이지가 있습니다.요약하자면, curl은 인터넷에서 데이터를 다운로드하는 데 유용한 커맨드 라인 도구입니다.\n가장 자주 쓰이는 세 가지 옵션은 진행 표시기를 끄는 -s, 사용자 이름과 비밀번호를 지정하는 -u, 그리고 자동으로 리다이렉션을 따라가는 -L입니다.\n더 자세한 정보는 매뉴얼 페이지를 참고하세요(아마 머리가 어지러울 수 있습니다).","code":"curl -s \"https://bit.ly/2XBxvwK\"curl -s \"https://youtu.be/dQw4w9WgXcQ\"curl -sI \"https://youtu.be/dQw4w9WgXcQ\" | trimman curl | trim 20"},{"path":"chapter-3-obtaining-data.html","id":"파일-압축-풀기","chapter":"제 3 데이터 획득하기","heading":"3.4 파일 압축 풀기","text":"원본 데이터셋이 매우 크거나 여러 파일의 모음인 경우, 압축된 아카이브 형태일 수 있습니다.\n텍스트 파일의 단어들이나 JSON 파일의 키(key)들과 같이 반복되는 값이 많은 데이터셋은 특히 압축하기에 적합합니다.압축 아카이브의 일반적인 확장자는 .tar.gz, .zip, .rar 등입니다.\n이들의 압축을 풀기 위해서는 각각 tar, unzip39, unrar40 도구를 사용합니다.\n(그리 흔하지는 않지만 다른 도구가 필요한 확장자들도 몇 가지 더 있습니다.).tar.gz(“지집 타볼(gzipped tarball)”이라고 읽습니다)를 예로 들어 봅시다.\nlogs.tar.gz라는 아카이브를 추출하기 위해 다음 명령어를 사용할 수 있습니다.➊ 제가 여기서 한 것처럼 이 세 개의 짧은 옵션을 결합하는 것이 일반적이지만, -x -z -f와 같이 따로 지정할 수도 있습니다.\n사실 많은 커맨드 라인 도구들이 단일 문자로 구성된 옵션들을 결합해서 쓸 수 있도록 허용합니다.과연 tar는 수많은 커맨드 라인 인자로 악명이 높습니다.\n여기서 세 옵션 -x, -z, -f는 각각 tar가 아카이브에서 파일을 추출(extract)하고, 압축 해제 알고리즘으로 gzip을 사용하며, logs.tar.gz 파일을 사용하도록 지정합니다.하지만 이 아카이브에 아직 익숙하지 않으므로 내용을 먼저 확인하는 것이 좋습니다.\n이는 -x 옵션 대신 -t 옵션으로 할 수 있습니다.이 아카이브에는 많은 파일이 들어 있는 것 같고, 디렉터리 안에 들어 있지는 않네요.\n현재 디렉터리를 깨끗하게 유지하기 위해 우선 mkdir로 새 디렉터리를 만들고, -C 옵션을 사용해 그곳에 파일들을 추출하는 것이 좋습니다.파일 수와 내용 일부를 확인해 봅시다.아주 좋군요.\n여러분이 이 로그 파일들을 정제하고 탐색하고 싶어 한다는 점은 이해하지만, 그 내용은 나중에 5장과 7장에서 다루겠습니다.시간이 지나면 이 옵션들에 익숙해지겠지만, 편리할 수 있는 대안적인 옵션을 하나 보여드리고 싶습니다.\n서로 다른 커맨드 라인 도구와 그 옵션들을 일일이 기억하는 대신, 여러 가지 형식을 알아서 풀어주는 unpack41이라는 편리한 스크립트가 있습니다.\nunpack은 압축을 풀고자 하는 파일의 확장자를 보고 적절한 커맨드 라인 도구를 호출합니다.\n이제 동일한 파일의 압축을 풀기 위해 다음을 실행하면 됩니다.","code":"tar -xzf logs.tar.gz ➊ #! enter=FALSEtar -tzf logs.tar.gz | trimmkdir logs\ntar -xzf logs.tar.gz -C logsls logs | wc -l\ncat logs/* | trimunpack logs.tar.gz #! enter=FALSE"},{"path":"chapter-3-obtaining-data.html","id":"마이크로소프트-엑셀-스프레드시트를-csv로-변환하기","chapter":"제 3 데이터 획득하기","heading":"3.5 마이크로소프트 엑셀 스프레드시트를 CSV로 변환하기","text":"많은 사람에게 마이크로소프트 엑셀은 소규모 데이터셋을 다루고 계산을 수행하는 직관적인 방법을 제공합니다.\n그 결과, 엄청나게 많은 데이터가 마이크로소프트 엑셀 스프레드시트에 내포되어 있습니다.\n이러한 스프레드시트는 파일 확장자에 따라 독자적인 바이너리 형식(.xls)이나 압축된 XML 파일들의 모음(.xlsx)으로 저장됩니다.\n두 경우 모두 데이터가 대부분의 커맨드 라인 도구에서 즉시 사용하기에 적합하지 않습니다.\n단지 이런 방식으로 저장되어 있다는 이유만으로 그 가치 있는 데이터셋들을 사용할 수 없다면 참 안타까운 일일 것입니다.특히 커맨드 라인을 막 시작했을 때는 스프레드시트를 마이크로소프트 엑셀이나 리브레오피스(LibreOffice) Calc 같은 오픈 소스 프로그램으로 열어서 수동으로 CSV로 내보내고 싶은 유혹을 느낄 수 있습니다.\n일회성 해결책으로는 괜찮을 수 있지만, 여러 파일로 확장하기 어렵고 자동화할 수 없다는 단점이 있습니다.\n게다가 서버에서 작업할 때는 그런 애플리케이션을 사용할 수 없을 가능성이 큽니다.\n저를 믿으세요, 곧 익숙해질 것입니다.다행히 마이크로소프트 엑셀 스프레드시트를 CSV 파일로 변환해 주는 in2csv라는 커맨드 라인 도구가 있습니다.\nCSV는 쉼표로 구분된 값(comma-separated values)을 의미합니다.\nCSV는 공식적인 사양이 부족하기 때문에 다루기가 까다로울 수 있습니다.\nYakov Shafranovich는 CSV 형식을 다음 세 가지 포인트로 정의합니다.42각 레코드는 줄바꿈(␊)으로 구분되어 별도의 줄에 위치합니다. 예를 들어 닌자 거북이에 대한 중요한 정보가 담긴 다음 CSV 파일을 봅시다.➊ -옵션은 bat이 공백, 탭, 줄바꿈 같은 출력되지 않는 모든 문자를 보여주게 합니다.파일의 마지막 레코드에는 끝 줄바꿈이 있을 수도 있고 없을 수도 있습니다. 예를 들면 다음과 같습니다.파일의 첫 번째 줄에 일반 레코드 줄과 동일한 형식의 헤더가 나타날 수 있습니다. 이 헤더는 파일의 필드에 대응하는 이름들을 포함하며, 파일의 나머지 부분에 있는 레코드들과 동일한 수의 필드를 가져야 합니다. 예를 들면 다음과 같습니다.보시다시피 CSV는 기본적으로 그리 읽기 편하지 않습니다.\n데이터를 csvlook43이라는 도구로 파이프하면 보기 좋은 표 형식으로 포맷해 줍니다.\n만약 tmnt-missing-newline.csv처럼 CSV 데이터에 헤더가 없다면 -H 옵션을 추가해야 합니다. 그렇지 않으면 첫 번째 줄이 헤더로 해석됩니다.➊ -H 옵션은 CSV 파일에 헤더가 없음을 지정합니다.네덜란드의 연례 라디오 프로그램인 Top 2000의 역대 인기곡 2000곡이 담긴 스프레드시트를 사용하여 in2csv를 시연해 보겠습니다.\n데이터를 추출하려면 다음과 같이 in2csv를 호출합니다.Danny Vera가 누구죠? 가장 인기 있는 노래는 당연히 Bohemian Rhapsody여야 합니다.\n음, 적어도 Queen은 Top 2000에 아주 많이 등장하니 불평할 수는 없겠네요.➊ --regex 옵션 뒤에 오는 값은 정규 표현식(regular expression, 줄여서 regex)입니다. 패턴을 정의하기 위한 특별한 구문입니다. 여기서는 아티스트 이름이 정확히 “Queen”인 것만 매칭하고 싶어서, ‘ARTIEST’ 컬럼 값의 시작과 끝을 나타내는 캐럿(^)과 달러 기호($)를 사용했습니다.참고로 in2csv, csvgrep, csvlook 도구들은 CSV 데이터를 다루기 위한 커맨드 라인 도구 모음인 CSVkit의 일부입니다.파일 형식은 확장자에 의해 자동으로 결정되며, 이 경우 .xlsx입니다.\n만약 데이터를 in2csv로 파이프한다면 형식을 명시적으로 지정해야 합니다.스프레드시트는 여러 개의 워크시트를 포함할 수 있습니다.\nin2csv는 기본적으로 첫 번째 워크시트를 추출합니다.\n다른 워크시트를 추출하려면 --sheet 옵션에 워크시트 이름을 전달해야 합니다.\n워크시트 이름이 무엇인지 확실하지 않다면 모든 워크시트의 이름을 출력하는 --names 옵션을 사용할 수 있습니다.\n여기서 top2000.xlsx에는 Blad1(네덜란드어로 Sheet1이라는 뜻)이라는 이름의 시트가 하나만 있음을 확인할 수 있습니다.","code":"bat -A tmnt-basic.csv ➊bat -A tmnt-missing-newline.csvbat -A tmnt-with-header.csvcsvlook tmnt-with-header.csv\ncsvlook tmnt-basic.csv\ncsvlook -H tmnt-missing-newline.csv ➊curl \"https://www.nporadio2.nl/data/download/TOP-2000-2020.xlsx\" > top2000.xlsx\nin2csv top2000.xlsx | tee top2000.csv | trimcsvgrep top2000.csv --columns ARTIEST --regex '^Queen$' | csvlook -Iin2csv --names top2000.xlsx"},{"path":"chapter-3-obtaining-data.html","id":"관계형-데이터베이스-쿼리하기","chapter":"제 3 데이터 획득하기","heading":"3.6 관계형 데이터베이스 쿼리하기","text":"많은 기업이 데이터를 관계형 데이터베이스에 저장합니다.\n스프레드시트와 마찬가지로, 커맨드 라인에서 그 데이터를 얻을 수 있다면 정말 좋을 것입니다.관계형 데이터베이스의 예로는 MySQL, PostgreSQL, SQLite 등이 있습니다.\n이 데이터베이스들은 모두 인터페이스하는 방식이 조금씩 다릅니다.\n어떤 것들은 커맨드 라인 도구나 인터페이스를 제공하지만, 그렇지 않은 것들도 있습니다.\n게다가 사용법과 출력 형식 면에서 일관성이 떨어지기도 합니다.다행히 CSVkit 스위트의 일부인 sql2csv라는 커맨드 라인 도구가 있습니다.\n이 도구는 MySQL, Oracle, PostgreSQL, SQLite, Microsoft SQL Server, Sybase를 포함한 다양한 데이터베이스와 공통된 인터페이스를 통해 작동합니다.\nsql2csv의 출력은 이름에서 알 수 있듯이 CSV 형식입니다.우리는 SELECT 쿼리를 실행하여 관계형 데이터베이스에서 데이터를 얻을 수 있습니다.\n(sql2csv는 INSERT, UPDATE, DELETE 쿼리도 지원하지만, 이 장의 목적에는 맞지 않습니다.)sql2csv에는 두 가지 인자가 필요합니다: 데이터베이스 URL을 지정하는 --db(일반적인 형식은 dialect+driver://username:password@host:port/database입니다)와 SELECT 쿼리를 포함하는 --query입니다.\n예를 들어, R의 표준 데이터셋이 담긴 SQLite 데이터베이스44가 있다면, 다음과 같이 mtcars 테이블의 모든 행을 선택하고 mpg 컬럼을 기준으로 정렬할 수 있습니다.이 SQLite 데이터베이스는 로컬 파일이므로 사용자 이름, 비밀번호, 호스트를 지정할 필요가 없습니다.\n회사의 데이터베이스를 쿼리하고 싶다면 당연히 접근 방법과 권한이 필요할 것입니다.","code":"sql2csv --db 'sqlite:///r-datasets.db' \\\n--query 'SELECT row_names AS car, mpg FROM mtcars ORDER BY mpg' | csvlook"},{"path":"chapter-3-obtaining-data.html","id":"웹-api-호출하기","chapter":"제 3 데이터 획득하기","heading":"3.7 웹 API 호출하기","text":"앞 절에서는 인터넷에서 파일을 다운로드하는 방법을 설명했습니다.\n인터넷에서 데이터를 얻는 또 다른 방법은 웹 API(Application Programming Interface)를 통하는 것입니다.\n제공되는 API의 수는 점점 더 빠르게 늘어나고 있으며, 이는 우리 데이터 과학자들에게 흥미로운 데이터가 많아짐을 의미합니다.웹 API는 웹사이트처럼 예쁜 레이아웃으로 보여주기 위한 것이 아닙니다.\n대신 대부분의 웹 API는 JSON이나 XML 같은 구조화된 형식으로 데이터를 반환합니다.\n구조화된 형식의 데이터를 가지면 jq와 같은 다른 도구로 데이터를 쉽게 처리할 수 있다는 장점이 있습니다.\n예를 들어 George R.R. Martin의 가상 세계에 대한 풍부한 정보를 담고 있는 API Ice Fire(얼음과 불의 노래 API)는 다음과 같은 JSON 구조로 데이터를 반환합니다.➊ 스포일러 주의: 이 데이터는 완전히 최신 상태가 아닐 수 있습니다.데이터를 보기 좋게 표시하기 위해 커맨드 라인 도구 jq로 파이프했습니다.\njq에는 정제와 탐색을 위한 훨씬 더 많은 가능성이 있으며, 이는 5장과 7장에서 살펴보겠습니다.","code":"curl -s \"https://anapioficeandfire.com/api/characters/583\" | jq '.'"},{"path":"chapter-3-obtaining-data.html","id":"인증-authentication","chapter":"제 3 데이터 획득하기","heading":"3.7.1 인증 (Authentication)","text":"일부 웹 API는 데이터를 사용하기 전에 인증(즉, 신원 증명)을 요구합니다.\n이를 수행하는 몇 가지 방법이 있습니다.\n어떤 웹 API는 API 키를 사용하고, 어떤 것들은 OAuth 프로토콜을 사용합니다.\n헤드라인과 뉴스 기사를 제공하는 독립 자원인 News API가 훌륭한 예입니다.\nAPI 키 없이 이 API에 접근하면 어떤 일이 일어나는지 봅시다.음, 예상했던 결과군요.\n참고로 물음표 뒤의 부분은 쿼리 매개변수를 전달하는 곳입니다.\nAPI 키를 지정해야 하는 곳이기도 하죠.\n제 API 키를 비밀로 유지하고 싶으므로, 명령어 치환(command substitution)을 사용하여 /data/.secret/newsapi.org_apikey 파일을 읽어서 입력하겠습니다.여러분만의 API 키는 News API 웹사이트에서 얻을 수 있습니다.","code":"curl -s \"http://newsapi.org/v2/everything?q=linux\" | jq .curl -s \"http://newsapi.org/v2/everything?q=linux&apiKey=$(< /data/.secret/newsapi.org_apikey)\" |\njq '.' | trim 30"},{"path":"chapter-3-obtaining-data.html","id":"스트리밍-api-streaming-apis","chapter":"제 3 데이터 획득하기","heading":"3.7.2 스트리밍 API (Streaming APIs)","text":"일부 웹 API는 데이터를 스트리밍 방식으로 반환합니다.\n즉, 한 번 연결하면 연결이 끊길 때까지 데이터가 쉬지 않고 쏟아져 들어옵니다.\n유명한 예로는 전 세계에서 전송되는 모든 트윗을 끊임없이 스트리밍하는 트위터의 “파이어호스(firehose)”가 있습니다.\n다행히 대부분의 커맨드 라인 도구들도 스트리밍 방식으로 작동합니다.예를 들어 위키미디어의 스트리밍 API 중 하나를 10초 동안 샘플링해 봅시다.이 특정 API는 위키백과와 위키미디어의 다른 속성들에 가해진 모든 변경 사항을 반환합니다.\n커맨드 라인 도구 sample은 10초 후에 연결을 닫는 데 사용됩니다.\nCtrl-C를 눌러 인터럽트를 보냄으로써 연결을 수동으로 닫을 수도 있습니다.\n출력은 wikimedia-stream-sample 파일에 저장됩니다.\ntrim을 사용해 살짝 엿볼까요?sed와 jq를 조금 사용해서 이 데이터를 정제하면 영어 위키백과에서 일어나는 변경 사항들을 엿볼 수 있습니다.➊ 이 sed 표현식은 data:로 시작하는 줄만 출력하고 세미콜론 뒤의 부분만 인쇄하는데, 이것이 마침 JSON입니다.\n➋ 이 jq 표현식은 특정 type과 server_name을 가진 JSON 객체의 title 키를 인쇄합니다.스트리밍 이야기가 나와서 말인데, telnet45을 사용해 스타워즈 에피소드 4: 새로운 희망을 무료로 스트리밍할 수 있다는 사실을 알고 계셨나요?그리고 시간이 좀 지나면, 한 솔로가 먼저 쐈다는 것을 알게 됩니다!물론 데이터로서 좋은 소스는 아니겠지만, 머신러닝 모델을 훈련시키면서 고전 명작을 즐기는 것도 나쁘지 않죠46.","code":"curl -s \"https://stream.wikimedia.org/v2/stream/recentchange\" |\nsample -s 10 > wikimedia-stream-sample #! enter=FALSE< wikimedia-stream-sample trim< wikimedia-stream-sample sed -n 's/^data: //p' |\njq 'select(.type == \"edit\" and .server_name == \"en.wikipedia.org\") | .title'telnet towel.blinkenlights.nl#! enter=FALSE cat towel.blinkenlights.nl"},{"path":"chapter-3-obtaining-data.html","id":"요약-2","chapter":"제 3 데이터 획득하기","heading":"3.8 요약","text":"축하합니다, OSEMN 모델의 첫 번째 단계를 마쳤습니다.\n다운로드부터 관계형 데이터베이스 쿼리까지 데이터를 얻는 다양한 방법을 배웠습니다.\n다음 장은 막간 장으로, 여러분만의 커맨드 라인 도구를 만드는 방법을 가르쳐 드릴 것입니다.\n데이터 정제에 대해 배우고 싶어 참을 수 없다면, 이 장을 건너뛰고 5장 (OSEMN 모델의 두 번째 단계)으로 바로 가셔도 좋습니다.","code":""},{"path":"chapter-3-obtaining-data.html","id":"더-읽을거리-2","chapter":"제 3 데이터 획득하기","heading":"3.9 더 읽을거리","text":"연습할 데이터셋을 찾고 계신가요? GitHub 저장소 Awesome Public Datasets에는 공개적으로 사용 가능한 수백 개의 고품질 데이터셋 목록이 있습니다.아니면 API로 연습하고 싶으신가요? GitHub 저장소 Public APIs에는 흥미로운 무료 API들이 많이 나열되어 있습니다. City Bikes와 One API는 제가 좋아하는 API들입니다.관계형 데이터베이스에서 데이터를 얻기 위해 SQL 쿼리를 작성하는 것은 중요한 기술입니다. Ben Forta의 저서 SQL 10 Minutes Day의 처음 15개 장은 SELECT 문과 필터링, 그룹화, 정렬 기능을 잘 가르쳐 줍니다.","code":""},{"path":"chapter-4-creating-command-line-tools.html","id":"chapter-4-creating-command-line-tools","chapter":"제 4 커맨드 라인 도구 만들기","heading":"제 4 커맨드 라인 도구 만들기","text":"이 책 전반에 걸쳐 기본적으로 한 줄로 작성되는 많은 명령어와 파이프라인을 소개해 드릴 것입니다.\n이들은 원라이너(one-liner) 또는 파이프라인으로 알려져 있습니다.\n단 한 줄의 명령어만으로 복잡한 작업을 수행할 수 있다는 점이 커맨드 라인을 강력하게 만드는 대목입니다.\n이는 전통적인 프로그램을 작성하고 사용하는 것과는 매우 다른 경험입니다.어떤 작업은 단 한 번만 수행하지만, 어떤 작업은 더 자주 수행하게 됩니다.\n또 어떤 작업은 매우 구체적이지만, 다른 작업은 일반화될 수 있습니다.\n특정 원라이너를 정기적으로 반복해야 한다면, 이를 하나의 독립적인 커맨드 라인 도구로 만드는 것이 가치가 있습니다.\n원라이너와 커맨드 라인 도구는 각자의 용도가 있습니다.\n그 기회를 알아보는 데는 연습과 기술이 필요합니다.\n커맨드 라인 도구의 장점은 원라이너 전체를 기억할 필요가 없다는 점과, 다른 파이프라인에 포함시켰을 때 가독성을 높여준다는 점입니다.\n그런 의미에서 커맨드 라인 도구는 프로그래밍 언어의 함수와 비슷하다고 생각할 수 있습니다.하지만 프로그래밍 언어로 작업할 때의 이점은 코드가 하나 이상의 파일에 담겨 있다는 점입니다.\n이는 코드를 쉽게 수정하고 재사용할 수 있음을 의미합니다.\n코드에 매개변수(parameter)가 있다면 일반화하여 비슷한 패턴의 문제들에 다시 적용할 수도 있습니다.커맨드 라인 도구는 양쪽의 장점을 모두 가집니다. 커맨드 라인에서 사용할 수 있고, 매개변수를 받을 수 있으며, 단 한 번만 만들면 됩니다.\n이 장에서는 두 가지 방식으로 커맨드 라인 도구를 만드는 법을 익힐 것입니다.\n첫째로, 원라이너를 재사용 가능한 커맨드 라인 도구로 바꾸는 방법을 설명합니다.\n명령어에 매개변수를 추가함으로써 프로그래밍 언어가 제공하는 것과 동일한 유연성을 더할 수 있습니다.\n그다음에는 프로그래밍 언어로 작성된 코드로부터 재사용 가능한 커맨드 라인 도구를 만드는 방법을 시연하겠습니다.\n유닉스 철학을 따름으로써, 여러분의 코드는 전혀 다른 언어로 작성된 다른 커맨드 라인 도구들과 조합될 수 있습니다.\n이 장에서는 Bash, Python, R 세 가지 프로그래밍 언어에 집중할 것입니다.재사용 가능한 커맨드 라인 도구를 만드는 것이 장기적으로 여러분을 더 효율적이고 생산적인 데이터 과학자로 만들어 줄 것이라고 믿습니다.\n여러분은 기존 도구들을 꺼내어 이전에 마주했던 문제들에 적용할 수 있는 자신만의 데이터 과학 도구 상자를 점진적으로 구축해 나갈 것입니다.\n원라이너나 기존 코드를 커맨드 라인 도구로 바꿀 기회를 포착하는 데는 연습이 필요합니다.","code":""},{"path":"chapter-4-creating-command-line-tools.html","id":"개요-1","chapter":"제 4 커맨드 라인 도구 만들기","heading":"4.1 개요","text":"이 장에서 여러분은 다음 내용을 배우게 됩니다.원라이너를 매개변수가 있는 쉘 스크립트로 변환하기기존 Python 및 R 코드를 재사용 가능한 커맨드 라인 도구로 바꾸기이 장은 다음 파일들로 시작합니다.이 파일들을 얻는 방법은 2장에 설명되어 있습니다.\n그 외의 파일들은 커맨드 라인 도구를 사용하여 다운로드하거나 생성한 것들입니다.","code":"cd /data/ch04\nl"},{"path":"chapter-4-creating-command-line-tools.html","id":"원라이너를-쉘-스크립트로-변환하기","chapter":"제 4 커맨드 라인 도구 만들기","heading":"4.2 원라이너를 쉘 스크립트로 변환하기","text":"이 절에서는 원라이너를 재사용 가능한 커맨드 라인 도구로 만드는 방법을 설명하겠습니다.\n텍스트 한 조각에서 가장 자주 등장하는 단어들을 구하고 싶다고 해봅시다.\n가장 훌륭한 다른 책들처럼 프로젝트 구텐베르크에서 무료로 제공되는 루이스 캐럴의 이상한 나라의 앨리스를 가져와 보겠습니다.다음과 같은 도구의 시퀀스 혹은 파이프라인이 그 작업을 해줄 것입니다.➊ curl을 사용하여 전자책을 다운로드합니다.\n➋ tr48을 사용하여 전체 텍스트를 소문자로 변환합니다.\n➌ grep49을 사용하여 모든 단어를 추출하고 각 단어를 별도의 줄에 놓습니다.\n➍ sort50를 사용하여 이 단어들을 알파벳 순으로 정렬합니다.\n➎ uniq51를 사용하여 모든 중복을 제거하고 각 단어가 목록에 몇 번이나 나타나는지 셉니다.\n➏ sort를 사용하여 이 고유 단어 목록을 빈도수에 따라 내림차순으로 정렬합니다.\n➐ head를 사용하여 상위 10개 행(단어)만 남깁니다.이러한 단어들이 실제로 텍스트에서 가장 자주 나타납니다.\n이 단어들은 (“alice”를 제외하고) 많은 영어 텍스트에서 매우 빈번하게 나타나기 때문에 의미를 거의 담고 있지 않습니다.\n사실 이러한 단어들을 불용어(stopwords)라고 부릅니다.\n이들을 제거한다면 우리는 이 텍스트와 관련된 가장 빈번한 단어들을 얻게 될 것입니다.찾아둔 불용어 목록은 다음과 같습니다.grep을 사용하여 빈도수를 세기 직전에 불용어들을 필터링할 수 있습니다.➊ -f를 사용하여 파일(우리의 경우 stopwords)에서 한 줄에 하나씩 패턴을 가져옵니다. -F를 사용하여 이 패턴들을 고정 문자열로 해석합니다. -w를 사용하여 전체 단어를 형성하는 일치 항목들만 선택합니다. -v를 사용하여 일치하지 않는 줄들을 선택합니다.이 원라이너를 단 한 번만 실행하는 데는 아무런 문제가 없습니다.\n하지만 프로젝트 구텐베르크의 모든 전자책에 대해 상위 10개 단어를 얻고 싶다고 상상해 보세요.\n혹은 뉴스 웹사이트의 상위 10개 단어를 매시간 얻고 싶다고 상상해 보세요.\n그럴 경우에는 이 원라이너를 더 큰 무언가의 일부가 될 수 있는 별도의 빌딩 블록으로 가지는 것이 가장 좋습니다.\n매개변수 측면에서 이 원라이너에 유연성을 더하기 위해, 이를 쉘 스크립트로 바꿔봅시다.이를 통해 원라이너를 출발점으로 삼아 점진적으로 개선해 나갈 수 있습니다.\n이 원라이너를 재사용 가능한 커맨드 라인 도구로 바꾸기 위해, 다음의 6단계를 안내해 드리겠습니다.원라이너를 파일에 복사하여 붙여넣습니다.실행 권한을 추가합니다.소위 쉬뱅(shebang)을 정의합니다.고정된 입력 부분을 제거합니다.매개변수를 추가합니다.(선택 사항) PATH를 확장합니다.","code":"curl -sL \"https://www.gutenberg.org/files/11/11-0.txt\" | trimcurl -sL \"https://www.gutenberg.org/files/11/11-0.txt\" | ➊\ntr '[:upper:]' '[:lower:]' | ➋\ngrep -oE \"[a-z\\']{2,}\" | ➌\nsort | ➍\nuniq -c | ➎\nsort -nr | ➏\nhead -n 10 ➐curl -sL \"https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/stopwords-en.txt\" |\nsort | tee stopwords | trim 20curl -sL \"https://www.gutenberg.org/files/11/11-0.txt\" |\ntr '[:upper:]' '[:lower:]' |\ngrep -oE \"[a-z\\']{2,}\" |\nsort |\ngrep -Fvwf stopwords |\nuniq -c |\nsort -nr |\nhead -n 10"},{"path":"chapter-4-creating-command-line-tools.html","id":"단계-파일-생성","chapter":"제 4 커맨드 라인 도구 만들기","heading":"4.2.1 1단계: 파일 생성","text":"첫 번째 단계는 새 파일을 만드는 것입니다.\n평소 즐겨 쓰는 텍스트 에디터를 열고 원라이너를 붙여넣으세요.\n우리의 새로운 커맨드 라인 도구를 향한 첫걸음이라는 의미로 파일 이름을 top-words-1.sh라고 지읍시다. 만약 커맨드 라인에 머물고 싶다면, 내장 명령어인 fc(fix command의 줄임말)를 사용할 수 있습니다. fc는 마지막으로 실행한 명령어를 수정하거나 편집할 수 있게 해줍니다.fc를 실행하면 환경 변수 EDITOR에 저장된 기본 텍스트 에디터가 실행됩니다.\nDocker 컨테이너에서는 이것이 사용하기 쉬운 텍스트 에디터인 nano로 설정되어 있습니다.\n보시다시피 이 파일에는 우리의 원라이너가 들어 있습니다.Ctrl-O를 누르고 임시 파일 이름을 지운 뒤 top-words-1.sh를 입력하여 이 임시 파일에 제대로 된 이름을 붙여줍시다.Enter를 누르세요.다른 이름으로 저장할 것인지 묻는 확인창에서 Y를 누릅니다.Ctrl-X를 눌러 nano를 종료하고 원래 있던 곳으로 돌아갑니다.쉘 스크립트를 만들고 있다는 것을 분명히 하기 위해 파일 확장자 .sh를 사용하고 있습니다.\n하지만 커맨드 라인 도구에 확장자가 꼭 필요한 것은 아닙니다.\n실제로 커맨드 라인 도구는 확장자를 거의 갖지 않습니다.파일의 내용을 확인합니다.이제 bash52를 사용하여 파일의 명령어를 해석하고 실행할 수 있습니다.이렇게 하면 다음에 다시 원라이너를 입력할 필요가 없습니다.하지만 파일이 스스로 독립적으로 실행될 수 없으므로, 아직은 진정한 커맨드 라인 도구라고 할 수 없습니다.\n다음 단계에서 이를 바꿔봅시다.","code":"fc #! enter=FALSEEnter #! literal=FALSE, hold=0.2, wait=0.2C-O BSpace BSpace BSpace BSpace BSpace BSpace BSpace BSpace BSpace BSpace BSpace BSpace BSpace BSpace BSpace BSpace BSpace BSpace BSpace BSpace BSpace #! literal=FALSE\ntop-words-1.sh#! enter=FALSEEnter #! literal=FALSEY #! literal=FALSEpwd\nl\nbat top-words-1.shbash top-words-1.sh"},{"path":"chapter-4-creating-command-line-tools.html","id":"단계-실행-권한-부여","chapter":"제 4 커맨드 라인 도구 만들기","heading":"4.2.2 2단계: 실행 권한 부여","text":"파일을 직접 실행할 수 없는 이유는 올바른 접근 권한이 없기 때문입니다.\n특히 여러분이라는 사용자가 파일에 대한 실행 권한을 가지고 있어야 합니다.\n이 절에서는 파일의 접근 권한을 변경합니다.각 단계 간의 차이를 비교하기 위해 cp -v top-words-{1,2}.sh를 사용하여 파일을 top-words-2.sh로 복사합니다.파일의 접근 권한을 변경하려면 chmod53(change mode의 줄임말)라는 커맨드 라인 도구를 사용해야 합니다.\n이 도구는 특정 파일의 파일 모드 비트를 변경합니다.\n다음 명령어는 여러분이라는 사용자에게 top-words-2.sh에 대한 실행 권한을 부여합니다.인자 u+x는 세 문자로 구성됩니다. (1) u는 파일을 소유한 사용자(파일을 만든 여러분)를 위해 권한을 변경하고 싶음을 나타내고, (2) +는 권한을 추가하고 싶음을 나타내며, (3) x는 실행 권한을 나타냅니다.이제 두 파일의 접근 권한을 살펴봅시다.첫 번째 컬럼은 각 파일에 대한 접근 권한을 보여줍니다.\ntop-words-2.sh의 경우 -rwxrw-r--입니다.\n첫 번째 문자 -(하이픈)는 파일 형식을 나타냅니다.\n-는 일반 파일을, d는 디렉터리를 의미합니다.\n그다음 세 문자 rwx는 파일을 소유한 사용자에 대한 접근 권한을 나타냅니다.\nr과 w는 각각 읽기(read)와 쓰기(write)를 의미합니다.\n(보시다시피 top-words-1.sh는 x 대신 -를 가지고 있는데, 이는 그 파일을 실행할 수 없음을 의미합니다.) 그다음 세 문자 rw-는 파일을 소유한 그룹의 모든 구성원에 대한 접근 권한을 나타냅니다.\n마지막 세 문자 r--는 다른 모든 사용자에 대한 접근 권한을 나타냅니다.이제 다음과 같이 파일을 실행할 수 있습니다.만약 top-words-1.sh처럼 올바른 접근 권한이 없는 파일을 실행하려고 하면 다음과 같은 오류 메시지가 보일 것입니다.","code":"cp -v top-words-{1,2}.sh\nchmod u+x top-words-2.shl top-words-{1,2}.sh./top-words-2.sh./top-words-1.sh"},{"path":"chapter-4-creating-command-line-tools.html","id":"단계-쉬뱅shebang-정의","chapter":"제 4 커맨드 라인 도구 만들기","heading":"4.2.3 3단계: 쉬뱅(Shebang) 정의","text":"파일을 독립적으로 실행할 수는 있지만, 파일에 소위 쉬뱅(shebang)이라고 불리는 것을 추가해야 합니다.\n쉬뱅은 시스템에 명령어를 해석하는 데 어떤 실행 파일을 사용해야 하는지 알려주는 명령 스크립트의 특별한 줄입니다.쉬뱅이라는 이름은 해시()와 느낌표(bang)인 처음 두 문자 #!에서 유래했습니다.\n이전 단계에서 했던 것처럼 이를 빠뜨리는 것은 좋지 않은데, 각 쉘마다 기본 실행 파일이 다르기 때문입니다.\n책 전체에서 사용하고 있는 Z 쉘(Z shell)은 쉬뱅이 정의되지 않은 경우 기본적으로 /bin/sh 실행 파일을 사용합니다.\n이 경우에는 bash가 명령어를 해석하도록 하고 싶은데, 이는 sh보다 더 많은 기능을 제공하기 때문입니다.다시 말하지만, 어떤 에디터를 사용하든 자유지만 저는 Docker 이미지에 설치된 nano54를 계속 사용하겠습니다.가서 #!/usr/bin/env bash를 타이핑하고 Enter를 누르세요.\n준비가 되면 Ctrl-X를 눌러 저장하고 종료합니다.파일을 저장할 것인지 묻는 확인창에서 Y를 누릅니다.top-words-3.sh가 어떻게 생겼는지 확인해 봅시다.우리가 필요한 바로 그 모습입니다. 원래의 파이프라인 앞에 쉬뱅이 붙어 있습니다.가끔 #!/usr/bin/bash나 #!/usr/bin/python(다음 절에서 보게 될 파이썬의 경우) 형태의 쉬뱅을 가진 스크립트를 보게 될 것입니다.\n대개는 잘 작동하지만, 만약 bash나 python55 실행 파일이 /usr/bin이 아닌 다른 위치에 설치되어 있다면 그 스크립트는 더 이상 작동하지 않습니다.\n제가 여기서 보여드린 형태인 #!/usr/bin/env bash나 #!/usr/bin/env python을 사용하는 것이 더 좋습니다. 왜냐하면 env56 실행 파일은 bash와 python이 어디에 설치되어 있는지 알고 있기 때문입니다.\n즉, env를 사용하는 것이 여러분의 스크립트를 더 이식성 있게(portable) 만듭니다.","code":"cp -v top-words-{2,3}.sh\nnano top-words-3.sh #! enter=FALSEEnter #! literal=FALSE# ! #! literal=FALSE\n/usr/bin/env bash #! expect_prompt=FALSE\nC-X #! literal=FALSEY #! literal=FALSEbat top-words-3.sh"},{"path":"chapter-4-creating-command-line-tools.html","id":"단계-고정된-입력-제거","chapter":"제 4 커맨드 라인 도구 만들기","heading":"4.2.4 4단계: 고정된 입력 제거","text":"이제 커맨드 라인에서 실행할 수 있는 유효한 커맨드 라인 도구를 갖게 되었습니다.\n하지만 여기서 더 개선할 수 있습니다.\n우리의 커맨드 라인 도구를 더 재사용 가능하게 만들 수 있습니다.\n파일의 첫 번째 명령어는 curl인데, 가장 자주 쓰이는 10개 단어를 얻고자 하는 텍스트를 다운로드합니다.\n즉, 데이터와 작업이 하나로 합쳐져 있습니다.다른 전자책이나 다른 텍스트에서 상위 10개 단어를 얻고 싶다면 어떻게 될까요? 입력 데이터가 도구 자체 내에 고정되어 있습니다.\n입력 데이터를 커맨드 라인 도구에서 분리하는 것이 더 낫습니다.커맨드 라인 도구의 사용자가 텍스트를 제공한다고 가정하면, 도구는 범용적으로 적용될 수 있게 됩니다.\n따라서 해결책은 스크립트에서 curl 명령어를 제거하는 것입니다.\ntop-words-4.sh라는 이름의 업데이트된 스크립트는 다음과 같습니다.이것이 작동하는 이유는 tr과 같이 표준 입력으로부터 데이터를 필요로 하는 명령어로 스크립트를 시작하면, 커맨드 라인 도구에 전달된 입력을 그대로 받기 때문입니다.\n예를 들면 다음과 같습니다.","code":"cp -v top-words-{3,4}.sh\nsed -i '2d' top-words-4.sh\nbat top-words-4.shcurl -sL 'https://www.gutenberg.org/files/11/11-0.txt' | ./top-words-4.sh\ncurl -sL 'https://www.gutenberg.org/files/12/12-0.txt' | ./top-words-4.sh\nman bash | ./top-words-4.sh"},{"path":"chapter-4-creating-command-line-tools.html","id":"단계-인자argument-추가","chapter":"제 4 커맨드 라인 도구 만들기","heading":"4.2.5 5단계: 인자(Argument) 추가","text":"커맨드 라인 도구를 더욱 재사용 가능하게 만들기 위한 한 단계가 더 남았습니다: 바로 매개변수입니다.\n우리의 커맨드 라인 도구에는 몇 가지 고정된 커맨드 라인 인자가 있습니다. 예를 들어 sort를 위한 -nr이나 head를 위한 -n 10 같은 것들입니다.\n전자의 인자는 고정된 상태로 두는 것이 좋을 것입니다.\n하지만 head 명령어에 대해 서로 다른 값을 허용하는 것은 매우 유용할 것입니다.\n이렇게 하면 최종 사용자가 출력할 “가장 자주 사용되는 단어의 개수”를 직접 설정할 수 있습니다.\n파일 top-words-5.sh의 모습은 다음과 같습니다.변수 NUM_WORDS는 Bash의 특별한 변수인 $1 값으로 설정됩니다. 이는 커맨드 라인 도구에 전달된 첫 번째 커맨드 라인 인자를 담고 있습니다. 아래 표는 Bash가 제공하는 다른 특수 변수들을 나열합니다. 만약 값이 지정되지 않으면 기본값으로 “10”을 가지게 됩니다.$NUM_WORDS 변수의 값을 사용하려면 앞에 달러 기호를 붙여야 한다는 점에 유의하세요. 값을 설정할 때는 달러 기호를 쓰지 않습니다.head의 인자로 $1을 직접 사용할 수도 있고 NUM_WORDS와 같은 추가 변수를 만드는 수고를 하지 않아도 됩니다.\n하지만 스크립트가 커지고 $2, $3과 같은 더 많은 커맨드 라인 인자가 생길 때는 이름이 붙은 변수를 사용하는 것이 코드의 가독성을 높여줍니다.이제 텍스트의 상위 20개 단어를 보고 싶다면 다음과 같이 커맨드 라인 도구를 호출하면 됩니다.만약 사용자가 숫자를 지정하지 않으면 스크립트는 상위 10개 단어를 보여줍니다.","code":"bat top-words-5.shcurl -sL \"https://www.gutenberg.org/files/11/11-0.txt\" > alice.txt\n< alice.txt ./top-words-5.sh 20< alice.txt ./top-words-5.sh"},{"path":"chapter-4-creating-command-line-tools.html","id":"단계-path-확장","chapter":"제 4 커맨드 라인 도구 만들기","heading":"4.2.6 6단계: PATH 확장","text":"앞선 5단계를 거쳐 마침내 재사용 가능한 커맨드 라인 도구 구축을 마쳤습니다.\n하지만 매우 유용할 수 있는 단계가 하나 더 있습니다.\n이 선택적인 단계에서는 여러분의 커맨드 라인 도구를 어디에서나 실행할 수 있도록 보장할 것입니다.현재로서는 커맨드 라인 도구를 실행하고 싶을 때, 해당 도구가 있는 디렉터리로 이동하거나 2단계에서 보여드린 것처럼 전체 경로 이름을 포함해야 합니다.\n커맨드 라인 도구가 가령 특정 프로젝트만을 위해 만들어진 것이라면 괜찮습니다.\n하지만 도구가 여러 상황에서 적용될 수 있다면, 우분투와 함께 제공되는 도구들처럼 어디서나 실행할 수 있는 것이 유용합니다.이를 달성하기 위해 Bash는 여러분의 커맨드 라인 도구를 어디서 찾아야 할지 알아야 합니다.\nBash는 PATH라 불리는 환경 변수에 저장된 디렉터리 목록을 탐색함으로써 이 작업을 수행합니다.\n새로 생성된 Docker 컨테이너에서 PATH는 다음과 같습니다.디렉터리들은 콜론으로 구분됩니다.\n콜론을 줄바꿈으로 변환(translating)하여 디렉터리 목록으로 출력할 수 있습니다.PATH를 영구적으로 변경하려면 홈 디렉터리에 있는 .bashrc 또는 .profile 파일을 편집해야 합니다.\n모든 사용자 지정 커맨드 라인 도구를 한 디렉터리(예: ~/tools)에 모아둔다면 PATH를 한 번만 변경하면 됩니다.\n그러면 이제 더 이상 ./를 붙일 필요 없이 파일 이름만 사용할 수 있습니다.\n나아가 도구가 어디에 위치해 있는지 기억할 필요도 없습니다.","code":"echo $PATHecho $PATH | tr ':' '\\n'cp -v top-words{-5.sh,}\nexport PATH=\"${PATH}:/data/ch04\"\necho $PATH\ncurl \"https://www.gutenberg.org/files/11/11-0.txt\" |\ntop-words 10"},{"path":"chapter-4-creating-command-line-tools.html","id":"python과-r로-커맨드-라인-도구-만들기","chapter":"제 4 커맨드 라인 도구 만들기","heading":"4.3 Python과 R로 커맨드 라인 도구 만들기","text":"앞 절에서 우리가 만든 커맨드 라인 도구는 Bash로 작성되었습니다.\n(Bash 프로그래밍 언어의 모든 기능이 사용된 것은 아니지만, 해석기는 여전히 bash였습니다.) 이제 아시다시피 커맨드 라인은 언어에 구애받지 않으므로 커맨드 라인 도구를 만드는 데 반드시 Bash를 사용해야 하는 것은 아닙니다.이 절에서는 커맨드 라인 도구가 다른 프로그래밍 언어로도 만들어질 수 있음을 보여드리겠습니다.\n데이터 과학 커뮤니티에서 가장 인기 있는 두 언어인 Python과 R에 집중하겠습니다.\n두 언어에 대한 완전한 입문을 제공할 수는 없으므로, 여러분이 Python이나 R에 어느 정도 익숙하다고 가정하겠습니다.\nJava, Go, Julia와 같은 다른 언어들도 커맨드 라인 도구를 만드는 데 있어서는 비슷한 패턴을 따릅니다.Bash가 아닌 다른 언어로 커맨드 라인 도구를 만드는 데는 세 가지 주요 이유가 있습니다.\n첫째, 커맨드 라인에서 사용하고 싶은 기존 코드가 이미 있을 수 있습니다.\n둘째, 커맨드 라인 도구가 수백 줄의 Bash 코드로 구성될 정도로 복잡해질 수 있습니다.\n셋째, 도구가 더 안전하고 견고해야 할 때입니다(Bash는 타입 체크와 같은 많은 기능이 부족합니다).이전 절에서 논의한 6단계는 다른 언어로 커맨드 라인 도구를 만들 때도 대략적으로 적용됩니다.\n하지만 첫 번째 단계는 커맨드 라인에서 복사해서 붙여넣는 것이 아니라, 관련 코드를 새 파일에 복사해서 붙여넣는 것이 될 것입니다.\nPython과 R로 작성된 커맨드 라인 도구는 쉬뱅 뒤에 각각 python과 Rscript57를 해석기로 지정해야 합니다.Python과 R을 사용하여 커맨드 라인 도구를 만들 때 특히 주의를 기울여야 할 두 가지 측면이 더 있습니다.\n첫째, 쉘 스크립트에는 자연스러운 표준 입력 처리를 Python과 R에서는 명시적으로 처리해야 합니다.\n둘째, Python과 R로 작성된 커맨드 라인 도구는 더 복잡해지는 경향이 있으므로 사용자에게 더 정교한 커맨드 라인 인자를 지정할 수 있는 기능을 제공하고 싶을 수 있습니다.","code":""},{"path":"chapter-4-creating-command-line-tools.html","id":"쉘-스크립트-포팅하기","chapter":"제 4 커맨드 라인 도구 만들기","heading":"4.3.1 쉘 스크립트 포팅하기","text":"시작점으로, 방금 만든 쉘 스크립트를 Python과 R로 어떻게 포팅하는지 살펴봅시다.\n다시 말해, 어떤 Python과 R 코드가 표준 입력으로부터 가장 자주 쓰이는 단어들을 우리에게 줄까요? 먼저 top-words.py와 top-words.R 두 파일을 보여드리고 쉘 코드와의 차이점을 논의하겠습니다.\nPython에서 코드는 다음과 같을 것입니다.이 Python 예제는 서드파티 패키지를 사용하지 않았음에 유의하세요.\n고급 텍스트 처리를 원한다면 NLTK 패키지58를 확인해 보기를 권장합니다.\n많은 수치 데이터를 다룰 예정이라면 Pandas 패키지59 사용을 추천합니다.그리고 R에서 코드는 다음과 같을 것입니다.세 가지 구현(Bash, Python, R)이 동일한 빈도수로 동일한 상위 5개 단어를 반환하는지 확인해 봅시다.훌륭합니다! 물론 출력 결과물 자체가 매우 흥미진진한 것은 아닙니다.\n흥미로운 점은 우리가 여러 언어로 동일한 과업을 완수할 수 있다는 사실입니다.\n접근 방식 간의 차이점을 살펴봅시다.우선 즉각적으로 명백한 것은 코드의 양 차이입니다.\n이 특정 과업에 대해서는 Python과 R 모두 Bash보다 훨씬 더 많은 코드를 요구합니다.\n이는 어떤 작업에는 커맨드 라인을 사용하는 것이 더 낫다는 점을 잘 보여줍니다.\n다른 작업에는 프로그래밍 언어를 사용하는 것이 더 나을 것입니다.\n커맨드 라인에서 더 많은 경험을 쌓을수록 언제 어떤 접근 방식을 사용해야 할지 인식하기 시작할 것입니다.\n모든 것이 커맨드 라인 도구일 때는 과업을 하위 과업으로 나누어 Bash 커맨드 라인 도구와 예를 들어 Python 커맨드 라인 도구를 결합할 수도 있습니다.\n당면한 과제에 가장 잘 맞는 접근 방식을 사용하면 됩니다.","code":"cd /data/ch04\nbat top-words.pybat top-words.Rtime < alice.txt top-words 5\ntime < alice.txt top-words.py 5\ntime < alice.txt top-words.R 5"},{"path":"chapter-4-creating-command-line-tools.html","id":"표준-입력으로부터-스트리밍-데이터-처리하기","chapter":"제 4 커맨드 라인 도구 만들기","heading":"4.3.2 표준 입력으로부터 스트리밍 데이터 처리하기","text":"이전의 두 코드 조각에서 Python과 R은 모두 표준 입력 전체를 한 번에 읽었습니다.\n커맨드 라인에서 대부분의 도구는 데이터를 스트리밍 방식으로 다음 도구로 파이프합니다.\nsort와 같이 표준 출력에 데이터를 쓰기 전에 전체 데이터를 필요로 하는 도구들이 몇몇 있습니다.\n이는 파이프라인이 그런 도구들에 의해 막힌다는 것을 의미합니다.\n파일과 같이 입력 데이터가 유한하다면 이는 문제가 되지 않을 수 있습니다.\n하지만 입력 데이터가 끊이지 않는 스트림일 때, 그런 블로킹(blocking) 커맨드 라인 도구들은 쓸모가 없습니다.다행히 Python과 R은 스트리밍 데이터 처리를 지원합니다.\n예를 들어 줄 단위로 함수를 적용할 수 있습니다.\n여기 Python과 R에서 이것이 어떻게 작동하는지 보여주는 최소한의 예제 두 가지가 있습니다.Python과 R 도구 모두 이제는 악명 높은 Fizz Buzz 문제를 해결합니다. 이 문제의 정의는 다음과 같습니다: 1부터 100까지 숫자를 출력하되, 3으로 나누어지면 “fizz”를, 5로 나누어지면 “buzz”를, 15로 나누어지면 “fizzbuzz”를 대신 출력합니다. 여기 파이썬 코드입니다60.그리고 여기 R 코드입니다.두 도구를 테스트해 봅시다(공간을 절약하기 위해 출력을 column으로 파이프합니다).출력이 정확해 보이네요!\n이 두 도구가 실제로 스트리밍 방식으로 작동한다는 것을 증명하기는 어렵습니다.\n입력 데이터를 sample -d 100으로 파이프한 뒤 Python이나 R 도구로 넘김으로써 이를 직접 확인할 수 있습니다.\n그렇게 하면 각 줄 사이에 약간의 지연이 추가되어 도구들이 모든 입력 데이터를 기다리지 않고 줄 단위로 작동한다는 것을 더 쉽게 확인할 수 있습니다.","code":"bat fizzbuzz.pybat fizzbuzz.Rseq 30 | fizzbuzz.py | column -x\nseq 30 | fizzbuzz.R | column -x"},{"path":"chapter-4-creating-command-line-tools.html","id":"요약-3","chapter":"제 4 커맨드 라인 도구 만들기","heading":"4.4 요약","text":"이 막간 장에서 여러분만의 커맨드 라인 도구를 만드는 방법을 보여드렸습니다.\n여러분의 코드를 재사용 가능한 빌딩 블록으로 바꾸는 데는 단 6단계면 충분합니다.\n그것이 여러분을 훨씬 더 생산적으로 만들어 준다는 것을 알게 될 것입니다.\n여러분만의 도구를 만들 기회를 계속해서 찾아보시기를 조언합니다.\n다음 장에서는 데이터 과학을 위한 OSEMN 모델의 두 번째 단계인 데이터 정제를 다룹니다.","code":""},{"path":"chapter-4-creating-command-line-tools.html","id":"더-읽을거리-3","chapter":"제 4 커맨드 라인 도구 만들기","heading":"4.5 더 읽을거리","text":"도구에 기억해야 할 옵션이 많아지고 다른 사람들과 도구를 공유하고 싶을 때 도움말 문서를 추가하는 것이 중요해집니다. docopt는 도움말을 제공하고 도구가 허용하는 가능한 옵션을 정의하기 위한 언어 중립적인 프레임워크입니다. Bash, Python, R을 포함한 거의 모든 언어에서 구현체를 사용할 수 있습니다.Bash 프로그래밍에 대해 더 알고 싶다면 Arnold Robbins와 Nelson Beebe의 Classic Shell Programming과 Carl Albing과 JP Vossen의 Bash Cookbook을 추천합니다.견고하고 안전한 Bash 스크립트를 작성하는 것은 꽤 까다롭습니다. ShellCheck는 여러분의 Bash 코드에서 실수와 취약점을 점검해 주는 온라인 도구입니다. 커맨드 라인 도구로도 사용 가능합니다.Joel Grus의 저서 Ten Essays Fizz Buzz는 파이썬으로 Fizz Buzz를 해결하는 10가지 서로 다른 방식에 대한 통찰력 있고 재미있는 모음집입니다.","code":""},{"path":"chapter-5-scrubbing-data.html","id":"chapter-5-scrubbing-data","chapter":"제 5 데이터 정제하기","heading":"제 5 데이터 정제하기","text":"두 장 전, 데이터 과학을 위한 OSEMN 모델의 첫 번째 단계에서 우리는 다양한 소스로부터 데이터를 획득(obtaining)하는 방법을 살펴보았습니다.\n이번 장은 두 번째 단계인 데이터를 정제(scrubbing)하는 것에 관한 모든 내용을 다룹니다.\n데이터를 얻자마자 즉시 탐색(exploring)하거나 모델링(modeling)을 계속할 수 있는 경우는 매우 드뭅니다.\n데이터에 먼저 약간의 청소, 즉 정제가 필요한 이유는 수없이 많습니다.우선, 데이터가 원하는 형식이 아닐 수 있습니다.\n예를 들어, API에서 JSON 데이터를 얻었지만 시각화를 만들기 위해 CSV 형식이 필요할 수 있습니다.\n다른 일반적인 형식으로는 일반 텍스트(plain text), HTML, XML 등이 있습니다.\n대부분의 커맨드 라인 도구는 한두 가지 형식만 지원하므로, 데이터를 한 형식에서 다른 형식으로 변환할 수 있는 능력을 갖추는 것이 중요합니다.데이터가 원하는 형식이 되었더라도 결측치(missing values), 불일치(inconsistencies), 이상한 문자, 또는 불필요한 부분과 같은 문제가 여전히 남아 있을 수 있습니다.\n필터를 적용하고, 값을 바꾸고, 여러 파일을 결합함으로써 이를 해결할 수 있습니다.\n커맨드 라인은 이러한 종류의 변환에 특히 잘 어울립니다. 사용 가능한 전문 도구가 많고, 그중 대부분은 대량의 데이터를 처리할 수 있기 때문입니다.\n이 장에서는 grep61과 awk62 같은 고전적인 도구부터 jq63와 pup64 같은 최신 도구까지 다룰 것입니다.가끔은 동일한 커맨드 라인 도구를 사용하여 여러 작업을 수행하거나, 여러 도구가 사용하여 동일한 작업을 수행할 수도 있습니다.\n이 장은 커맨드 라인 도구 자체를 깊게 파고들기보다는 문제나 레시피(recipe)에 초점을 맞춘 요리책(cookbook)과 같은 구조로 되어 있습니다.","code":""},{"path":"chapter-5-scrubbing-data.html","id":"개요-2","chapter":"제 5 데이터 정제하기","heading":"5.1 개요","text":"이 장에서 여러분은 다음 내용을 배우게 됩니다.데이터를 한 형식에서 다른 형식으로 변환하기CSV에 직접 SQL 쿼리 적용하기행(line) 필터링하기값 추출 및 교체하기열(column) 분할, 병합, 추출하기여러 파일 결합하기이 장은 다음 파일들로 시작합니다.이 파일들을 얻는 방법은 2장에 설명되어 있습니다.\n그 외의 파일들은 커맨드 라인 도구를 사용하여 다운로드하거나 생성한 것들입니다.실제 변환 작업을 살펴보기 전에, 커맨드 라인에서 작업할 때 이러한 변환이 얼마나 도처에 널려 있는지 보여드리고 싶습니다.","code":"cd /data/ch05\nl"},{"path":"chapter-5-scrubbing-data.html","id":"변환-어디에나-있는-변환들","chapter":"제 5 데이터 정제하기","heading":"5.2 변환, 어디에나 있는 변환들","text":"1장에서 실제 데이터 과학의 OSEMN 모델 단계들이 선형적으로만 따라가지는 않는다고 언급했습니다.\n이러한 맥락에서, 정제(scrubbing)가 OSEMN 모델의 두 번째 단계이긴 하지만, 단지 획득한 데이터만이 정제가 필요한 것은 아니라는 점을 알아두셨으면 합니다.\n이 장에서 배울 변환 기술들은 파이프라인의 어느 부분에서든, 그리고 OSEMN 모델의 어느 단계에서든 유용할 수 있습니다.\n일반적으로 한 커맨드 라인 도구가 다음 도구에서 즉시 사용 가능한 출력을 생성한다면, 파이프 연산자(|)를 사용하여 두 도구를 연결할 수 있습니다.\n그렇지 않다면, 파이프라인 중간에 변환 도구를 삽입하여 데이터에 변환을 먼저 적용해야 합니다.이를 더 구체적으로 만들기 위해 예시를 하나 들어보겠습니다.\n여러분에게 fizzbuzz 시퀀스의 처음 100개 항목이 있고(4장 참조), fizz, buzz, fizzbuzz라는 단어가 얼마나 자주 나타나는지 막대 그래프로 시각화하고 싶다고 가정해 봅시다.\n아직 익숙하지 않은 도구가 사용되더라도 걱정하지 마세요. 나중에 모두 자세히 다룰 것입니다.먼저 시퀀스를 생성하여 데이터를 얻고 이를 fb.seq에 씁니다.➊ 사용자 정의 도구인 fizzbuzz.py는 4장에서 가져온 것입니다.그런 다음 grep을 사용하여 fizz 또는 buzz 패턴과 일치하는 행만 남기고, sort와 uniq65를 사용하여 각 단어의 빈도를 셉니다.➊ 이 정규 표현식은 fizzbuzz와도 일치합니다.\n➋ sort와 uniq를 이런 식으로 사용하는 것은 행의 개수를 세고 내림차순으로 정렬하는 일반적인 방법입니다. 개수를 추가하는 것은 -c 옵션입니다.sort가 두 번 사용된 점에 주목하세요. 첫 번째는 uniq가 정렬된 데이터를 입력으로 기대하기 때문이고, 두 번째는 개수를 숫자순으로 정렬하기 위함입니다.\n어떤 의미에서 이는 미묘하긴 하지만 중간 변환 과정입니다.다음 단계는 rush66를 사용하여 빈도를 시각화하는 것입니다.\n하지만 rush는 입력 데이터가 CSV 형식일 것으로 예상하므로, 그전에 명확한 변환 과정이 필요합니다.\nawk를 사용하면 헤더를 추가하고 두 필드의 순서를 바꾸고 쉼표를 삽입하는 작업을 한 번에 수행할 수 있습니다.이제 rush를 사용하여 막대 그래프를 만들 준비가 되었습니다.\n결과는 Figure ??를 확인하세요.\n(rush의 이 구문은 7장에서 자세히 다룰 것입니다.)이 예제는 약간 인위적이긴 하지만 커맨드 라인에서 작업할 때 흔히 나타나는 패턴을 보여줍니다.\n데이터를 획득하거나 시각화하거나 모델을 학습시키는 등의 핵심 도구들은 파이프라인으로 연결되기 위해 중간 변환을 필요로 하는 경우가 많습니다.\n그런 의미에서 파이프라인을 작성하는 것은 핵심 조각들이 서로 맞물리기 위해 도우미 조각들을 필요로 하는 퍼즐을 맞추는 것과 같습니다.이제 데이터 정제의 중요성을 확인했으니, 실제 변환 기술들에 대해 배울 준비가 되었습니다.","code":"seq 100 |\n/data/ch04/fizzbuzz.py |\ntee fb.seq | trimgrep -E \"fizz|buzz\" fb.seq | ➊\nsort | uniq -c | sort -nr > fb.cnt ➋\nbat -A fb.cnt< fb.cnt awk 'BEGIN { print \"value,count\" } { print $2\",\"$1 }' > fb.csv\nbat fb.csv\ncsvlook fb.csvrush plot -x value -y count --geom col --height 2 fb.csv > fb.png\ndisplay fb.png"},{"path":"chapter-5-scrubbing-data.html","id":"일반-텍스트-plain-text","chapter":"제 5 데이터 정제하기","heading":"5.3 일반 텍스트 (Plain Text)","text":"엄밀히 말하면, 일반 텍스트는 사람이 읽을 수 있는 문자들의 시퀀스와 선택적으로 탭이나 줄바꿈 같은 특정 유형의 제어 문자들을 의미합니다67.\n로그, 전자책, 이메일, 소스 코드 등이 그 예입니다.\n일반 텍스트는 바이너리 데이터에 비해 다음과 같은 많은 이점을 가집니다68.어떤 텍스트 에디터로도 열고 수정하고 저장할 수 있습니다.자기 기술적(self-describing)이며 이를 생성한 애플리케이션으로부터 독립적입니다.데이터를 처리하는 데 추가적인 지식이나 애플리케이션이 필요하지 않으므로 다른 데이터 형식보다 더 오래 살아남을 것입니다.하지만 가장 중요한 점은, 유닉스 철학이 일반 텍스트를 커맨드 라인 도구 간의 보편적인 인터페이스로 간주한다는 점입니다69.\n즉, 대부분의 도구는 일반 텍스트를 입력으로 받고 일반 텍스트를 출력으로 내보냅니다.이것만으로도 제가 일반 텍스트부터 시작할 이유는 충분합니다.\n이 장에서 다룰 다른 형식들인 CSV, JSON, XML, HTML 또한 사실 일반 텍스트입니다.\n우선은 일반 텍스트가 (CSV처럼) 명확한 표 구조나 (JSON, XML, HTML처럼) 중첩된 구조를 가지고 있지 않다고 가정하겠습니다.\n이 장의 뒷부분에서 이러한 형식들을 다루기 위해 특별히 설계된 도구들을 소개하겠습니다.","code":""},{"path":"chapter-5-scrubbing-data.html","id":"행-필터링하기","chapter":"제 5 데이터 정제하기","heading":"5.3.1 행 필터링하기","text":"첫 번째 정제 작업은 행을 필터링하는 것입니다.\n이는 입력 데이터에서 각 행을 유지할지 버릴지 평가하는 것을 의미합니다.","code":""},{"path":"chapter-5-scrubbing-data.html","id":"위치에-기반한-필터링","chapter":"제 5 데이터 정제하기","heading":"5.3.1.1 위치에 기반한 필터링","text":"행을 필터링하는 가장 간단한 방법은 위치에 기반하는 것입니다.\n이는 파일의 첫 10행을 검사하고 싶거나, 다른 커맨드 라인 도구의 출력에서 특정 행을 추출하고 싶을 때 유용할 수 있습니다.\n위치 기반 필터링을 설명하기 위해 10개의 행을 가진 더미 파일을 만들어 봅시다.head70, sed71, 또는 awk를 사용하여 처음 3행을 인쇄할 수 있습니다.➊ awk에서 NR은 지금까지 확인한 총 입력 레코드 수를 나타냅니다.마찬가지로 tail72을 사용하여 마지막 3행을 인쇄할 수 있습니다.이 작업에도 sed나 awk를 사용할 수 있지만 tail이 훨씬 빠릅니다.\n처음 3행을 제거하는 방법은 다음과 같습니다.tail에서는 출력하고 싶은 행 번호에 1을 더해서 지정해야 한다는 점에 유의하세요.\n인쇄를 시작하고 싶은 행이라고 생각하면 됩니다.\nhead를 사용하여 마지막 3행을 지울 수도 있습니다.sed, awk, 또는 head와 tail의 조합을 사용하여 특정 행들을 인쇄할 수 있습니다.\n여기서는 4, 5, 6행을 인쇄합니다.sed에서 시작 지점과 단계를 지정하거나 awk에서 머듈로(modulo) 연산자를 사용하여 홀수 행만 인쇄할 수 있습니다.짝수 행 인쇄도 비슷한 방식으로 작동합니다.","code":"seq -f \"Line %g\" 10 | tee lines< lines head -n 3\n< lines sed -n '1,3p'\n< lines awk 'NR <= 3'< lines tail -n 3< lines tail -n +4\n< lines sed '1,3d'\n< lines sed -n '1,3!p'< lines head -n -3< lines sed -n '4,6p'\n< lines awk '(NR>=4) && (NR<=6)'\n< lines head -n 6 | tail -n 3< lines sed -n '1~2p'\n< lines awk 'NR%2'< lines sed -n '0~2p'\n< lines awk '(NR+1)%2'"},{"path":"chapter-5-scrubbing-data.html","id":"패턴에-기반한-필터링","chapter":"제 5 데이터 정제하기","heading":"5.3.1.2 패턴에 기반한 필터링","text":"가끔은 내용에 따라 행을 유지하거나 버리고 싶을 때가 있습니다.\n행을 필터링하는 표준적인 커맨드 라인 도구인 grep을 사용하면 특정 패턴이나 정규 표현식과 일치하는 모든 행을 인쇄할 수 있습니다.\n예를 들어, 이상한 나라의 앨리스에서 모든 장(chapter)의 제목을 추출해 보겠습니다.➊ -옵션은 대소문자를 구분하지 않고 매칭하도록 지정합니다.정규 표현식을 지정할 수도 있습니다.\n예를 들어, The로 시작하는 제목만 인쇄하고 싶다면 다음과 같이 합니다.정규 표현션을 활성화하려면 -E 옵션을 지정해야 한다는 점에 유의하세요.\n그렇지 않으면 grep은 패턴을 리터럴 문자열로 해석하여 검색 결과가 나오지 않을 가능성이 큽니다.-v 옵션을 사용하면 일치 항목을 반전시켜서 패턴과 일치하지 않는 행들을 인쇄합니다.\n아래의 정규 표현식은 공백 문자만 포함된 행과 일치합니다.\n따라서 이를 반전 시키고 wc -l을 사용하면 비어있지 않은 행의 개수를 셀 수 있습니다.","code":"< alice.txt grep -i chapter< alice.txt grep -E '^CHAPTER (.*)\\. The'< alice.txt grep '^CHAPTER (.*)\\. The'< alice.txt grep -Ev '^\\s$' | wc -l"},{"path":"chapter-5-scrubbing-data.html","id":"무작위성에-기반한-필터링","chapter":"제 5 데이터 정제하기","heading":"5.3.1.3 무작위성에 기반한 필터링","text":"데이터 파이프라인을 구성하는 과정에서 데이터 양이 아주 많다면 파이프라인을 디버깅하는 것이 번거로울 수 있습니다.\n그럴 때 데이터에서 더 작은 샘플을 생성하는 것이 유용할 수 있습니다.\n이때 sample73이 요긴하게 쓰입니다.\nsample의 주요 목적은 입력 데이터에서 각 행마다 일정 비율만큼만 출력하여 데이터의 부분 집합을 얻는 것입니다.여기서는 모든 입력 행이 인쇄될 확률이 1%입니다.\n이 백분율은 분수(예: 1/100)나 확률(예: 0.01)로도 지정할 수 있습니다.sample은 파이프라인을 디버깅할 때 유용한 두 가지 다른 용도가 있습니다.\n첫째, 출력에 약간의 지연을 추가할 수 있습니다.\n이는 입력이 지속적인 스트림일 때(예: 3장에서 본 위키피디아 스트림), 데이터가 너무 빨리 들어와서 무슨 일이 일어나고 있는지 확인하기 어려울 때 유용합니다.\n둘째, sample에 타이머를 설정하여 진행 중인 프로세스를 수동으로 종료할 필요가 없게 할 수 있습니다.\n예를 들어, 각 행이 인쇄될 때 1초의 지연을 추가하고 5초 동안만 실행하려면 다음과 같이 입력합니다.➊ ts74 도구는 각 행의 맨 앞에 타임스탬프를 추가합니다.불필요한 계산을 방지하기 위해 파이프라인에서 가능한 한 앞부분에 sample을 배치하도록 하세요.\n사실 이 논리는 head나 tail처럼 데이터를 줄여주는 모든 커맨드 라인 도구에 적용됩니다.\n파이프라인이 제대로 작동한다는 확신이 들면 파이프라인에서 이를 제거하면 됩니다.","code":"seq -f \"Line %g\" 1000 | sample -r 1%seq -f \"Line %g\" 1000 | sample -r 1% -d 1000 -s 5 | ts"},{"path":"chapter-5-scrubbing-data.html","id":"값-추출하기","chapter":"제 5 데이터 정제하기","heading":"5.3.2 값 추출하기","text":"앞의 예제에서 실제 장 제목을 추출하려면, grep의 출력을 cut으로 파이핑하는 간단한 접근 방식을 사용할 수 있습니다.여기서 cut으로 전달된 각 행은 공백을 기준으로 필드(field)로 나뉘고, 세 번째 필드부터 마지막 필드까지 인쇄됩니다.\n필드의 총 개수는 입력 행마다 다를 수 있습니다.\nsed를 사용하면 훨씬 더 복잡한 방식으로 동일한 작업을 수행할 수 있습니다.(출력이 동일하므로 3행으로 잘랐습니다.) 이 방식은 정규 표현식과 후방 참조(back reference)를 사용합니다.\n여기서 sed는 grep이 하던 작업까지 도맡았습니다.\n더 간단한 방법으로는 해결되지 않을 정도로 복잡한 상황에서만 이 방식을 사용하는 것을 권장합니다.\n예를 들어, chapter라는 단어가 단순히 새 장의 시작을 알리는 용도뿐만 아니라 텍스트 자체의 일부로 쓰인 경우가 있을 때 그렇습니다.\n물론 이를 해결할 수 있는 다양한 수준의 복잡한 방법이 있겠지만, 여기서는 매우 엄격한 접근 방식을 예로 보여 드린 것입니다.\n실제로 중요한 과제는 복잡성과 유연성 사이에서 균형을 잘 잡는 파이프라인을 고안하는 것입니다.cut은 문자 위치를 기준으로도 나눌 수 있다는 점을 언급할 가치가 있습니다.\n이는 입력 행마다 동일한 위치의 문자 집합을 추출(또는 제거)하고 싶을 때 유용합니다.grep은 -o 옵션을 사용하여 모든 일치 항목을 별도의 행으로 출력하는 훌륭한 기능을 가지고 있습니다.그렇다면 a로 시작하고 e로 끝나는 모든 단어의 데이터셋을 만들고 싶다면 어떻게 해야 할까요?\n당연히 이를 위한 파이프라인도 있습니다.➊ 여기서 텍스트를 소문자로 만들기 위해 tr을 사용합니다. 다음 절에서 tr을 더 자세히 살펴보겠습니다.두 개의 grep 명령어를 하나로 합칠 수도 있었겠지만, 이 경우에는 이전 파이프라인을 재사용하고 조정하는 것이 더 쉽다고 판단했습니다.\n일을 완수하기 위해 실용적인 태도를 취하는 것은 전혀 부끄러운 일이 아닙니다!","code":"grep -i chapter alice.txt | cut -d ' ' -f 3-sed -rn 's/^CHAPTER ([IVXLCDM]{1,})\\. (.*)$/\\2/p' alice.txt | trim 3grep -i chapter alice.txt | cut -c 9-< alice.txt grep -oE '\\w{2,}' | trim< alice.txt tr '[:upper:]' '[:lower:]' |\ngrep -oE '\\w{2,}' |\ngrep -E '^a.*e$' |\nsort | uniq | sort -nr | trim"},{"path":"chapter-5-scrubbing-data.html","id":"값-교체-및-삭제하기","chapter":"제 5 데이터 정제하기","heading":"5.3.3 값 교체 및 삭제하기","text":"반전(translate)의 약자인 커맨드 라인 도구 tr75을 사용하여 개별 문자를 교체하거나 삭제할 수 있습니다.\n예를 들어, 공백을 다음과 같이 언더스코어로 바꿀 수 있습니다.둘 이상의 문자를 교체해야 한다면 다음과 같이 조합할 수 있습니다.tr에 -d 인자를 지정하여 개별 문자를 삭제할 수도 있습니다.이 경우 위 두 명령어는 동일한 결과를 냅니다.\n하지만 두 번째 명령어는 두 가지 추가 기능을 사용합니다.\n대괄호와 하이픈([-])을 사용하여 문자 범위(모든 소문자)를 지정했고, -c 옵션은 그에 대한 보집합(complement)을 사용함을 나타냅니다.\n즉, 이 명령어는 소문자만 남깁니다.\ntr을 사용하여 텍스트를 대문자로 변환할 수도 있습니다.하지만 ASCII가 아닌 문자를 번역해야 한다면 tr이 작동하지 않을 수 있습니다. tr은 단일 바이트 문자에서만 작동하기 때문입니다. 그런 경우에는 대신 sed를 사용해야 합니다.개별 문자 이상의 것을 조작해야 한다면 sed가 유용하다는 사실을 알게 될 것입니다.\n이미 alice.txt에서 장 제목을 추출하는 sed 예제를 보셨습니다.\n추출, 삭제, 교체는 사실 sed에서 모두 동일한 작업입니다.\n단지 서로 다른 정규 표현식을 지정할 뿐입니다.\n예를 들어, 단어를 바꾸거나 반복된 공백을 제거하고 앞부분의 공백을 제거하려면 다음과 같이 합니다.➊ hello를 bye로 바꿉니다.\n➋ 임의의 공백을 하나의 공백으로 바꿉니다. 플래그 g는 글로벌(global)의 약자로, 동일한 행에서 동일한 치환을 여러 번 적용할 수 있음을 의미합니다.\n➌ 여기서는 g 플래그를 지정하지 않았으므로 맨 앞의 공백만 제거합니다.이전의 grep 예제와 마찬가지로, 이 세 개의 sed 명령어는 하나로 합쳐질 수 있습니다.글쎄요, 여러분은 어느 쪽이 읽기 더 편하신가요?","code":"echo 'hello world!' | tr ' ' '_'echo 'hello world!' | tr ' !' '_?'echo 'hello world!' | tr -d ' !'\necho 'hello world!' | tr -d -c '[a-z]'echo 'hello world!' | tr '[a-z]' '[A-Z]'\necho 'hello world!' | tr '[:lower:]' '[:upper:]'echo 'hello world!' | tr '[a-z]' '[A-Z]'\necho 'hallo wêreld!' | tr '[a-z]' '[A-Z]'\necho 'hallo wêreld!' | tr '[:lower:]' '[:upper:]'\necho 'hallo wêreld!' | sed 's/[[:lower:]]*/\\U&/g'\necho 'helló világ' | tr '[:lower:]' '[:upper:]'\necho 'helló világ' | sed 's/[[:lower:]]*/\\U&/g'echo ' hello     world!' |\nsed -re 's/hello/bye/' |\nsed -re 's/\\s+/ /g' |\nsed -re 's/\\s+//'echo ' hello     world!' |\nsed -re 's/hello/bye/;s/\\s+/ /g;s/\\s+//'"},{"path":"chapter-5-scrubbing-data.html","id":"csv","chapter":"제 5 데이터 정제하기","heading":"5.4 CSV","text":"","code":""},{"path":"chapter-5-scrubbing-data.html","id":"본문bodies과-헤더headers와-열columns","chapter":"제 5 데이터 정제하기","heading":"5.4.1 본문(Bodies)과 헤더(Headers)와 열(Columns)","text":"일반 텍스트를 정제하기 위해 사용했던 tr이나 grep 같은 커맨드 라인 도구가 CSV에 항상 적용될 수 있는 것은 아닙니다.\n그 이유는 이러한 커맨드 라인 도구들이 헤더, 본문, 열에 대한 개념이 없기 때문입니다.\n만약 grep을 사용하여 행을 필터링하면서 항상 출력에 헤더를 포함하고 싶다면 어떻게 해야 할까요?\n혹은 tr을 사용하여 다른 열은 건드리지 않고 특정 열의 값만 대문자로 바꾸고 싶다면 어떻게 해야 할까요?이를 위한 여러 단계의 우회 방법이 있긴 하지만 매우 번거롭습니다.\n더 좋은 방법이 있습니다.\n일반적인 커맨드 라인 도구를 CSV에 활용하기 위해 body76, header77, cols78라는 이름의 세 가지 도구를 소개하겠습니다.첫 번째 도구인 body부터 시작해 보겠습니다.\nbody를 사용하면 CSV 파일의 헤더를 제외한 나머지 부분, 즉 본문에 임의의 커맨드 라인 도구를 적용할 수 있습니다.\n예를 들어 다음과 같습니다.이 도구는 CSV 파일의 헤더가 한 줄이라고 가정합니다.\n작동 방식은 다음과 같습니다.표준 입력에서 한 줄을 가져와 $header라는 변수에 저장합니다.헤더를 출력합니다.표준 입력에 남은 데이터에 대해 body에 전달된 모든 커맨드 라인 인자를 실행합니다.또 다른 예입니다.\n다음 CSV 파일의 행 개수를 세고 싶다고 가정해 봅시다.wc -l을 사용하면 모든 행의 개수를 셀 수 있습니다.본문의 행만 고려하고 싶다면(즉, 헤더를 제외한 모든 것), body를 추가합니다.헤더는 계산에 포함되지 않으며 출력에 다시 인쇄됩니다.두 번째 도구인 header를 사용하면 CSV 파일의 헤더를 조작할 수 있습니다.\n인자를 제공하지 않으면 CSV 파일의 헤더가 인쇄됩니다.이는 head -n 1과 같습니다.\n헤더가 두 줄 이상인 경우(권장되지는 않음) -n 2를 지정할 수 있습니다.\nCSV 파일에 헤더를 추가할 수도 있습니다.이는 echo \"count\" | cat - <(seq 5)와 동일합니다.\n헤더 삭제는 -d 옵션으로 수행합니다.이는 tail -n +2와 비슷하지만 기억하기 조금 더 쉽습니다.\n헤더 교체는 -r 옵션으로 수행합니다. 이는 내부적으로 헤더를 먼저 삭제한 다음 추가하는 방식입니다. 여기서는 body와 결합해 보겠습니다.마지막으로, body 도구가 본문에 하는 것처럼 헤더에만 명령을 적용할 수 있습니다.\n예를 들어 다음과 같습니다.세 번째 도구는 cols로, 특정 열의 부분 집합에만 명령을 적용할 수 있게 해줍니다.\n예를 들어, tips 데이터셋에서 다른 열과 헤더는 건드리지 않고 day 열의 값만 대문자로 바꾸고 싶다면 다음과 같이 cols와 body를 조합하여 사용합니다.header -e, body, cols에 여러 커맨드 라인 도구와 인자를 명령어로 전달할 때 따옴표 처리가 까다로울 수 있다는 점에 유의하세요.\n이런 문제가 발생하면 별도의 커맨드 라인 도구를 만들어서 그 명령어를 전달하는 것이 가장 좋습니다.결론적으로, CSV 데이터를 위해 특별히 제작된 커맨드 라인 도구를 사용하는 것이 일반적으로 바람직하지만, body, header, cols를 사용하면 필요한 경우 고전적인 커맨드 라인 도구들을 CSV 파일에도 적용할 수 있습니다.","code":"echo -e \"value\\n7\\n2\\n5\\n3\" | body sort -nseq 5 | header -a countseq 5 | header -a count | wc -lseq 5 | header -a count | body wc -l< tips.csv headerseq 5 | header -a count< iris.csv header -d | trimseq 5 | header -a line | body wc -l | header -r countseq 5 | header -a line | header -e \"tr '[a-z]' '[A-Z]'\"< tips.csv cols -c day body \"tr '[a-z]' '[A-Z]'\" | head -n 5 | csvlook"},{"path":"chapter-5-scrubbing-data.html","id":"csv에서-sql-쿼리-수행하기","chapter":"제 5 데이터 정제하기","heading":"5.4.2 CSV에서 SQL 쿼리 수행하기","text":"만약 이번 장에서 언급된 커맨드 라인 도구들이 충분한 유연성을 제공하지 못한다면, 커맨드 라인에서 데이터를 정제하는 또 다른 접근 방식이 있습니다.\ncsvsql79 도구는 CSV 파일에서 직접 SQL 쿼리를 실행할 수 있게 해줍니다.\nSQL은 데이터 정제 작업을 정의하는 강력한 언어이며, 개별 커맨드 라인 도구를 사용하는 것과는 매우 다른 방식입니다.아래의 정제 작업들에는 csvsql을 포함한 몇 가지 솔루션을 소개하겠습니다. 기본적인 명령어는 다음과 같습니다.표준 입력을 csvsql로 전달하면 테이블 이름은 stdin이 됩니다.\n열의 유형은 데이터로부터 자동으로 유추됩니다.\n나중에 CSV 파일 결합 섹션에서 보겠지만, 여러 개의 CSV 파일을 지정할 수도 있습니다.\n\ncsvsql은 SQL 표준과 약간의 차이가 있는 SQLite 방식을 사용한다는 점을 유념하세요.\nSQL은 일반적으로 다른 솔루션보다 장황하긴 하지만 훨씬 더 유연합니다.\n만약 이미 SQL로 정제 문제를 해결하는 방법을 알고 있다면 커맨드 라인에서도 이를 사용하지 않을 이유가 없습니다.","code":"seq 5 | header -a val | csvsql --query \"SELECT SUM(val) AS sum FROM stdin\""},{"path":"chapter-5-scrubbing-data.html","id":"열-추출-및-재정렬하기","chapter":"제 5 데이터 정제하기","heading":"5.4.3 열 추출 및 재정렬하기","text":"csvcut80 도구를 사용하여 열을 추출하고 재정렬할 수 있습니다.\n예를 들어, Iris 데이터셋에서 숫자 값이 포함된 열만 남기고 가운데 두 열의 순서를 바꾸려면 다음과 같이 합니다.또는 -C(complement) 옵션을 사용하여 제외할 열을 지정할 수도 있습니다.이 경우 포함된 열들은 원래 순서를 유지합니다.\n열 이름 대신 1부터 시작하는 열 인덱스를 지정할 수도 있습니다.\n예를 들어, 홀수 번째 열만 선택하는 것도 가능합니다.만약 어떤 값에도 쉼표가 포함되어 있지 않다는 확신이 있다면 cut을 사용하여 열을 추출할 수도 있습니다.\n하지만 cut은 다음 명령어에서 볼 수 있듯이 열의 순서를 바꾸지 못한다는 점에 주의하세요.보시다시피 -f 옵션으로 열을 어떤 순서로 지정하든 상관없습니다. cut을 사용하면 항상 원래 순서대로 나타납니다.\n완전성을 위해, Iris 데이터셋의 숫자 열을 추출하고 재정렬하는 SQL 방식도 살펴보겠습니다.","code":"< iris.csv csvcut -c sepal_length,petal_length,sepal_width,petal_width | csvlook< iris.csv csvcut -C species | csvlookecho 'a,b,c,d,e,f,g,h,i\\n1,2,3,4,5,6,7,8,9' |\ncsvcut -c $(seq 1 2 9 | paste -sd,)echo 'a,b,c,d,e,f,g,h,i\\n1,2,3,4,5,6,7,8,9' | cut -d, -f 5,1,3< iris.csv csvsql --query \"SELECT sepal_length, petal_length, \"\\\n\"sepal_width, petal_width FROM stdin\" | head -n 5 | csvlook"},{"path":"chapter-5-scrubbing-data.html","id":"행-필터링하기-1","chapter":"제 5 데이터 정제하기","heading":"5.4.4 행 필터링하기","text":"CSV 파일에서 행(rows)을 필터링하는 것이 일반 텍스트 파일에서 행(lines)을 필터링하는 것과 다른 점은, 특정 열의 값에만 기반하여 필터링하고 싶을 수 있다는 것입니다.\n위치 기반 필터링은 기본적으로 동일하지만, CSV 파일의 첫 번째 행이 대개 헤더라는 점을 고려해야 합니다.\n헤더를 유지하고 싶다면 항상 body 도구를 사용할 수 있다는 점을 기억하세요.특정 열 내의 특정 패턴에 따라 필터링하려는 경우 csvgrep81, awk, 또는 csvsql을 사용할 수 있습니다.\n예를 들어, 일행의 수(party size)가 5명 미만인 모든 영수증을 제외하려면 다음과 같이 합니다.awk와 csvsql은 숫자 비교도 수행할 수 있습니다.\n예를 들어, 토요일이나 일요일에 40달러가 넘는 모든 영수증을 가져오려면 다음과 같이 합니다.csvsql 솔루션은 더 장황하지만 인덱스 대신 열 이름을 사용하므로 더 견고합니다.SQL의 절은 날짜나 집합에 대해 작동할 수 있고 복잡한 절의 조합을 형성할 수 있기 때문에, 그 유연성은 다른 커맨드 라인 도구들이 쉽게 따라가기 어렵습니다.","code":"seq 5 | sed -n '3,5p'\nseq 5 | header -a count | body sed -n '3,5p'csvgrep -c size -i -r \"[1-4]\" tips.csv< tips.csv awk -F, 'NR==1 || ($1 > 40.0) && ($5 ~ /^S/)'csvsql --query \"SELECT * FROM tips WHERE bill > 40 AND day LIKE 'S%'\" tips.csv"},{"path":"chapter-5-scrubbing-data.html","id":"열-병합하기","chapter":"제 5 데이터 정제하기","heading":"5.4.5 열 병합하기","text":"열 병합은 관심 있는 값이 여러 열에 분산되어 있을 때 유용합니다.\n날짜(연, 월, 일이 별도의 열인 경우)나 이름(이름과 성이 별도의 열인 경우)에서 이런 일이 발생할 수 있습니다.\n두 번째 상황을 살펴보겠습니다.입력 CSV는 작곡가 목록입니다.\n여러분의 과제는 이름(first name)과 성(last name)을 합쳐서 전체 이름(full name)을 만드는 것이라고 가정해 봅시다.\n이 과제를 해결하기 위해 sed, awk, cols + tr, csvsql의 네 가지 접근 방식을 제시하겠습니다.\n먼저 입력 CSV를 살펴보겠습니다.첫 번째 방식인 sed는 두 개의 문(statement)을 사용합니다.\n첫 번째는 헤더를 교체하는 것이고, 두 번째는 두 번째 행부터 적용되는 후방 참조를 포함한 정규 표현식입니다.awk 접근 방식은 다음과 같습니다.tr과 결합한 cols 방식입니다.csvsql은 쿼리를 실행하기 위해 SQLite 데이터베이스를 사용하며, ||는 문자열 연결(concatenation)을 의미한다는 점에 유의하세요.만약 last_name에 쉼표가 포함되어 있다면 어떻게 될까요? 명확성을 위해 가공되지 않은 입력 CSV를 살펴보겠습니다.글쎄요, 처음 세 가지 방식은 모두 각기 다른 이유로 실패하는 것 같군요. 오직 csvsql만이 first_name과 full_name을 제대로 결합할 수 있습니다.잠깐만요! 저 마지막 명령어는 무엇인가요? R인가요? 사실 그렇습니다.\nrush라는 커맨드 라인 도구를 통해 실행된 R 코드입니다. 지금 말씀드릴 수 있는 것은 이 방식 또한 두 개의 열을 병합하는 데 성공했다는 점입니다.\n이 멋진 도구에 대해서는 나중에 다시 다루겠습니다.","code":"csvlook -I names.csv< names.csv sed -re '1s/.*/id,full_name,born/g;2,$s/(.*),(.*),(.*),(.*)/\\1,\\3 \\2,\\4/g' |\ncsvlook -I< names.csv awk -F, 'BEGIN{OFS=\",\"; print \"id,full_name,born\"} {if(NR > 1) {print $1,$3\" \"$2,$4}}' |\ncsvlook -I< names.csv |\ncols -c first_name,last_name tr \\\",\\\" \\\" \\\" |\nheader -r full_name,id,born |\ncsvcut -c id,full_name,born |\ncsvlook -I< names.csv csvsql --query \"SELECT id, first_name || ' ' || last_name \"\\\n\"AS full_name, born FROM stdin\" | csvlook -Icat names-comma.csv< names-comma.csv sed -re '1s/.*/id,full_name,born/g;2,$s/(.*),(.*),(.*),(.*)/\\1,\\3 \\2,\\4/g' | tail -n 1< names-comma.csv awk -F, 'BEGIN{OFS=\",\"; print \"id,full_name,born\"} {if(NR > 1) {print $1,$3\" \"$2,$4}}' | tail -n 1< names-comma.csv | cols -c first_name,last_name tr \\\",\\\" \\\" \\\" |\nheader -r full_name,id,born | csvcut -c id,full_name,born | tail -n 1< names-comma.csv csvsql --query \"SELECT id, first_name || ' ' || last_name AS full_name, born FROM stdin\" | tail -n 1< names-comma.csv rush run -t 'unite(df, full_name, first_name, last_name, sep = \" \")' - | tail -n 1"},{"path":"chapter-5-scrubbing-data.html","id":"가로로-결합하기","chapter":"제 5 데이터 정제하기","heading":"5.4.5.1 가로로 결합하기","text":"나란히 붙이고 싶은 세 개의 CSV 파일이 있다고 가정해 봅시다. 파이프라인 중간에 csvcut의 결과를 저장하기 위해 tee82를 사용합니다.행들이 서로 일치한다고 가정하면, paste83를 사용하여 파일들을 하나로 합칠 수 있습니다.여기서 커맨드 라인 인자 -d는 paste가 구분자로 쉼표를 사용하도록 지시합니다.","code":"< tips.csv csvcut -c bill,tip | tee bills.csv | head -n 3 | csvlook\n< tips.csv csvcut -c day,time | tee datetime.csv |\nhead -n 3 | csvlook -I\n< tips.csv csvcut -c sex,smoker,size | tee customers.csv |\nhead -n 3 | csvlookpaste -d, {bills,customers,datetime}.csv | head -n 3 | csvlook -I"},{"path":"chapter-5-scrubbing-data.html","id":"결합joining","chapter":"제 5 데이터 정제하기","heading":"5.4.5.2 결합(Joining)","text":"가끔은 데이터를 세로로나 가로로 단순히 이어 붙이는 것만으로는 합칠 수 없습니다.\n특히 관계형 데이터베이스의 경우, 중복을 최소화하기 위해 데이터가 여러 테이블(또는 파일)에 분산되어 있는 경우가 있습니다.\nIris 데이터셋에 세 가지 붓꽃 종류에 대한 더 많은 정보(예: USDA 식별자)를 추가하여 확장하고 싶다고 가정해 봅시다.\n마침 이러한 식별자가 포함된 별도의 CSV 파일이 있습니다.이 데이터셋과 Iris 데이터셋의 공통점은 species 열입니다.\ncsvjoin84을 사용하여 두 데이터셋을 결합할 수 있습니다.물론 csvsql을 사용하여 SQL 방식으로 접근할 수도 있습니다. 평소와 같이 조금 더 길지만, 잠재적으로 훨씬 더 유연합니다.","code":"csvlook irismeta.csvcsvjoin -c species iris.csv irismeta.csv | csvcut -c sepal_length,sepal_width,species,usda_id | sed -n '1p;49,54p' | csvlookcsvsql --query 'SELECT i.sepal_length, i.sepal_width, i.species, m.usda_id FROM iris i JOIN irismeta m ON (i.species = m.species)' iris.csv irismeta.csv | sed -n '1p;49,54p' | csvlook"},{"path":"chapter-5-scrubbing-data.html","id":"xmlhtml-및-json-작업하기","chapter":"제 5 데이터 정제하기","heading":"5.5 XML/HTML 및 JSON 작업하기","text":"이 섹션에서는 데이터를 한 형식에서 다른 형식으로 변환할 수 있는 몇 가지 커맨드 라인 도구를 시연하겠습니다.\n데이터를 변환하는 데에는 두 가지 이유가 있습니다.첫째, 많은 시각화 및 머신러닝 알고리즘이 표 형식의 데이터를 요구하기 때문에, 데이터를 데이터베이스 테이블이나 스프레드시트와 같은 표 형태로 만들어야 하는 경우가 많습니다.\nCSV는 본래 표 형식이지만, JSON과 HTML/XML 데이터는 깊게 중첩된 구조를 가질 수 있습니다.둘째, cut이나 grep 같은 고전적인 커맨드 라인 도구들은 대부분 일반 텍스트를 대상으로 작동합니다.\n텍스트는 커맨드 라인 도구 간의 보편적인 인터페이스로 간주되기 때문입니다.\n게다가 다른 형식들은 비교적 나중에 등장했습니다. 이러한 각 형식을 일반 텍스트로 취급하면 고전적인 커맨드 라인 도구들을 다른 형식에도 적용할 수 있습니다.가끔은 구조화된 데이터에 고전적인 도구들을 적용하여 문제를 해결할 수 있습니다.\n예를 들어, 아래의 JSON 데이터를 일반 텍스트로 취급하여 sed를 사용해 gender 속성을 sex로 바꿀 수 있습니다.다른 많은 커맨드 라인 도구와 마찬가지로 sed는 데이터의 구조를 활용하지 않습니다.\n구조를 활용하는 도구(아래에서 설명할 jq 등)를 사용하거나, 데이터를 먼저 CSV와 같은 표 형식으로 변환한 다음 적절한 커맨드 라인 도구를 적용하는 것이 더 좋습니다.실제 사용 사례를 통해 XML/HTML 및 JSON을 CSV로 변환하는 과정을 보여드리겠습니다.\n여기서 사용할 커맨드 라인 도구는 curl, pup85, xml2json86, jq, json2csv87입니다.위키피디아에는 방대한 정보가 담겨 있습니다. 이 정보의 상당 부분은 표(table)로 정리되어 있으며, 이는 데이터셋으로 간주될 수 있습니다.\n예를 들어, 이 페이지에는 국가 및 영토 목록과 함께 국경 길이, 면적, 그리고 그 둘 사이의 비율이 포함되어 있습니다.이 데이터를 분석하는 데 관심이 있다고 가정해 봅시다. 이 섹션에서는 필요한 모든 단계와 그에 해당하는 명령어를 안내해 드리겠습니다. 세세한 부분까지 모두 설명하지는 않으므로 모든 것을 즉시 이해하지 못할 수도 있습니다. 걱정하지 마세요. 핵심 내용은 파악하실 수 있을 것입니다. 이 섹션의 목적은 커맨드 라인의 활용 능력을 보여주는 것임을 기억하세요. 이 섹션에서 사용된 모든 도구와 개념(그 이상)은 이후 장에서 설명될 것입니다.관심 있는 데이터셋은 HTML에 포함되어 있습니다.\n여러분의 목표는 작업하기 편한 형태의 데이터셋 표현을 얻는 것입니다.\n가장 먼저 할 일은 curl을 사용하여 HTML을 다운로드하는 것입니다.HTML은 wiki.html이라는 파일로 저장됩니다.\n처음 10행이 어떻게 생겼는지 확인해 봅시다.제대로 된 것 같습니다.\n관심 있는 루트 HTML 요소가 wikitable 클래스를 가진 <table>이라는 것을 알아냈다고 가정해 봅시다.\n그러면 grep을 사용하여 관심 있는 부분을 살펴볼 수 있습니다(-옵션은 일치하는 행 이후에 인쇄할 행의 수를 지정합니다).이제 실제 국가들과 그 값들이 보입니다.\n다음 단계는 HTML 파일에서 필요한 요소를 추출하는 것입니다.\n이를 위해 pup을 사용할 수 있습니다.pup에 전달된 표현식은 CSS 선택자(selector)입니다.\n이 구문은 보통 웹 페이지의 스타일을 지정하는 데 사용되지만, HTML에서 특정 요소를 선택하는 데에도 사용할 수 있습니다.\n이 경우 wikitable 클래스를 가진 table의 tbody를 선택하려고 합니다.\n그다음은 XML(및 HTML)을 JSON으로 변환하는 xml2json입니다.HTML을 JSON으로 변환하는 이유는 JSON 데이터를 처리하는 매우 강력한 도구인 jq가 있기 때문입니다.\n다음 명령어는 JSON 데이터의 특정 부분을 추출하고 우리가 작업할 수 있는 형태로 재구성합니다.이제 데이터가 작업하기 편한 형태가 되었습니다.\n위키피디아 페이지에서 CSV 데이터셋을 얻기까지 꽤 많은 단계를 거쳤습니다.\n하지만 위의 모든 명령어를 하나로 합치면 실제로는 매우 간결하고 표현력이 풍부하다는 것을 알 수 있습니다.이로써 XML/HTML에서 JSON을 거쳐 CSV로 변환하는 시연을 마칩니다.\njq는 훨씬 더 많은 작업을 수행할 수 있고 XML 데이터를 다루는 전문 도구들도 존재하지만, 제 경험상 가능한 한 빨리 데이터를 CSV 형식으로 변환하는 것이 잘 작동하는 경향이 있습니다.\n이렇게 하면 특정 도구보다는 범용적인 커맨드 라인 도구에 능숙해지는 데 더 많은 시간을 할애할 수 있습니다.","code":"sed -e 's/\"gender\":/\"sex\":/g' users.json | jq | trimcurl -sL 'http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_border/area_ratio' > wiki.html< wiki.html trimgrep wikitable -A 21 wiki.html< wiki.html pup 'table.wikitable tbody' | tee table.html | trim< table.html xml2json > table.json\njq . table.json | trim 20< table.json jq -r '.tbody.tr[1:][] | [.td[][\"$t\"]] | @csv' | header -a rank,country,border,surface,ratio > countries.csvcsvlook --max-column-width 28 countries.csv"},{"path":"chapter-5-scrubbing-data.html","id":"요약-4","chapter":"제 5 데이터 정제하기","heading":"5.6 요약","text":"이 장에서는 데이터를 청소하거나 정제하는 과정을 살펴보았습니다.\n보셨듯이 데이터의 모든 지저분함을 마법처럼 없애주는 단일 도구는 없습니다. 원하는 결과를 얻기 위해 여러 가지 서로 다른 도구들을 결합해야 하는 경우가 많습니다.\ncut이나 sort 같은 고전적인 커맨드 라인 도구들은 구조화된 데이터를 해석할 수 없다는 점을 기억하세요.\n다행히 JSON이나 XML 같은 데이터 형식을 CSV 같은 다른 데이터 형식으로 변환해 주는 도구들이 있습니다.\n다음 장은 다시 한 번 막간을 이용한 장으로, make를 사용하여 프로젝트를 관리하는 방법을 보여드리겠습니다.\n7장에서 데이터 탐색과 시각화를 시작하는 것이 너무 기다려진다면 이 장은 건너뛰어도 좋습니다.","code":""},{"path":"chapter-5-scrubbing-data.html","id":"더-읽어보기","chapter":"제 5 데이터 정제하기","heading":"5.7 더 읽어보기","text":"awk에 대해 더 많이 설명할 수 있었다면 좋았을 것입니다. awk는 매우 강력한 도구이자 프로그래밍 언어입니다. 시간을 내어 배워보시길 강력히 추천합니다. 좋은 자료로는 Doherty와 Robbins가 쓴 sed & awk 책과 온라인 GNU Awk User’s Guide가 있습니다.이 장의 몇 군데에서 정규 표현식을 사용했습니다. 아쉽게도 정규 표현식에 대한 튜토리얼은 이 책의 범위를 벗어납니다. 정규 표현식은 많은 서로 다른 도구에서 사용될 수 있으므로 배워두시는 것을 추천합니다. 좋은 책으로는 Jan Goyvaerts와 Steven Levithan이 쓴 Regular Expressions Cookbook이 있습니다.","code":""},{"path":"chapter-6-project-management-with-make.html","id":"chapter-6-project-management-with-make","chapter":"제 6 Make으로 프로젝트 관리하기","heading":"제 6 Make으로 프로젝트 관리하기","text":"이제 여러분도 커맨드 라인이 데이터를 다루기에 매우 편리한 환경이라는 것을 체감하셨으리라 생각합니다.\n커맨드 라인에서 작업하면서 우리는 다음과 같은 상황을 겪게 된다는 것을 눈치채셨을 것입니다.수많은 서로 다른 명령어를 호출합니다.다양한 디렉터리에서 작업합니다.우리만의 커맨드 라인 도구를 개발합니다.많은 (중간) 파일들을 획득하고 생성합니다.이 과정은 탐색적인 성격이 강하기 때문에, 우리의 워크플로우는 다소 혼란스러워지는 경향이 있으며, 그로 인해 우리가 무엇을 했는지 추적하기가 어려워집니다.\n우리가 거쳐온 단계들은 본인이나 타인에 의해 재현될 수 있어야 합니다.\n얼마간의 시간이 흐른 뒤 프로젝트를 다시 시작할 때, 어떤 명령어를 어떤 디렉터리에서, 어떤 파일을 대상으로, 어떤 매개변수와 함께, 그리고 어떤 순서로 실행했는지 잊어버릴 가능성이 큽니다.\n이런 상황에서 협업자와 프로젝트를 공유하는 것은 얼마나 힘들지 상상해 보십시오.history 명령어의 출력을 뒤져서 일부 명령어를 복구할 수도 있겠지만, 이는 당연히 신뢰할 수 있는 방법이 아닙니다.\n그보다 나은 방법은 명령어를 쉘 스크립트에 저장하는 것입니다.\n최소한 이 방법을 통해 여러분과 협업자들은 프로젝트를 재현할 수 있습니다.\n하지만 쉘 스크립트 역시 다음과 같은 이유로 최선의 방법은 아닙니다.읽고 유지보수하기가 어렵습니다.단계들 사이의 의존성이 불분명합니다.매번 모든 단계가 실행되는데, 이는 비효율적이며 때로는 바람직하지 않습니다.이것이 바로 make가 정말 빛을 발하는 지점입니다88. make는 다음과 같은 작업을 가능하게 해주는 커맨드 라인 도구입니다.입력과 출력의 의존성 관점에서 데이터 워크플로우 단계를 공식화합니다.워크플로우의 특정 단계만 실행합니다.인라인 코드를 사용합니다.외부 소스로부터 데이터를 저장하고 가져옵니다.중요한 관련 주제로 버전 관리(version control)가 있습니다. 이는 프로젝트의 변경 사항을 추적하고, 프로젝트를 서버에 백업하며, 다른 사람들과 협업하고, 문제가 생겼을 때 이전 버전으로 되돌릴 수 있게 해줍니다.\n버전 관리를 위한 대중적인 커맨드 라인 도구는 git90입니다.\nGit은 흔히 분산 버전 관리를 위한 온라인 서비스인 GitHub와 함께 사용됩니다.\n이 책을 포함한 많은 오픈 소스 프로젝트가 GitHub에서 호스팅되고 있습니다.\n버전 관리라는 주제는 이 책의 범위를 벗어나지만, 특히 다른 사람들과 협업을 시작하게 된다면 이를 꼭 살펴보시길 강력히 추천합니다.\n이 장의 끝부분에서 더 배울 수 있는 몇 가지 자료를 추천해 드리겠습니다.","code":""},{"path":"chapter-6-project-management-with-make.html","id":"개요-3","chapter":"제 6 Make으로 프로젝트 관리하기","heading":"6.1 개요","text":"make로 데이터 워크플로우를 관리하는 것이 이 장의 주요 주제입니다.\n이에 대해 다음 내용을 배우게 됩니다.Makefile을 사용하여 워크플로우를 정의하기.입력 및 출력 의존성의 관점에서 워크플로우 생각하기.작업(task)을 실행하고 타겟(target)을 빌드하기.이 파일들을 가져오는 방법은 2장에 설명되어 있습니다.\n그 외의 파일들은 커맨드 라인 도구를 사용하여 다운로드하거나 생성한 것들입니다.","code":"cd /data/ch06\nl"},{"path":"chapter-6-project-management-with-make.html","id":"make-소개","chapter":"제 6 Make으로 프로젝트 관리하기","heading":"6.2 Make 소개","text":"make는 데이터와 그 의존성을 중심으로 명령어 실행을 조직화합니다.\n데이터 처리 단계는 별도의 텍스트 파일(워크플로우)에 공식화됩니다.\n각 단계는 입력과 출력을 가질 수 있습니다.\nmake는 그들의 의존성을 자동으로 해결하고 어떤 명령어를 어떤 순서로 실행해야 하는지 결정합니다.예를 들어, 10분 정도 걸리는 SQL 쿼리가 있다고 할 때, 이는 결과 파일이 없거나 쿼리가 나중에 변경되었을 때만 실행됩니다.\n또한 특정 단계를 (재)실행하고 싶을 때, make는 해당 단계가 의존하는 단계들만 재실행합니다.\n이는 시간을 대폭 절약해 줍니다.공식화된 워크플로우를 갖추면 몇 주 후에 프로젝트를 다시 시작하거나 다른 사람들과 협업하기가 훨씬 수월해집니다.\n일회성 프로젝트라고 생각하더라도 이를 수행하시기를 강력히 권장합니다.\n언제 특정 단계를 다시 실행해야 하거나 다른 프로젝트에서 재사용하게 될지 누구도 모르기 때문입니다.","code":""},{"path":"chapter-6-project-management-with-make.html","id":"작업task-실행하기","chapter":"제 6 Make으로 프로젝트 관리하기","heading":"6.3 작업(Task) 실행하기","text":"기본적으로 make는 현재 디렉터리에서 Makefile이라는 설정 파일을 찾습니다.\n파일 이름을 makefile(소문자)로 할 수도 있지만, 더 일반적이고 디렉터리 목록의 상단에 나타나도록 Makefile로 지정하는 것을 추천합니다.\n보통 프로젝트당 하나의 설정 파일만 가집니다.\n이 장에서는 여러 장을 논의하기 때문에 각각 .make 확장자를 가진 다른 파일 이름을 부여했습니다.\n다음 Makefile부터 시작해 보겠습니다.이 Makefile은 numbers라는 하나의 타겟(target)을 포함하고 있습니다.\n타겟은 하나의 작업과 같습니다.\n보통 생성하고자 하는 파일의 이름이지만, 그보다 더 일반적인 것일 수도 있습니다.\n그 아래 줄인 seq 7은 규칙(rule)으로 알려져 있습니다.\n규칙은 요리 레시피(recipe)라고 생각하면 됩니다. 타겟을 어떻게 빌드할지 명시하는 하나 이상의 명령어입니다.규칙 앞의 공백은 하나의 탭(tab) 문자입니다.\nmake는 공백에 매우 민감합니다.\n일부 에디터는 TAB 키를 눌렀을 때 소프트 탭(soft tab)이라고 불리는 공백들을 삽입하는데, 이는 make 에러를 유발할 수 있으니 주의하십시오.\n다음 코드는 탭을 8개의 공백으로 확장하여 이를 보여줍니다.➊ 설정 파일의 이름이 기본값인 Makefile이 아니기 때문에 --makefile 옵션의 약어인 -f 옵션을 추가해야 합니다.\n➋ 커맨드 라인에서 볼 수 있는 매우 친절한 에러 메시지 중 하나입니다!이제부터는 실제 사용 사례와 더 비슷하도록 해당 파일의 이름을 Makefile로 바꿀 것입니다.\n그냥 make를 실행해 보겠습니다.그러면 make가 먼저 규칙 자체(seq 7)를 출력한 다음, 규칙에 의해 생성된 출력을 인쇄하는 것을 볼 수 있습니다.\n이 과정을 타겟을 빌드(building)한다고 합니다.\n타겟의 이름을 지정하지 않으면 make는 Makefile에 정의된 첫 번째 타겟을 빌드합니다.\n실제로는 빌드하고 싶은 타겟을 직접 지정하는 경우가 더 많습니다.이 경우 실제로는 아무것도 빌드하지 않았습니다. 즉, 새로운 파일을 생성하지 않았습니다.\nmake는 numbers라는 파일을 찾지 못하므로 기꺼이 numbers 타겟을 다시 빌드할 것입니다.\n다음 섹션에서 이에 대해 자세히 다루겠습니다.가끔은 동일한 이름의 파일 존재 여부와 상관없이 무조건 빌드되는 타겟이 필요할 때가 있습니다.\n프로젝트의 일부로 수행해야 하는 작업들을 생각해 보십시오.\n그러한 타겟들은 Makefile의 최상단에 .PHONY라는 특수 타겟을 사용하고 그 뒤에 가짜(phony) 타겟들의 이름을 나열하여 가짜 타겟으로 선언하는 것이 좋은 관례입니다.\n다음은 가짜 타겟의 사용법을 보여주는 예시 Makefile입니다.➊ $(pwd) 앞의 달러 기호가 하나 더 있다는 점에 주목하십시오. 이는 make가 단일 달러 기호를 나중에 설명할 다양한 특수 변수를 참조하는 데 사용하기 때문에 필요합니다.위 내용은 제가 이 책을 작업하면서 사용하는 Makefile에서 가져온 것입니다.\n제가 make를 고성능 작업 실행기(task runner)로 사용하고 있다고 말할 수도 있습니다.\n이것이 make의 주된 목적은 아니었지만, 제가 어떤 명령어를 사용했는지 기억하거나 찾아볼 필요가 없기 때문에 여전히 큰 가치가 있습니다.\n대신 make publish를 입력하면 책의 최신 버전이 발행됩니다.\nMakefile에 시간이 오래 걸리는 명령어를 넣는 것도 전혀 문제가 되지 않습니다.그리고 make는 우리를 위해 훨씬 더 많은 일을 할 수 있습니다!","code":"bat -A numbers.make< numbers.make expand > spaces.make\nbat -A spaces.make\nmake -f spaces.make\nrm spaces.makecp numbers.make Makefile\nmakemake numbersbat tasks.make"},{"path":"chapter-6-project-management-with-make.html","id":"본격적으로-빌드하기","chapter":"제 6 Make으로 프로젝트 관리하기","heading":"6.4 본격적으로 빌드하기","text":"규칙의 출력이 파일 numbers에 기록되도록 Makefile을 수정해 보겠습니다.이제 make가 실제로 무언가를 빌드하고 있다고 말할 수 있습니다.\n또한, 이를 다시 실행하면 make는 numbers 타겟이 이미 최신 상태(--date)라고 보고합니다.파일 numbers가 이미 존재하기 때문에 numbers 타겟을 다시 빌드할 필요가 없습니다.\nmake가 불필요한 작업을 반복하지 않음으로써 우리의 시간을 절약해 주는 것입니다.make에서는 모든 것이 파일에 관한 것입니다.\n하지만 make는 타겟의 이름에만 신경 쓴다는 점을 명심하십시오.\n해당 이름의 파일이 실제로 규칙에 의해 생성되었는지 여부는 확인하지 않습니다.\n만약 우리가 “문자”라는 뜻의 네덜란드어인 nummers 파일에 쓰고 타겟 이름이 여전히 numbers라면, make는 항상 이 타겟을 빌드할 것입니다. 반대로, 자동이든 수동이든 다른 프로세스에 의해 numbers 파일이 생성되었다면, make는 여전히 해당 타겟이 최신 상태라고 간주할 것입니다.타겟의 이름으로 확장되는 자동 변수 $@를 사용하여 중복을 피할 수 있습니다.make가 타겟을 다시 빌드하는 또 다른 이유는 의존성(dependencies) 때문입니다. 이에 대해 다음에서 논의해 보겠습니다.","code":"cp numbers-write.make Makefile\nbat Makefile\nmake numbers\nbat numbersmake numberscp numbers-write-var.make Makefile\n200: bat Makefile\n201: ```\n\n*numbers* 파일을 삭제하고 `make`를 다시 호출하여 제대로 작동하는지 확인해 보겠습니다.\nrm numbers\nmake numbers\nbat numbers"},{"path":"chapter-6-project-management-with-make.html","id":"의존성-추가하기","chapter":"제 6 Make으로 프로젝트 관리하기","heading":"6.5 의존성 추가하기","text":"지금까지는 개별적으로 존재하는 타겟들을 살펴보았습니다.\n전형적인 데이터 과학 워크플로우에서 많은 단계는 다른 단계들에 의존합니다.\nMakefile에서 의존성에 대해 제대로 이야기하기 위해, 스타워즈 캐릭터에 대한 데이터셋을 다루는 두 가지 작업을 예로 들어보겠습니다.데이터셋의 일부 내용을 살펴보겠습니다.첫 번째 작업은 키가 가장 큰 인간 10명을 계산하는 것입니다.➊ 패턴 Human을 포함하는 행만 유지합니다.\n➋ 처음 두 열을 추출합니다.\n➌ 두 번째 열을 기준으로 숫자 역순으로 행을 정렬합니다.\n➍ 기본적으로 head는 처음 10개 행을 출력합니다. -n 옵션으로 이를 변경할 수 있습니다.두 번째 작업은 종(species)별 키 분포를 보여주는 박스 플롯을 생성하는 것입니다(?? 참조).이 두 작업을 Makefile에 넣어보겠습니다.\n이를 하나씩 점진적으로 하는 대신, 완성된 Makefile이 어떤 모습인지 먼저 보여준 다음 모든 구문을 단계별로 설명하겠습니다.이 Makefile을 단계별로 살펴보겠습니다.\n처음 세 줄은 make 자체와 관련된 몇 가지 기본 설정을 변경하기 위한 것입니다.모든 규칙은 쉘에서 실행되며, 기본값은 sh입니다. SHELL 변수를 사용하여 이를 bash와 같은 다른 쉘로 변경할 수 있습니다. 이렇게 하면 Bash가 제공하는 루프와 같은 모든 기능을 사용할 수 있습니다.기본적으로 규칙의 각 줄은 쉘로 별도로 전송됩니다. .ONESHELL이라는 특수 타겟을 사용하면 이를 무시할 수 있으며, 이 덕분에 top10 타겟의 규칙이 제대로 작동합니다..SHELLFLAGS 줄은 Bash를 더 엄격하게 만드는데, 이는 모범 사례(best practice)로 간주됩니다. 예를 들어, 이 설정 덕분에 top10 타겟 규칙의 파이프라인은 오류가 발생하는 즉시 중단됩니다.우리는 URL이라는 사용자 정의 변수를 정의합니다.\n이 변수가 한 번만 사용되기는 하지만, 파일의 시작 부분 부분에 이러한 정보를 배치하면 나중에 설정을 쉽게 변경할 수 있어 도움이 됩니다.특수 타겟 .PHONY를 사용하면 어떤 타겟이 파일로 나타나지 않는지 나타낼 수 있습니다. 이 예제에서는 all과 top10 타겟이 이에 해당합니다. 이제 이 타겟들은 디렉터리에 동일한 이름의 파일이 있는지 여부와 상관없이 항상 실행됩니다.타겟은 , data, data/starwars.csv, top10, heights.png의 다섯 가지가 있습니다.\nFigure ??에서는 이러한 타겟들과 그들 사이의 의존성을 개괄적으로 보여줍니다.각 타겟을 차례대로 살펴보겠습니다.타겟 all은 두 개의 의존성을 갖지만 규칙은 없습니다. 이는 지정된 순서대로 하나 이상의 타겟을 실행하기 위한 지름길과 같습니다. 이 경우 top10과 heights.png입니다. 타겟 all은 Makefile에서 첫 번째 타겟으로 나타나므로, 우리가 make를 실행하면 이 타겟이 빌드됩니다.타겟 data는 data 디렉터리를 생성합니다. 앞에서 make는 파일에 관한 것이라고 말씀드렸지만, 디렉터리에 관한 것이기도 합니다. 이 타겟은 data 디렉터리가 아직 존재하지 않을 때만 실행됩니다.타겟 data/starwars.csv는 타겟 data에 의존합니다. 만약 data 디렉터리가 없다면 먼저 생성될 것입니다. 모든 의존성이 충족되면 규칙이 실행되며, 여기에는 파일을 다운로드하여 타겟과 동일한 이름의 파일로 저장하는 과정이 포함됩니다.타겟 top10은 가짜 타겟으로 표시되어 있으므로 지정되면 항상 빌드됩니다. 이 타겟은 data/starwars.csv 타겟에 의존합니다. 첫 번째 전제 조건인 data/starwars.csv로 확장되는 특수 변수 $<를 사용합니다.타겟 heights.png는 top10 타겟과 마찬가지로 data/starwars.csv에 의존하며, 이 장에서 살펴본 두 가지 자동 변수를 모두 사용합니다. 다른 자동 변수에 대해 더 알고 싶다면 온라인 문서를 참조하십시오.마지막으로 이 Makefile이 제대로 작동하는지 확인해 보겠습니다.놀라운 일은 없군요. 타겟을 지정하지 않았으므로 타겟이 빌드되고, 결과적으로 top10과 heights.png 타겟이 모두 빌드됩니다. 전자의 출력은 표준 출력으로 인쇄되고 후자는 heights.png 파일을 생성합니다. data 디렉터리는 한 번만 생성되며, CSV 파일 역시 한 번만 다운로드됩니다.데이터를 가지고 놀다 보면 다른 모든 것을 잊어버리는 것보다 더 즐거운 일은 없습니다.\n하지만 Makefile을 사용하여 작업 내용을 기록해 두는 것이 가치 있다는 제 말을 믿으셔야 합니다.\n그것이 여러분의 삶을 더 편하게 만들어줄 뿐만 아니라(말장난입니다), 여러분이 데이터 워크플로우를 단계별로 생각하기 시작하는 데 도움을 줄 것입니다.\n시간이 지나면서 여러분만의 커맨드 라인 도구 상자가 확장되는 것처럼, make 워크플로우도 마찬가지입니다.\n정의된 단계가 많을수록 특정 단계들을 재사용할 수 있는 경우가 매우 많기 때문에 작업을 꾸준히 해나가기가 더 쉬워집니다.\n여러분이 make에 익숙해지고, 그것이 여러분의 삶을 더 풍요롭게(make life easier) 만들기를 바랍니다.","code":"curl -sL 'https://raw.githubusercontent.com/tidyverse/dplyr/master/data-raw/starwars.csv' |\nxsv select name,height,mass,homeworld,species |\ncsvlookcurl -sL 'https://raw.githubusercontent.com/tidyverse/dplyr/master/data-raw/starwars.csv' |\ngrep Human |\ncut -d, -f 1,2 |\nsort -t, -k2 -nr |\nheadcurl -sL 'https://raw.githubusercontent.com/tidyverse/dplyr/master/data-raw/starwars.csv' |\nrush plot --x height --y species --geom boxplot > heights.png\ndisplay heights.pngcp starwars.make Makefile\nbat Makefilemake"},{"path":"chapter-6-project-management-with-make.html","id":"요약-5","chapter":"제 6 Make으로 프로젝트 관리하기","heading":"6.6 요약","text":"커맨드 라인의 묘미 중 하나는 데이터를 마음껏 가지고 놀 수 있다는 점입니다.\n다양한 명령어를 쉽게 실행하고 여러 데이터 파일을 처리할 수 있습니다.\n이는 매우 상호작용적이고 반복적인 과정입니다.\n시간이 지나면 원하는 결과를 얻기 위해 어떤 단계를 거쳤는지 잊어버리기 쉽습니다.\n따라서 이따금 여러분이 수행한 단계들을 기록해 두는 것이 매우 중요합니다.\n그래야 나중에 여러분이나 동료가 다시 프로젝트를 시작했을 때, 동일한 단계를 실행하여 같은 결과를 만들어낼 수 있습니다.이 장에서는 모든 명령어를 하나의 Bash 스크립트에 몰아넣는 것이 최선이 아님을 보여드렸습니다.\n대신 데이터 워크플로우를 관리하기 위한 커맨드 라인 도구로 make를 사용할 것을 제안했습니다.\n다음 장은 데이터 과학을 위한 OSEMN 모델의 세 번째 단계인 데이터 탐색(Exploring data)을 다룹니다.","code":""},{"path":"chapter-6-project-management-with-make.html","id":"더-읽어보기-1","chapter":"제 6 Make으로 프로젝트 관리하기","heading":"6.7 더 읽어보기","text":"Robert Mecklenburg의 저서 Managing Projects GNU Make와 온라인 GNU Make Manual은 make에 대한 포괄적이고 고급스러운 내용을 제공합니다.make 외에도 수많은 워크플로우 관리자가 존재합니다. 구문과 기능은 다르지만 타겟, 규칙, 의존성과 같은 개념을 공통적으로 사용합니다. 대표적인 예로 Luigi, Apache Airflow, Nextflow 등이 있습니다.버전 관리, 특히 git과 GitHub에 대해 더 자세히 알고 싶다면 Scott Chacon과 Ben Straub이 쓴 Pro Git을 추천합니다. 무료로 제공됩니다. 온라인 GitHub 문서 또한 좋은 시작점입니다.","code":""},{"path":"chapter-7-exploring-data.html","id":"chapter-7-exploring-data","chapter":"제 7 데이터 탐색하기","heading":"제 7 데이터 탐색하기","text":"그 모든 고된 작업 끝에 (이미 정제된 데이터가 준비되어 있지 않았다면 말이죠), 이제 즐거운 시간을 보낼 때입니다.\n데이터를 획득하고 정제했으니, 이제 OSEMN 모델의 세 번째 단계인 데이터 탐색(explore)으로 넘어갈 수 있습니다.데이터 탐색은 데이터와 친숙해지는 과정입니다.\n데이터를 잘 아는 것은 그로부터 가치를 추출하는 데 필수적입니다.\n예를 들어, 데이터가 어떤 특성(features)을 가지고 있는지 알면, 어떤 특성을 더 깊이 파고들 가치가 있는지, 그리고 가지고 있는 질문에 답하기 위해 어떤 특성을 사용할 수 있을지 알 수 있게 됩니다.데이터 탐색은 세 가지 관점에서 진행할 수 있습니다.\n첫 번째 관점은 데이터와 그 속성을 검사(inspect)하는 것입니다.\n여기서는 원시 데이터가 어떻게 생겼는지, 데이터셋에 데이터 포인트가 몇 개인지, 어떤 특성들이 있는지 등을 파악하고자 합니다.두 번째는 기술 통계량(descriptive statistics)을 계산하는 것입니다. 이 관점은 개별 특성에 대해 더 자세히 배우는 데 유용합니다.\n출력 결과는 대게 짧은 텍스트 형태이므로 커맨드 라인에 바로 출력할 수 있습니다.세 번째 관점은 데이터 시각화(visualizations)를 생성하는 것입니다. 이 관점을 통해 여러 특성이 서로 어떻게 상호작용하는지에 대한 통찰을 얻을 수 있습니다. 커맨드 라인에 출력할 수 있는 시각화 생성 방법을 설명하겠지만, 시각화는 그래픽 사용자 인터페이스(GUI)에 표시하는 것이 가장 적합합니다. 기술 통계량과 비교했을 때 데이터 시각화의 장점은 더 유연하고 훨씬 더 많은 정보를 전달할 수 있다는 점입니다.","code":""},{"path":"chapter-7-exploring-data.html","id":"개요-4","chapter":"제 7 데이터 탐색하기","heading":"7.1 개요","text":"이 장에서 여러분은 다음 내용을 배우게 됩니다.데이터와 그 속성 검사하기기술 통계량 계산하기커맨드 라인 안팎에서 데이터 시각화 생성하기이 장은 다음 파일들로 시작합니다.이 파일들을 가져오는 방법은 2장에 설명되어 있습니다.\n그 외의 파일들은 커맨드 라인 도구를 사용하여 다운로드하거나 생성한 것들입니다.","code":"cd /data/ch07\nl"},{"path":"chapter-7-exploring-data.html","id":"데이터와-그-속성-검사하기","chapter":"제 7 데이터 탐색하기","heading":"7.2 데이터와 그 속성 검사하기","text":"이 섹션에서는 데이터셋과 그 속성을 검사하는 방법을 보여드리겠습니다. 앞으로 다룰 시각화 및 모델링 기술들은 데이터가 표(rectangular) 형태일 것으로 예상하므로, 데이터가 CSV 형식이라고 가정하겠습니다. 필요한 경우 5장에서 설명한 기술을 사용하여 데이터를 CSV로 변환할 수 있습니다.편의상 데이터에 헤더(header)가 있다고 가정하겠습니다.\n첫 번째 하위 섹션에서는 실제로 헤더가 있는지 확인하는 방법을 보여드리겠습니다.\n헤더가 있다는 것을 알게 되면 다음과 같은 질문들에 답할 수 있습니다.데이터셋에 데이터 포인트와 특성이 몇 개나 있는가?원시 데이터는 어떻게 생겼는가?데이터셋에는 어떤 종류의 특성들이 있는가?이러한 특성 중 일부를 범주형(categorical)으로 취급할 수 있는가?","code":""},{"path":"chapter-7-exploring-data.html","id":"헤더의-유무-확인하기","chapter":"제 7 데이터 탐색하기","heading":"7.2.1 헤더의 유무 확인하기","text":"head를 사용하여 처음 몇 줄을 출력함으로써 파일에 헤더가 있는지 확인할 수 있습니다.행이 너무 길어 화면을 넘어간다면 nl을 사용하여 행 번호를 추가해 보세요.또는 trim을 사용할 수도 있습니다.이 경우에는 첫 번째 행이 대문자 이름만 포함하고 있고 그 다음 행들은 숫자를 포함하고 있으므로 헤더임이 분명합니다.\n이는 사실 상당히 주관적인 과정이며, 첫 번째 행을 헤더로 볼지 아니면 첫 번째 데이터 포인트로 볼지는 여러분의 판단에 달려 있습니다.\n데이터셋에 헤더가 없는 경우, 5장에서 다룬 header 도구를 사용하여 이를 바로잡는 것이 가장 좋습니다.","code":"head -n 5 venture.csvhead -n 3 venture.csv | nl< venture.csv trim 5"},{"path":"chapter-7-exploring-data.html","id":"모든-데이터-검사하기","chapter":"제 7 데이터 탐색하기","heading":"7.2.2 모든 데이터 검사하기","text":"원시 데이터를 자신의 속도에 맞춰 검토하고 싶다면 cat을 사용하는 것은 좋은 생각이 아닙니다. 모든 데이터가 한꺼번에 출력되기 때문입니다.\n커맨드 라인에서 데이터를 상호작용적으로 검사할 수 있게 해주는 less91를 사용하는 것을 추천합니다.\n-S 옵션을 지정하면 (venture.csv처럼) 긴 행이 줄바꿈되는 것을 방지할 수 있습니다.오른쪽의 크다 기호(>)는 가로로 스크롤할 수 있음을 나타냅니다.\n위 및 아래 방향키를 눌러 위아래로 스크롤할 수 있습니다.\n스페이스바를 누르면 한 화면씩 아래로 스크롤됩니다.\n가로 스크롤은 왼쪽 및 오른쪽 방향키를 사용합니다.\ng와 G를 누르면 각각 파일의 시작과 끝으로 이동합니다.\nless를 종료하려면 q를 누르면 됩니다.\n매뉴얼 페이지(manual page)에는 사용 가능한 모든 키 바인딩이 나열되어 있습니다.less의 장점 중 하나는 파일 전체를 메모리에 로드하지 않는다는 점입니다. 즉, 대용량 파일을 볼 때도 매우 빠릅니다.","code":"less -S venture.csv#! enter=FALSEEnter Right Left#! literal=FALSE, hold=0.1, wait=0.1"},{"path":"chapter-7-exploring-data.html","id":"특성-이름과-데이터-유형","chapter":"제 7 데이터 탐색하기","heading":"7.2.3 특성 이름과 데이터 유형","text":"열(또는 특성) 이름은 특성의 의미를 나타낼 수 있습니다.\n다음과 같은 head와 tr 조합을 사용할 수 있습니다.이 기본적인 명령어는 파일이 쉼표로 구분되어 있다고 가정합니다.\n더 견고한 접근 방식은 csvcut을 사용하는 것입니다.단순히 열 이름을 출력하는 것보다 한 걸음 더 나아갈 수 있습니다.\n열 이름 외에도 각 열이 문자열, 숫자, 또는 날짜와 같이 어떤 유형의 값을 포함하고 있는지 아는 것이 매우 유용할 것입니다.\n다음과 같은 장난감 데이터셋이 있다고 가정해 봅시다.csvlook은 이를 다음과 같이 해석합니다.5장에서 CSV 데이터에 직접 SQL 쿼리를 실행하기 위해 csvsql을 이미 사용해 보았습니다.\n커맨드 라인 인자를 전달하지 않으면, 이 데이터를 실제 데이터베이스에 삽입할 때 필요한 SQL 문을 생성합니다.\n이 출력을 사용하여 유추된 열 유형이 무엇인지 검사할 수도 있습니다.\n데이터 유형 뒤에 NULL 문자열이 인쇄되어 있다면 해당 열에는 결측치가 없다는 뜻입니다.이 출력은 csvgrep, csvsort, csvsql과 같은 csvkit 제품군의 다른 도구들을 사용할 때 특히 유용합니다.\nventure.csv의 경우 열 유형은 다음과 같이 유추됩니다.","code":"< venture.csv head -n 1 | tr , '\\n'csvcut -n venture.csvbat -A datatypes.csvcsvlook datatypes.csvcsvsql datatypes.csvcsvsql venture.csv"},{"path":"chapter-7-exploring-data.html","id":"고유-식별자-연속형-변수-그리고-요인factors","chapter":"제 7 데이터 탐색하기","heading":"7.2.4 고유 식별자, 연속형 변수, 그리고 요인(Factors)","text":"각 특성의 데이터 유형을 아는 것만으로는 충분하지 않습니다.\n각 특성이 무엇을 나타내는지 아는 것도 필수적입니다.\n도메인 지식이 있으면 매우 유용하겠지만, 데이터 자체를 살펴봄으로써 맥락을 파악할 수도 있습니다.문자열과 정수 모두 고유 식별자일 수도 있고 범주를 나타낼 수도 있습니다.\n후자의 경우 시각화에서 색상을 할당하는 데 사용될 수 있습니다.\n하지만 정수가 우편번호 등을 나타낸다면 평균을 계산하는 것은 의미가 없습니다.특성을 고유 식별자로 취급해야 할지 아니면 범주형 변수로 취급해야 할지 결정하기 위해, 특정 열의 고유 값(unique values) 개수를 세어볼 수 있습니다.csvkit의 일부인 csvstat92을 사용하여 각 열의 고유 값 개수를 얻을 수 있습니다.고유 값이 단 하나만 있다면 (OBS_STATUS처럼), 해당 열은 어떠한 가치도 제공하지 못하므로 삭제할 가능성이 큽니다.\n그러한 모든 열을 자동으로 삭제하고 싶다면 다음과 같은 파이프라인을 사용할 수 있습니다.➊ -C 옵션은 명령 대치(command substitution)를 통해 제공된 위치(또는 이름)의 열을 선택 해제합니다.\n➋ venture.csv의 각 열에 대한 고유 값 개수를 얻습니다.\n➌ 고유 값이 1개인 열만 남깁니다.\n➍ 열의 위치를 추출합니다.\n➎ 모든 공백을 제거합니다.\n➏ 모든 열 위치를 쉼표로 구분된 한 줄로 만듭니다.\n➐ 처음 10행만 보여줍니다.그렇긴 하지만, 일단은 그 열들을 유지하도록 하겠습니다.일반적으로 고유 값의 개수가 전체 행 수에 비해 적다면, 해당 특성은 범주형으로 취급될 수 있습니다 (venture.csv의 GEO처럼).\n만약 개수가 행 수와 같다면 고유 식별자일 수도 있지만 수치형 값일 수도 있습니다.\n이를 알아낼 수 있는 유일한 방법은 더 깊이 파고드는 것입니다.","code":"wc -l tips.csv\n< tips.csv csvcut -c day | header -d | sort | uniq | wc -lcsvstat tips.csv --unique\ncsvstat venture.csv --unique< venture.csv csvcut -C $(\n  csvstat venture.csv --unique |\n  grep ': 1$' |\n  cut -d. -f 1 |\n  tr -d ' ' |\n  paste -sd,\n) | trim"},{"path":"chapter-7-exploring-data.html","id":"기술-통계량-계산하기","chapter":"제 7 데이터 탐색하기","heading":"7.3 기술 통계량 계산하기","text":"","code":""},{"path":"chapter-7-exploring-data.html","id":"열-통계column-statistics","chapter":"제 7 데이터 탐색하기","heading":"7.3.1 열 통계(Column Statistics)","text":"커맨드 라인 도구인 csvstat은 많은 정보를 제공합니다. 각 특성(열)에 대해 다음과 같은 내용을 보여줍니다.데이터 유형결측치(nulls)의 존재 여부고유 값의 개수해당 특성에 적합한 다양한 기술 통계량 (최솟값, 최댓값, 합계, 평균, 표준 편차, 중앙값)csvstat을 다음과 같이 실행합니다.내용이 매우 많기 때문에 처음 32행만 보여드리고 있습니다. 이 출력을 less로 연결해서 보는 것이 좋습니다.\n특정 통계량에만 관심이 있다면 다음 옵션 중 하나를 사용할 수도 있습니다.--max (최댓값)--min (최솟값)--sum (합계)--mean (평균)--median (중앙값)--stdev (표준 편차)--nulls (결측치 포함 여부)--unique (고유 값 개수)--freq (빈번한 값)--len (최대 값 길이)예를 들어 다음과 같습니다.정수와 열 이름을 모두 사용할 수 있는 -c 옵션으로 특정 특성들만 선택할 수 있습니다.참고로 csvstat은 맨 마지막에 전체 데이터 포인트(행)의 수도 출력합니다.\n값 내부의 줄바꿈과 쉼표도 올바르게 처리됩니다.\n마지막 줄만 보고 싶다면 tail을 사용할 수 있습니다.\n또는 실제 행의 개수만 반환하는 xsv를 사용할 수도 있습니다.이 두 가지 옵션은 줄바꿈 횟수를 세는 (따라서 헤더도 포함하는) wc -l과 결과가 다르다는 점에 유의하십시오.","code":"csvstat venture.csv | trim 32csvstat venture.csv --freq | trimcsvstat venture.csv -c 3,GEOcsvstat venture.csv | tail -n 1\nxsv count venture.csv"},{"path":"chapter-7-exploring-data.html","id":"쉘에서의-r-한-줄-명령어","chapter":"제 7 데이터 탐색하기","heading":"7.3.2 쉘에서의 R 한 줄 명령어","text":"이 섹션에서는 커맨드 라인에서 직접 통계 프로그래밍 환경인 R93을 활용할 수 있게 해주는 rush94라는 커맨드 라인 도구를 소개해 드리고자 합니다.\nrush가 무엇을 하고 왜 존재하는지 설명하기 전에, 먼저 R 자체에 대해 잠깐 이야기해 보겠습니다.R은 데이터 과학을 하기에 매우 강력한 통계 소프트웨어 패키지입니다.\n패키지 모음이 방대하고 자체적인 REPL을 제공하는 인터프리터 언어이므로, 커맨드 라인과 비슷하게 데이터를 가지고 놀 수 있습니다.\nR을 시작하면 유닉스 커맨드 라인과는 분리된 상호작용 세션에 들어가게 됩니다.tips.csv라는 CSV 파일이 있고, 팁 비율(tip percentage)을 계산하여 그 결과를 저장하고 싶다고 가정해 봅시다.\n이를 R에서 수행하려면 먼저 R을 실행합니다.➊ 긴 시작 메시지를 생략하기 위해 --quiet 옵션을 사용했습니다.그 다음 아래의 코드를 실행합니다.➊ 필요한 패키지를 로드합니다.\n➋ CSV 파일을 읽어 들여 df 변수에 할당합니다.\n➌ 새로운 percent 열을 계산합니다.\n➍ 결과를 디스크에 저장합니다.\n➎ R을 종료합니다.그 후에 커맨드 라인에서 저장된 percent.csv 파일로 작업을 계속할 수 있습니다.여기서 세 번째 줄만이 여러분이 구체적으로 달성하고자 하는 작업과 관련이 있다는 점에 주목하십시오.\n나머지 줄들은 필요한 상용구(boilerplate)일 뿐입니다.\n단순한 작업을 위해 이런 상용구들을 일일이 입력하는 것은 번거롭고 워크플로우를 방해합니다.\n때로는 데이터에 대해 한두 가지만 하고 싶을 때가 있습니다.\nR의 위력을 커맨드 라인에서 바로 사용할 수 있다면 정말 좋지 않을까요?이것이 바로 rush가 등장하는 지점입니다.\n방금 전과 똑같은 작업을 rush를 사용하여 수행해 보겠습니다.rush가 모든 상용구를 대신 처리해주기 때문에 이러한 짧은 한 줄 명령어가 가능합니다.\n여기서는 run 서브 명령어를 사용하고 있습니다. 다음 섹션에서 데이터 시각화를 빠르게 생성할 때 사용할 plot 서브 명령어도 있습니다.\n입력 데이터를 전달할 때 rush는 기본적으로 헤더가 있고 쉼표로 구분된 CSV 형식이라고 가정합니다.\n또한 열 이름은 작업하기 쉽도록 정리(sanitized)됩니다.\n이러한 기본값들은 각각 ---header (또는 -H), --delimiter (또는 -d), ---clean-names (또는 -C) 옵션을 사용하여 변경할 수 있습니다.\n도움말을 보면 run 서브 명령어에서 사용 가능한 옵션들을 잘 확인할 수 있습니다.내부적으로 rush는 R 스크립트를 생성하고 바로 실행합니다.\n--dry-run (또는 -n) 옵션을 지정하면 이렇게 생성된 스크립트를 확인할 수 있습니다.생성된 스크립트는 다음과 같은 일을 합니다.커맨드 라인에서 R 스크립트를 실행하는 데 필요한 쉬뱅(#!; 4장 참조)을 작성합니다.tidyverse와 glue 패키지를 임포트합니다.tips.csv를 데이터 프레임으로 로드하고 열 이름을 정리한 뒤 df 변수에 할당합니다.지정된 표현식을 실행합니다.결과를 표준 출력으로 인쇄합니다.생성된 스크립트를 파일로 리다이렉션하면 쉬뱅 덕분에 아주 쉽게 새로운 커맨드 라인 도구로 만들 수 있습니다.rush의 출력이 반드시 CSV 형식이어야 할 필요는 없습니다. 여기서는 평균 팁 비율, 최대 일행 인원수, 시간 열의 고유 값들, 전체 금액과 팁 사이의 상관관계를 계산합니다. 마지막으로 열 하나를 통째로 추출합니다 (처음 10개 값만 보여줍니다).마지막의 대시(-) 기호는 rush가 표준 입력을 읽어야 함을 의미합니다.이제 여러분의 데이터셋에 R로 한두 가지 작업을 하고 싶을 때, 이를 한 줄 명령어로 지정하고 계속해서 커맨드 라인에서 작업을 이어 나갈 수 있습니다.\n여러분이 이미 알고 있는 R에 대한 모든 지식을 이제 커맨드 라인에서 사용할 수 있습니다. rush를 사용하면 다음 섹션에서 보여드릴 것처럼 정교한 시각화도 만들 수 있습니다.","code":"R --quietlibrary(tidyverse)                            ➊\ndf <- read_csv(\"tips.csv\")                    ➋\ndf <- mutate(df, percent = tip / bill * 100)  ➌\nwrite_csv(df, \"percent.csv\")                  ➍\nq(\"no\")                                       ➎< percent.csv trim 5rm percent.csv\nrush run -t 'mutate(df, percent = tip / bill * 100)' tips.csv > percent.csv\n< percent.csv trim 5rush run --helprush run -n --tidyverse 'mutate(df, percent = tip / bill * 100)' tips.csv< percent.csv rush run 'mean(df$percent)' -\n< percent.csv rush run 'max(df$size)' -\n< percent.csv rush run 'unique(df$time)' -\n< percent.csv rush run 'cor(df$bill, df$tip)' -\n< percent.csv rush run 'df$tip' - | trim"},{"path":"chapter-7-exploring-data.html","id":"시각화-생성하기","chapter":"제 7 데이터 탐색하기","heading":"7.4 시각화 생성하기","text":"이 섹션에서는 커맨드 라인에서 데이터 시각화를 생성하는 방법을 보여드리겠습니다.\nrush plot을 사용하여 막대 그래프, 산점도, 박스 플롯을 만들어 볼 것입니다.\n본격적으로 시작하기 전에, 생성한 시각화를 어떻게 표시(display)할 수 있는지 먼저 설명하겠습니다.","code":""},{"path":"chapter-7-exploring-data.html","id":"커맨드-라인에서-이미지-표시하기","chapter":"제 7 데이터 탐색하기","heading":"7.4.1 커맨드 라인에서 이미지 표시하기","text":"tips.png 이미지를 예로 들어보겠습니다.\nrush와 tips.csv 데이터셋을 사용하여 생성한 데이터 시각화인 Figure ??를 살펴보십시오.\n(rush 구문은 잠시 후에 설명하겠습니다.)\n저는 책에 이미지를 삽입하기 위해 display 도구를 사용하지만, 여러분이 직접 display 명령어를 실행하면 작동하지 않을 수도 있습니다.\n커맨드 라인에서 이미지를 표시하는 것은 사실 꽤나 까다로운 일이기 때문입니다.환경 설정에 따라 이미지를 표시하는 몇 가지 옵션이 있습니다.\n제가 아는 네 가지 옵션은 각각 장단점이 있습니다.\n(1) 텍스트 표현으로 표시,\n(2) 인라인(inline) 이미지로 표시,\n(3) 이미지 뷰어 사용,\n(4) 브라우저 사용.\n이들을 빠르게 훑어봅시다.옵션 1은 Figure ??의 상단에 표시된 것처럼 터미널 내부에 이미지를 표시하는 것입니다.\n표준 출력이 파일로 리다이렉션되지 않을 때 rush가 이 출력을 생성합니다.\n이는 ASCII 문자와 ANSI 이스케이프 시퀀스를 기반으로 하므로 모든 터미널에서 사용 가능합니다.\n이 책을 읽는 방식에 따라, 코드를 실행했을 때 얻는 출력이 Figure ??의 스크린샷과 일치할 수도 있고 아닐 수도 있습니다.만약 ASCII 문자만 보인다면, 이 책을 읽고 있는 매체가 색상을 담당하는 ANSI 이스케이프 시퀀스를 지원하지 않는다는 뜻입니다.\n다행히 위 명령어를 직접 실행해 보면 스크린샷과 똑같이 보일 것입니다.옵션 2는 Figure ??의 하단에서 볼 수 있듯이 터미널 내부에 이미지를 바로 표시합니다.\n이는 macOS에서만 사용 가능한 iTerm2 터미널이며, 작은 스크립트(제가 display라고 이름 붙인)를 통해 인라인 이미지 프로토콜(Inline Images Protocol)을 사용합니다.\n이 스크립트는 Docker 이미지에 포함되어 있지 않지만 쉽게 설치할 수 있습니다.macOS에서 iTerm2를 사용하지 않더라도 인라인으로 이미지를 표시할 수 있는 다른 옵션이 있을 수 있습니다.\n선호하는 검색 엔진을 통해 확인해 보시기 바랍니다.옵션 3은 이미지(이 예제에서는 tips.csv [역주: tips.png의 오타로 보임])를 이미지 뷰어에서 수동으로 여는 것입니다.\nFigure ??의 왼쪽은 macOS의 파일 탐색기(Finder)와 이미지 뷰어(Preview)를 보여줍니다.\n로컬에서 작업할 때는 이 옵션이 항상 작동합니다.\nDocker 컨테이너 안에서 작업할 때는 -v 옵션을 사용하여 로컬 디렉터리를 매핑한 경우에만 운영체제에서 생성된 이미지에 접근할 수 있습니다.\n방법에 대해서는 2장을 참조하십시오.\n이 옵션의 장점은 이미지가 변경되었을 때 대부분의 이미지 뷰어가 자동으로 화면을 갱신해준다는 것입니다. 덕분에 시각화를 미세 조정하면서 빠르게 반복 작업을 할 수 있습니다.옵션 4는 브라우저에서 이미지를 여는 것입니다.\nFigure ??의 오른쪽은 *http://localhost:8000/tips.png*를 보여주는 파이어폭스(Firefox) 스크린샷입니다.\n어떤 브라우저든 상관없지만, 이 옵션을 사용하려면 두 가지 전제 조건이 필요합니다.\n첫째, -p 옵션을 사용하여 Docker 컨테이너의 포트(이 예제에서는 8000번 포트)를 열어두어야 합니다.\n(역시 2장에 설명되어 있습니다.)\n둘째, 웹 서버를 실행해야 합니다.\n이를 위해 Docker 컨테이너에는 파이썬을 사용하여 현재 작업 디렉터리를 서비스하는 servewd95라는 작은 도구가 들어 있습니다.디렉터리(예: /data/)에서 servewd를 한 번만 실행해 두면 백그라운드에서 계속 실행됩니다.\n그림을 그린 뒤에는 브라우저에서 localhost:8000에 접속하여 해당 디렉터리와 모든 하위 디렉터리의 내용에 접근할 수 있습니다.\n기본 포트는 8000번이지만 servewd에 인자로 지정하여 변경할 수도 있습니다.해당 포트에 접근 가능한지만 확인하면 됩니다.\nservewd는 백그라운드에서 실행되므로 다음과 같이 종료해야 합니다.옵션 4는 원격 시스템에서도 작동할 수 있습니다.이제 이미지를 표시하는 네 가지 옵션을 다루었으니, 실제로 시각화를 만들어 봅시다.","code":"rush plot --x bill --y tip --color size --facets '~day' tips.csvcurl -s \"https://iterm2.com/utilities/imgcat\" > display && chmod u+x display#!enter=FALSE\nC-C#!literal=FALSEbat $(which servewd)servewd 9999#!enter=FALSE\nC-C#!literal=FALSEpkill -f http.server"},{"path":"chapter-7-exploring-data.html","id":"서둘러-그림-그리기","chapter":"제 7 데이터 탐색하기","heading":"7.4.2 서둘러 그림 그리기","text":"데이터 시각화를 생성할 때 가질 수 있는 선택지는 매우 많습니다.\n개인적으로 저는 R의 시각화 패키지인 ggplot2를 강력하게 지지합니다.\n기저에 흐르는 ’그래픽 문법(grammar graphics)’과 그를 따르는 일관된 API 덕분에, 매번 문서를 찾아볼 필요 없이 아름다운 데이터 시각화를 빠르고 반복적으로 만들어낼 수 있습니다.\n이는 데이터를 탐색할 때 매우 환영할 만한 특성입니다.지금 당장 급한(rush) 것은 아니지만, 하나의 시각화에 너무 많은 시간을 쏟고 싶지는 않습니다.\n또한 가능한 한 커맨드 라인에 머물고 싶습니다.\n다행히 우리에게는 커맨드 라인에서 ggplot2를 사용할 수 있게 해주는 rush가 있습니다.\nFigure ??의 시각화는 다음과 같이 만들 수 있었습니다.하지만 눈치채셨겠지만, 저는 tips.png를 만들기 위해 아주 다른 명령어를 사용했습니다.ggplot2의 구문이 유연성에 비해 상당히 간결하긴 하지만, 기본적인 그래프를 빠르게 생성할 수 있는 지름길이 있습니다.\n이 지름길은 rush의 plot 서브 명령어를 통해 제공됩니다.\n덕분에 R이나 그래픽 문법을 배우지 않고도 아름다운 기초 그래프를 만들 수 있습니다.내부적으로 rush plot은 ggplot2 패키지의 qplot 함수를 사용합니다.\n다음은 해당 함수의 문서 중 일부입니다.저도 이 조언에 동의합니다. 이 책을 다 읽고 나면 ggplot2를 배울 가치가 충분하며, 특히 탐색용 시각화를 외부에 전달하기 적합한 수준으로 업그레이드하고 싶다면 더욱 그렇습니다.\n지금은 커맨드 라인에 있으니 지름길을 택해 봅시다.Figure ??에서 이미 보았듯이, rush plot은 동일한 구문으로 그래픽 시각화(픽셀로 구성)와 텍스트 시각화(ASCII 문자와 ANSI 이스케이프 시퀀스로 구성)를 모두 만들 수 있습니다.\nrush가 자신의 출력이 다른 명령어로 파이프되거나(예: display) 파일로 리다이렉션되는 것을 감지하면 그래픽 시각화를 생성하고, 그렇지 않으면 텍스트 시각화를 생성합니다.잠시 시간을 내어 rush plot의 그래프 생성 및 저장 옵션들을 읽어봅시다.가장 중요한 옵션은 <name>을 인자로 받는 그래프 옵션들입니다.\n예를 들어 --x 옵션은 x축을 따라 물체들이 놓일 위치를 결정하는 데 어떤 열을 사용할지 지정해 줍니다.\n--y 옵션도 마찬가지입니다.\n--color와 --fill 옵션은 색상을 입히는 데 사용할 열을 지정합니다.\n--size와 --alpha 옵션이 무엇에 관한 것인지는 아마 짐작하실 수 있을 것입니다.\n기타 공통 옵션들은 다양한 시각화를 직접 만들어 보면서 각 섹션에서 설명하겠습니다.\n각 시각화에 대해 먼저 텍스트 표현(ASCII 및 ANSI 문자)을 보여드린 다음, 실제 그래픽 표현(픽셀)을 보여드리겠습니다.","code":"rush run --library ggplot2 'ggplot(df, aes(x = bill, y = tip, color = size)) + geom_point() + facet_wrap(~day)' tips.csv > tips.png#!enter=FALSE\nC-C#!literal=FALSErush plot --x bill --y tip --color size --facets '~day' tips.csv > tips.png#!enter=FALSE\nC-C#!literal=FALSER -q -e '?ggplot2::qplot' | trim 14rush plot --help"},{"path":"chapter-7-exploring-data.html","id":"막대-그래프bar-charts-생성하기","chapter":"제 7 데이터 탐색하기","heading":"7.4.3 막대 그래프(Bar Charts) 생성하기","text":"막대 그래프는 범주형 특성의 값 개수(counts)를 표시하는 데 특히 유용합니다.\n다음은 tips 데이터셋의 time 특성에 대한 텍스트 시각화입니다.Figure ??는 출력이 파일로 리다이렉션될 때 rush plot이 생성하는 그래픽 시각화입니다.이 막대 그래프로부터 내릴 수 있는 결론은 간단합니다. 점심(lunch)보다 저녁(dinner) 데이터 포인트가 두 배 이상 많다는 것입니다.","code":"rush plot --x time tips.csvrush plot --x time tips.csv > plot-bar.png\ndisplay plot-bar.png"},{"path":"chapter-7-exploring-data.html","id":"히스토그램histograms-생성하기","chapter":"제 7 데이터 탐색하기","heading":"7.4.4 히스토그램(Histograms) 생성하기","text":"연속형 변수의 개수는 히스토그램으로 시각화할 수 있습니다.\n여기서는 time 특성을 사용하여 채우기(fill) 색상을 설정했습니다.\n그 결과, rush plot은 편리하게도 누적 히스토그램(stacked histogram)을 생성합니다.Figure ??는 그래픽 시각화를 보여줍니다.이 히스토그램은 대부분의 팁이 2.5달러 근처임을 보여줍니다.\n저녁과 점심 두 그룹이 서로 겹쳐 쌓여 있고 절대적인 개수를 보여주기 때문에 서로 비교하기가 어렵습니다.\n아마도 밀도 그래프(density plot)가 도움이 될 수 있을 것 같습니다.","code":"rush plot --x tip --fill time tips.csv!! > plot-histogram.png\ndisplay !$"},{"path":"chapter-7-exploring-data.html","id":"밀도-그래프density-plots-생성하기","chapter":"제 7 데이터 탐색하기","heading":"7.4.5 밀도 그래프(Density Plots) 생성하기","text":"밀도 그래프는 연속형 변수의 분포를 시각화하는 데 유용합니다.\nrush plot은 적절한 도형(geometry)을 결정하기 위해 휴리스틱을 사용하지만, geom 옵션을 사용하여 이를 직접 지정할 수 있습니다.이 경우에는 텍스트 시각화가 Figure ??의 그래픽 시각화와 비교했을 때 확실히 한계가 있음을 보여줍니다.","code":"rush plot --x tip --fill time --geom density tips.csvrush plot --x tip --fill time --geom density tips.csv > plot-density.png\ndisplay plot-density.png"},{"path":"chapter-7-exploring-data.html","id":"행복한-사고happy-little-accidents","chapter":"제 7 데이터 탐색하기","heading":"7.4.6 행복한 사고(Happy Little Accidents)","text":"여러분은 이미 세 가지 유형의 시각화를 보았습니다.\nggplot2에서 이들은 각각 geom_bar, geom_histogram, geom_density 함수에 해당합니다.\ngeom은 기하학(geometry)의 약자로, 실제로 무엇을 그릴지 지시합니다.\n이 ggplot2 컨닝 페이퍼는 사용 가능한 도형 유형에 대한 좋은 개요를 제공합니다.\n사용할 수 있는 도형 유형은 지정한 열(및 그 유형)에 따라 달라집니다.\n모든 조합이 의미가 있는 것은 아닙니다.\n이 선 그래프(line plot)를 예로 들어보겠습니다.이 행복한 사고(역주: 밥 로스가 말하던 ’Happy Accident’를 인용한 듯함)는 Figure ??의 그래픽 표현에서 더 분명해집니다.tips.csv의 행들은 독립적인 관측치인 반면, 데이터 포인트 사이에 선을 긋는 것은 그들이 서로 연결되어 있다고 가정하는 것입니다.\ntip과 bill 사이의 관계를 시각화하려면 산점도(scatter plot)를 사용하는 것이 더 낫습니다.","code":"rush plot --x tip --y bill --color size --size day --geom path tips.csvrush plot --x tip --y bill --color size --size day --geom path tips.csv > plot-accident.png\ndisplay plot-accident.png"},{"path":"chapter-7-exploring-data.html","id":"산점도scatter-plots-생성하기","chapter":"제 7 데이터 탐색하기","heading":"7.4.7 산점도(Scatter Plots) 생성하기","text":"산점도는 도형이 점(point)인 그래프로, 두 개의 연속형 특성을 지정했을 때 기본적으로 선택됩니다.각 점의 색상은 --color 옵션(채우기 옵션인 --fill이 아닙니다)으로 지정된다는 점에 유의하십시오.\n그래픽 표현은 Figure ??를 참조하십시오.이 산점도로부터 우리는 전체 금액(bill)과 팁(tip) 사이에 관계가 있다는 결론을 내릴 수 있습니다.\n아마도 추세선(trend lines)을 만들어 더 높은 수준에서 이 데이터를 검토하는 것이 유용할 것입니다.","code":"rush plot --x bill --y tip --color time tips.csvrush plot --x bill --y tip --color time tips.csv > plot-scatter.png\ndisplay plot-scatter.png"},{"path":"chapter-7-exploring-data.html","id":"추세선trend-lines-생성하기","chapter":"제 7 데이터 탐색하기","heading":"7.4.8 추세선(Trend Lines) 생성하기","text":"기본 도형을 smooth로 변경하면 추세선을 시각화할 수 있습니다.\n이는 전체적인 그림을 파악하는 데 유용합니다.rush plot은 투명도를 처리할 수 없으므로, 이 경우에는 Figure ??와 같은 그래픽 표현이 훨씬 낫습니다.추세선과 함께 원래의 점들도 시각화하고 싶다면 rush run을 사용하여 직접 ggplot2 코드를 작성해야 합니다 (?? 참조).","code":"rush plot --x bill --y tip --color time --geom smooth tips.csvrush plot --x bill --y tip --color time --geom smooth tips.csv > plot-trend.png\ndisplay plot-trend.pngrush run --library ggplot2 'ggplot(df, aes(x = bill, y = tip, color = time)) + geom_point() + geom_smooth()' tips.csv > plot-trend-points.png\ndisplay plot-trend-points.png"},{"path":"chapter-7-exploring-data.html","id":"박스-플롯box-plots-생성하기","chapter":"제 7 데이터 탐색하기","heading":"7.4.9 박스 플롯(Box Plots) 생성하기","text":"박스 플롯은 하나 이상의 특성에 대해 다섯 수치 요약(최솟값, 최댓값, 표본 중앙값, 제1 및 제3 사분위수)을 시각화합니다.\n이 경우 size 특성을 factor() 함수를 사용하여 범주형으로 변환해야 합니다. 그렇지 않으면 bill 특성의 모든 값이 하나로 뭉쳐지게 됩니다.텍스트 표현도 나쁘지는 않지만, 그래픽 표현이 훨씬 더 명확합니다 (?? 참조).당연하게도, 이 박스 플롯은 평균적으로 일행 인원수가 많을수록 전체 금액이 더 높아짐을 보여줍니다.","code":"rush plot --x 'factor(size)' --y bill --geom boxplot tips.csvrush plot --x 'factor(size)' --y bill --geom boxplot tips.csv > plot-boxplot.png\ndisplay plot-boxplot.png"},{"path":"chapter-7-exploring-data.html","id":"레이블-추가하기","chapter":"제 7 데이터 탐색하기","heading":"7.4.10 레이블 추가하기","text":"기본 레이블은 열 이름(또는 명세)을 기반으로 합니다.\n이전 이미지에서 factor(size)라는 레이블은 개선될 필요가 있습니다.\n--xlab 및 --ylab 옵션을 사용하여 x축과 y축의 레이블을 변경할 수 있습니다.\n제목은 --title 옵션으로 추가할 수 있습니다.\n다음은 이를 보여주는 바이올린 플롯(박스 플롯과 밀도 그래프를 혼합한 것)입니다 (?? 참조).적절한 레이블과 제목을 시각화에 추가하는 것은 다른 사람(또는 미래의 자신)과 공유할 때 무엇을 보여주고 있는지 더 쉽게 이해할 수 있게 해주므로 특히 유용합니다.","code":"rush plot --x 'factor(size)' --y bill --geom violin --title '일행 인원수별 전체 금액 분포' --xlab '일행 인원수' --ylab '전체 금액 (USD)' tips.csvrush plot --x 'factor(size)' --y bill --geom violin --title '일행 인원수별 전체 금액 분포' --xlab '일행 인원수' --ylab '전체 금액 (USD)' tips.csv > plot-labels.png\ndisplay plot-labels.png"},{"path":"chapter-7-exploring-data.html","id":"기초적인-그래프-그-이상으로","chapter":"제 7 데이터 탐색하기","heading":"7.4.11 기초적인 그래프 그 이상으로","text":"데이터를 탐색할 때 기초적인 그래프를 만드는 데는 rush plot이 적합하지만, 분명히 한계가 있습니다.\n때로는 여러 도형을 겹쳐 그리거나 좌표계 변환, 테마 적용과 같이 더 유연하고 정교한 옵션이 필요할 때가 있습니다.\n그럴 경우에는 rush plot의 기능이 기반하고 있는 R 패키지인 ggplot2에 대해 더 배워보는 것이 가치 있을 것입니다.\n만약 R보다 파이썬을 더 좋아하신다면 파이썬용 ggplot2 재구현체인 plotnine 패키지가 있습니다.","code":""},{"path":"chapter-7-exploring-data.html","id":"요약-6","chapter":"제 7 데이터 탐색하기","heading":"7.5 요약","text":"이 장에서는 데이터를 탐색하는 다양한 방법을 살펴보았습니다.\n텍스트 기반 시각화와 그래픽 기반 시각화 모두 장단점이 있습니다.\n그래픽 시각화는 분명히 품질이 훨씬 높지만 커맨드 라인에서 보기가 까다로울 수 있습니다.\n이때 텍스트 시각화가 유용하게 쓰입니다.\n최소한 rush는 R과 ggplot2 덕분에 두 가지 유형 모두에 대해 일관된 구문을 제공합니다.다음 장은 다시 한번 ‘인터메조(intermezzo)’ 장으로, 명령어와 파이프라인의 속도를 높이는 방법을 다룹니다.\n9장에서 데이터 모델링을 시작하고 싶어 견딜 수 없다면 이 장은 나중에 읽으셔도 좋습니다.","code":""},{"path":"chapter-7-exploring-data.html","id":"더-읽어보기-2","chapter":"제 7 데이터 탐색하기","heading":"7.6 더 읽어보기","text":"아쉽게도 제대로 된 ggplot2 튜토리얼은 이 책의 범위를 벗어납니다. 데이터 시각화 능력을 키우고 싶다면 그래픽 문법의 힘과 아름다움을 이해하는 데 시간을 투자하시기를 강력히 권장합니다. Hadley Wickham과 Garrett Grolemund의 저서 R Data Science의 3장과 28장은 훌륭한 자료입니다.3장과 28장 이야기가 나와서 말인데, 만약 R보다 파이썬을 선호하신다면 제가 Plotnine과 Pandas를 사용하여 해당 장들을 파이썬으로 번역해둔 자료가 있습니다.","code":""},{"path":"chapter-8-parallel-pipelines.html","id":"chapter-8-parallel-pipelines","chapter":"제 8 병렬 파이프라인","heading":"제 8 병렬 파이프라인","text":"이전 장들에서 우리는 전체 작업을 한 번에 처리하는 명령어와 파이프라인을 다루었습니다.\n하지만 실제로는 동일한 명령어 또는 파이프라인을 여러 번 실행해야 하는 작업에 직면하게 될 수도 있습니다.\n예를 들어 다음과 같은 상황입니다.수백 개의 웹 페이지 스크래핑하기수십 개의 API를 호출하고 그 출력을 변환하기다양한 매개변수 값에 대해 분류기(classifier) 훈련하기데이터셋의 모든 특성 쌍에 대해 산점도 생성하기위의 예시들에는 모두 특정한 형태의 반복 작업이 포함되어 있습니다.\n여러분은 선호하는 스크립트나 프로그래밍 언어에서 루프나 루프를 통해 이를 처리했을 것입니다.\n커맨드 라인에서 가장 먼저 하고 싶은 일은 아마 위 방향키를 눌러 이전 명령어를 불러온 뒤, 필요한 경우 수정하고 엔터를 눌러 다시 실행하는 것일지도 모릅니다.\n두세 번 정도는 괜찮겠지만, 이를 수십 번 한다고 상상해 보십시오.\n그러한 접근 방식은 금세 번거롭고 효율적이지 않으며 오류가 발생하기 쉬워집니다.\n다행인 점은 이러한 루프를 커맨드 라인에서도 작성할 수 있다는 것입니다.\n이것이 바로 이번 장에서 다룰 주제입니다.때로는 빠른 명령어를 하나씩 차례대로(순차적으로) 반복하는 것만으로도 충분합니다.\n하지만 데이터 집약적인 작업에 처했을 때, 여러 개의 코어(혹은 여러 대의 컴퓨터)가 있다면 이를 활용할 수 있으면 좋을 것입니다.\n여러 코어나 장비를 사용하면 전체 실행 시간을 크게 단축할 수 있습니다.\n이번 장에서는 바로 이러한 작업을 처리해 주는 parallel96이라는 매우 강력한 도구를 소개해 드리겠습니다. 이 도구를 사용하면 숫자, 텍스트 줄(lines), 파일과 같은 다양한 인자(arguments) 범위에 대해 명령어 또는 파이프라인을 적용할 수 있습니다.\n게다가 이름에서 알 수 있듯이, 명령어를 병렬로 실행할 수 있게 해줍니다.","code":""},{"path":"chapter-8-parallel-pipelines.html","id":"개요-5","chapter":"제 8 병렬 파이프라인","heading":"8.1 개요","text":"이번 인터메조(intermezzo) 장에서는 명령어와 파이프라인을 여러 번 실행해야 하는 작업의 속도를 높이는 여러 가지 접근 방식을 다룹니다.\n저의 주된 목표는 parallel의 유연함과 강력함을 보여드리는 것입니다.\n이 도구는 이 책에서 다루는 다른 어떤 도구와도 결합될 수 있으므로, 데이터 과학을 위해 커맨드 라인을 사용하는 여러분의 방식을 긍정적으로 바꿔놓을 것입니다.\n이번 장에서 여러분은 다음 내용을 배우게 됩니다.숫자, 텍스트 줄, 파일 범위에 대해 명령어를 순차적으로 실행하기큰 작업을 여러 개의 작은 작업으로 쪼개기파이프라인을 병렬로 실행하기파이프라인을 여러 대의 컴퓨터로 분산하기이번 장은 다음 파일들로 시작합니다.이 파일들을 가져오는 방법은 2장에 설명되어 있습니다.\n그 외의 파일들은 커맨드 라인 도구를 사용하여 다운로드하거나 생성한 것들입니다.","code":"cd /data/ch08\nl"},{"path":"chapter-8-parallel-pipelines.html","id":"순차-처리serial-processing","chapter":"제 8 병렬 파이프라인","heading":"8.2 순차 처리(Serial Processing)","text":"병렬화에 대해 깊이 파고들기 전에, 순차적인 방식으로 루프를 실행하는 것에 대해 짧게 다루어 보겠습니다.\n이 기능을 알아두는 가치가 있는 이유는 항상 사용 가능한 기능이고, 구문이 다른 프로그래밍 언어의 루프와 매우 비슷하며, 무엇보다 나중에 parallel의 소중함을 정말로 깨닫게 해주기 때문입니다.이번 장의 서론에서 제공한 예시들로부터 루프를 돌릴 세 가지 유형의 항목을 추출할 수 있습니다. 바로 숫자, 텍스트 줄, 그리고 파일입니다.\n이 세 가지 유형에 대해 다음 세 개의 하위 섹션에서 각각 논의하겠습니다.","code":""},{"path":"chapter-8-parallel-pipelines.html","id":"숫자에-대해-루프-돌리기","chapter":"제 8 병렬 파이프라인","heading":"8.2.1 숫자에 대해 루프 돌리기","text":"0에서 100 사이의 모든 짝수의 제곱을 계산해야 한다고 상상해 봅시다. bc97라는 도구가 있는데, 이는 수식을 파이프로 전달할 수 있는 ’기본 계산기(basic calculator)’입니다.\n4의 제곱을 계산하는 명령어는 다음과 같습니다.일회성 계산이라면 이것으로 충분합니다.\n하지만 서론에서 언급했듯이, 위 방향키를 누르고 숫자를 바꾼 뒤 엔터를 누르는 짓을 50번이나 반복하는 것은 제정신이 아닐 것입니다!\n이런 경우에는 루프를 사용하여 쉘이 대신 고된 일을 하도록 만드는 것이 좋습니다.➊ Z 쉘에는 중괄호 확장(brace expansion)이라는 기능이 있어, {0..100..2}를 공백으로 구분된 목록인 0 2 4 … 98 100으로 변환합니다. 변수 i에는 첫 번째 반복에서 “0”, 두 번째 반복에서 “1” [역주: 여기서는 2씩 증가하므로 “2”] 등의 값이 할당됩니다.\n➋ 이 변수의 값은 앞에 달러 기호($)를 붙여서 사용할 수 있습니다. 쉘은 echo가 실행되기 전에 $i를 그 값으로 교체합니다. do와 done 사이에는 하나 이상의 명령어가 올 수 있다는 점에 유의하십시오.비록 구문이 여러분이 선호하는 프로그래밍 언어에 비해 조금 이상해 보일 수 있지만, 쉘에서 항상 사용할 수 있는 기능이므로 기억해 둘 가치가 있습니다.\n잠시 후에 명령어를 반복하는 더 좋고 유연한 방법을 소개해 드리겠습니다.","code":"echo \"4^2\" | bcfor i in {0..100..2}  ➊\ndo\necho \"$i^2\" | bc      ➋\ndone | trim"},{"path":"chapter-8-parallel-pipelines.html","id":"텍스트-줄lines에-대해-루프-돌리기","chapter":"제 8 병렬 파이프라인","heading":"8.2.2 텍스트 줄(Lines)에 대해 루프 돌리기","text":"두 번째로 루프를 돌릴 수 있는 항목 유형은 텍스트 줄입니다.\n이 줄들은 파일에서 올 수도 있고 표준 입력에서 올 수도 있습니다.\n이 항목들은 숫자, 날짜, 이메일 주소 등 무엇이든 포함할 수 있으므로 매우 일반적인 접근 방식입니다.모든 연락처에 이메일을 보내고 싶다고 상상해 봅시다.\n먼저 무료 Random User Generator API를 사용하여 가짜 사용자 데이터를 생성해 보겠습니다.루프를 사용하여 emails 파일의 줄들에 대해 루프를 돌릴 수 있습니다.➊ 이 경우에는 Z 쉘이 입력이 몇 줄로 구성되어 있는지 미리 알 수 없으므로 루프를 사용해야 합니다.\n➋ 이 경우 line 변수 주위의 중괄호는 필수적이지 않지만 (변수 이름에 마침표가 포함될 수 없으므로), 그래도 중괄호를 사용하는 것은 좋은 습관입니다.\n➌ 이 리다이렉션은 앞에 올 수도 있습니다.표준 입력 장치인 /dev/stdin을 지정하여 상호작용적으로 루프에 입력을 제공할 수도 있습니다. 작업이 끝나면 Ctrl-D를 누르십시오.하지만 이 방식은 엔터를 누르는 즉시 해당 입력 줄에 대해 do와 done 사이의 명령어가 바로 실행된다는 단점이 있습니다. 되돌릴 수 없습니다.","code":"curl -s \"https://randomuser.me/api/1.2/?results=5&seed=dsatcl2e\" > users.json\n< users.json jq -r '.results[].email' > emails\nbat emailswhile read line                         ➊\ndo\necho \"Sending invitation to ${line}.\"   ➋\ndone < emails                           ➌while read line; do echo \"You typed: ${line}.\"; done < /dev/stdin#! expect_prompt=FALSE\none#! expect_prompt=FALSE\ntwo#! expect_prompt=FALSE\nthree#! expect_prompt=FALSE\nC-D#! literal=FALSE, expect_prompt=TRUE"},{"path":"chapter-8-parallel-pipelines.html","id":"파일에-대해-루프-돌리기","chapter":"제 8 병렬 파이프라인","heading":"8.2.3 파일에 대해 루프 돌리기","text":"이 섹션에서는 우리가 자주 루프를 돌려야 하는 세 번째 항목인 파일에 대해 논의합니다.특수 문자를 처리하려면 ls98 대신 글로빙(globbing, 즉 경로명 확장)을 사용하십시오.중괄호 확장과 마찬가지로, /data/* 표현식은 루프에 의해 처리되기 전에 Z 쉘에 의해 목록으로 먼저 확장됩니다.파일 목록을 작성하는 더 정교한 대안은 find99입니다. 이 도구는 다음과 같은 특징이 있습니다.하위 디렉터리까지 탐색할 수 있음크기, 접근 시간, 권한 등 속성에 대한 정교한 검색 가능공백이나 줄바꿈과 같은 특수 문자 처리 가능예를 들어, 다음 find 실행은 /data 디렉터리 아래에 있는 파일 중 확장자가 csv이고 크기가 2킬로바이트 미만인 모든 파일을 나열합니다.","code":"for chapter in /data/*\ndo\necho \"Processing Chapter ${chapter}.\"\ndonefind /data -type f -name '*.csv' -size -2k"},{"path":"chapter-8-parallel-pipelines.html","id":"병렬-처리parallel-processing","chapter":"제 8 병렬 파이프라인","heading":"8.3 병렬 처리(Parallel Processing)","text":"여기에 있는 것처럼 매우 오래 실행되는 도구가 있다고 가정해 봅시다.➊ ts100는 타임스탬프를 추가합니다.\n➋ 마법 변수 RANDOM은 0에서 32767 사이의 의사 난수 개수를 반환하는 내부 Bash 함수를 호출합니다. 그 정수를 5로 나눈 나머지에 1을 더하면 duration이 1에서 5 사이가 되도록 보장합니다.\n➌ sleep은 주어진 초 동안 실행을 일시 중지합니다.이 프로세스는 아마 모든 가용 리소스를 다 쓰지는 않을 것입니다.\n그런데 이 명령어를 아주 많이 실행해야 하는 상황이 생겼다고 해봅시다.\n예를 들어, 일련의 파일 전체를 다운로드해야 하는 경우입니다.병렬화하는 순진한 방법은 명령어를 백그라운드에서 실행하는 것입니다.\nslow.sh를 세 번 실행해 보겠습니다.➊ 앰퍼샌드(&)는 명령어를 백그라운드로 보내어, 루프가 즉시 다음 반복으로 계속 진행할 수 있게 합니다.\n➋ 이 줄은 Z 쉘이 부여한 작업 번호와 프로세스 ID를 보여주며, 이는 더 세밀한 작업 제어에 사용될 수 있습니다. 이 주제는 강력하지만 이 책의 범위를 벗어납니다.Figure ??은 개념적인 수준에서 순차 처리, 순진한 병렬 처리, 그리고 GNU Parallel을 사용한 병렬 처리 사이의 차이를 동시에 실행되는 프로세스 수와 전체 실행 시간 측면에서 보여줍니다.이 순진한 접근 방식에는 두 가지 문제가 있습니다.\n첫째, 동시에 실행되는 프로세스의 수를 제어할 방법이 없습니다.\n너무 많은 작업을 한꺼번에 시작하면 CPU, 메모리, 디스크 액세스, 네트워크 대역폭과 같은 동일한 리소스를 놓고 서로 경쟁하게 될 수 있습니다.\n이는 오히려 전체 실행 시간을 늘리는 결과를 초래할 수 있습니다.\n둘째, 어떤 출력이 어떤 입력에 속하는지 구분하기 어렵습니다.\n이제 더 나은 접근 방식을 살펴보겠습니다.","code":"bat slow.shfor i in {A..C}; do\n./slow.sh $i &\ndone#! hold=7"},{"path":"chapter-8-parallel-pipelines.html","id":"gnu-parallel-소개","chapter":"제 8 병렬 파이프라인","heading":"8.3.1 GNU Parallel 소개","text":"명령어와 파이프라인을 병렬화하고 분산할 수 있게 해주는 커맨드 라인 도구인 parallel을 소개하겠습니다.\n이 도구의 장점은 기존 도구들을 있는 그대로 사용할 수 있다는 것입니다. 도구들을 수정할 필요가 없습니다.parallel의 세부 사항을 살펴보기 전에, 이전의 루프를 얼마나 쉽게 대체할 수 있는지 보여드리는 짧은 예시를 보십시오.이것이 parallel의 가장 단순한 형태입니다. 루프를 돌릴 항목들은 표준 입력을 통해 전달되며, parallel이 실행해야 할 명령어 외에는 다른 인자가 없습니다.\nparallel이 어떻게 입력을 프로세스들에 동시에 분산하고 그 출력들을 수집하는지에 대한 그림은 Figure ??를 참조하십시오.보시다시피 기본적으로 루프 역할을 합니다.\n이전 섹션의 루프를 대체하는 또 다른 예시입니다.여기서는 --jobs 옵션을 사용하여 parallel이 동시에 최대 두 개의 작업을 실행하도록 지정했습니다. slow.sh에 전달할 인자들은 표준 입력 대신 명령문의 인자로 지정되었습니다 (::: 사용).parallel은 무려 159개의 서로 다른 옵션을 제공하여 매우 방대한 기능을 갖추고 있습니다.\n(아마도 너무 많을지도 모릅니다.)\n다행히 효과적으로 사용하기 위해 알아야 할 옵션은 몇 개 되지 않습니다.\n덜 일반적인 옵션을 사용해야 할 경우 매뉴얼 페이지가 아주 유용합니다.","code":"seq 0 2 100 | parallel \"echo {}^2 | bc\" | trimparallel --jobs 2 ./slow.sh ::: {A..C}"},{"path":"chapter-8-parallel-pipelines.html","id":"입력-지정하기","chapter":"제 8 병렬 파이프라인","heading":"8.3.2 입력 지정하기","text":"parallel에서 가장 중요한 인자는 모든 입력에 대해 실행하고 싶은 명령어 또는 파이프라인입니다.\n여기서 질문은, 입력 항목이 명령어의 어느 위치에 삽입되어야 하느냐는 것입니다.\n아무것도 지정하지 않으면 입력 항목이 파이프라인의 맨 끝에 추가됩니다.위의 명령어는 다음과 같이 실행하는 것과 같습니다.➊ 출력이 이전과 같으므로 출력을 억제하기 위해 /dev/null로 리다이렉션했습니다.이 방식이 작동할 때도 많지만, 플레이스홀더(placeholders)를 사용하여 명령어의 어느 부분에 입력 항목이 삽입되어야 하는지 명시하는 것이 좋습니다.\n이 경우 입력 줄 전체(숫자 하나)를 한 번에 사용하고 싶으므로 하나의 플레이스홀더만 있으면 됩니다.\n플레이스홀더, 즉 입력 항목을 넣을 위치는 한 쌍의 중괄호({})로 지정합니다.입력 항목이 파일 이름인 경우, 파일 이름의 일부만 사용할 수 있는 몇 가지 수식어(modifiers)가 있습니다.\n예를 들어 {/}를 사용하면 파일 이름의 베이스 이름(basename)만 사용됩니다.➊ 괄호())나 따옴표(\") 같은 문자는 쉘에서 특별한 의미를 갖습니다. 이를 문자 그대로 사용하려면 앞에 백슬래시(\\)를 붙입니다. 이를 이스케이프(escaping)라고 합니다.입력 줄이 구분자로 나뉜 여러 부분으로 구성된 경우 플레이스홀더에 번호를 추가할 수 있습니다. 예를 들어 다음과 같습니다.여기에 똑같은 플레이스홀더 수식어를 적용할 수 있습니다.\n동일한 입력 항목을 재사용하는 것도 가능합니다.\n만약 parallel의 입력이 헤더가 있는 CSV 파일이라면, 열 이름을 플레이스홀더로 사용할 수 있습니다.","code":"seq 3 | parallel cowsay#! enter=FALSE\nC-C#! literal=FALSE\nparallel --jobs 1 --keep-order cowsay ::: 1 2 3cowsay 1 > /dev/null ➊\ncowsay 2 > /dev/null\ncowsay 3 > /dev/nullseq 3 | parallel cowsay {} > /dev/nullfind /data/ch03 -type f | parallel echo '{#}\\) \\\"{}\\\" has basename \\\"{/}\\\"'touch input.csv\n< input.csv parallel --colsep , \"mv {2} {1}\" > /dev/null#! enter=FALSE\nC-C#! literal=FALSE< input.csv parallel -C, --header : \"invite {name} {email}\"#! enter=FALSE\nC-C#! literal=FALSE"},{"path":"chapter-8-parallel-pipelines.html","id":"동시-실행-작업-수-제어하기","chapter":"제 8 병렬 파이프라인","heading":"8.3.3 동시 실행 작업 수 제어하기","text":"기본적으로 parallel은 CPU 코어당 하나의 작업을 실행합니다.\n동시에 실행되는 작업 수는 --jobs 또는 -j 옵션으로 제어할 수 있습니다.\n숫자를 지정하면 해당 숫자만큼의 작업이 동시에 실행됩니다.\n숫자 앞에 플러스 기호(+)를 붙이면 CPU 코어 수에 해당 숫자를 더한 만큼의 작업을 실행합니다. 마이너스 기호(-)를 붙이면 CPU 코어 수에서 해당 숫자를 뺀 만큼의 작업을 실행합니다.\n여기서 CPU 코어 수는 N을 의미합니다.\n퍼센트를 지정할 수도 있으며, 기본값은 CPU 코어 수의 100%입니다.\n동시에 실행할 최적의 작업 수는 여러분이 실행하고 있는 실제 명령어에 따라 달라집니다.-j1을 지정하면 명령어가 한 번에 하나씩 순차적으로 실행됩니다. 비록 도구 이름의 의미를 무색하게 만들긴 하지만, 여전히 유용할 때가 있습니다. 예를 들어, 한 번에 하나의 연결만 허용하는 API에 접근해야 할 때입니다. -j0을 지정하면 parallel은 가능한 한 많은 작업을 병렬로 실행합니다. 이는 루프 끝에 앰퍼샌드를 붙여서 실행하는 것과 비슷합니다. 이 방식은 권장되지 않습니다.","code":"seq 5 | parallel -j0 \"echo Hi {}\"seq 5 | parallel -j200% \"echo Hi {}\""},{"path":"chapter-8-parallel-pipelines.html","id":"로깅과-출력","chapter":"제 8 병렬 파이프라인","heading":"8.3.4 로깅과 출력","text":"각 명령어의 출력을 저장하고 싶을 때, 아마 다음과 같은 방식을 생각할 수도 있습니다.이렇게 하면 출력이 각각의 개별 파일로 저장됩니다.\n혹은 모든 내용을 하나의 큰 파일에 저장하고 싶다면 다음과 같이 할 수 있습니다.하지만 parallel은 출력을 개별 파일로 저장해 주는 --results 옵션을 제공합니다.\n각 작업에 대해 parallel은 세 가지 파일을 생성합니다. 작업 번호를 담은 seq, 작업에 의해 생성된 표준 출력을 담은 stdout, 그리고 작업 중에 발생한 오류를 담은 stderr입니다.\n이 세 파일은 입력 값에 기반한 하위 디렉터리에 배치됩니다.parallel은 여전히 모든 출력을 인쇄하는데, 이 경우에는 불필요합니다.\n다음과 같이 표준 입력과 표준 출력 모두를 넘길 수 있습니다.--results 옵션이 어떻게 작동하는지에 대한 그림은 Figure ??를 참조하십시오.여러 작업을 병렬로 실행할 때, 작업이 실행되는 순서는 입력 순서와 일치하지 않을 수 있습니다.\n따라서 작업의 출력도 섞이게 됩니다.\n순서를 동일하게 유지하려면 --keep-order 또는 -k 옵션을 지정하십시오.가끔 어떤 입력이 어떤 출력을 생성했는지 기록하는 것이 유용할 때가 있습니다.\nparallel은 --tag 옵션을 통해 출력에 ’태그’를 달 수 있게 해주는데, 이는 각 줄의 앞에 입력 항목을 붙여줍니다.","code":"seq 5 | parallel \"echo \\\"Hi {}\\\" > hi-{}.txt\"seq 5 | parallel \"echo Hi {}\" >> one-big-file.txtseq 10 | parallel --results outdir \"curl 'https://anapioficeandfire.com/api/characters/{}' | jq -r '.aliases[0]'\" 2>/dev/null 1>&2\ntree outdir | trimseq 5 | parallel --tag \"echo 'sqrt({})' | bc -l\"\nparallel --tag --keep-order \"echo '{1}*{2}' | bc -l\" ::: 3 4 ::: 5 6 7"},{"path":"chapter-8-parallel-pipelines.html","id":"병렬-도구-만들기","chapter":"제 8 병렬 파이프라인","heading":"8.3.5 병렬 도구 만들기","text":"본 장의 서두에서 사용했던 bc 도구는 그 자체로는 병렬로 작동하지 않습니다.\n하지만 parallel을 사용하여 병렬화할 수 있습니다.\nDocker 이미지에는 pbc101라는 도구가 포함되어 있습니다.\n해당 코드는 다음과 같습니다.이 도구를 사용하면 본 장 서두에서 사용한 코드를 단순화할 수 있습니다.\n또한 쉼표로 구분된 값들을 동시에 처리할 수 있습니다.","code":"bat $(which pbc)seq 100 | pbc '{1}^2' | trim\npaste -d, <(seq 4) <(seq 4) <(seq 4) | pbc 'sqrt({1}+{2})^{3}'"},{"path":"chapter-8-parallel-pipelines.html","id":"분산-처리distributed-processing","chapter":"제 8 병렬 파이프라인","heading":"8.4 분산 처리(Distributed Processing)","text":"가끔은 모든 코어를 다 쓴다 하더라도 로컬 머신이 제공할 수 있는 것보다 더 많은 컴퓨팅 성능이 필요할 때가 있습니다.\n다행히 parallel은 원격 머신의 성능도 활용할 수 있게 해주어, 파이프라인의 속도를 비약적으로 높여줍니다.대단한 점은 원격 머신에 parallel이 반드시 설치되어 있을 필요가 없다는 것입니다.\n원격 머신에 보안 쉘(Secure Shell) 프로토콜(즉, SSH)로 접속할 수만 있으면 됩니다. parallel은 이 SSH를 사용하여 파이프라인을 분산시킵니다.\n(parallel이 원격 머신에 설치되어 있다면 각 원격 머신에서 몇 개의 코어를 고용할지 결정하는 데 도움이 되므로 설치되어 있는 것이 좋습니다. 이에 대해서는 나중에 더 자세히 다루겠습니다.)먼저, 실행 중인 AWS EC2 인스턴스 목록을 가져오겠습니다.\n원격 머신이 없더라도 걱정하지 마십시오. parallel에게 어떤 원격 머신을 사용할지 알려주는 --slf hostnames 옵션이 나올 때마다 이를 --sshlogin :으로 바꾸면 됩니다.\n이렇게 하면 이 섹션의 예제들을 여전히 따라 할 수 있습니다.사용할 원격 머신들을 파악했다면, 다음 세 가지 방식의 분산 처리를 고려해 볼 것입니다.원격 머신에서 일반 명령어 실행하기로컬 데이터를 직접 원격 머신들로 분산하기원격 머신으로 파일을 보내고, 처리한 뒤, 결과를 가져오기","code":""},{"path":"chapter-8-parallel-pipelines.html","id":"실행-중인-aws-ec2-인스턴스-목록-가져오기","chapter":"제 8 병렬 파이프라인","heading":"8.4.1 실행 중인 AWS EC2 인스턴스 목록 가져오기","text":"이 섹션에서는 원격 머신의 호스트네임을 한 줄에 하나씩 담고 있는 hostnames라는 파일을 만들 것입니다.\n여기서는 아마존 웹 서비스(AWS)를 예로 들겠습니다.\n여러분에게 AWS 계정이 있고 인스턴스를 시작하는 방법을 알고 있다고 가정합니다.\n다른 클라우드 컴퓨팅 서비스(예: 구글 클라우드 플랫폼 또는 마이크로소프트 애저)를 사용 중이거나 독자적인 서버를 보유하고 있다면, 다음 섹션으로 넘어가기 전에 직접 hostnames 파일을 만드시기 바랍니다.AWS API의 커맨드 라인 인터페이스인 aws102를 사용하여 실행 중인 AWS EC2 인스턴스 목록을 가져올 수 있습니다.\naws를 사용하면 온라인 AWS 관리 콘솔에서 할 수 있는 거의 모든 일을 할 수 있습니다.aws ec2 describe-instances 명령어는 모든 EC2 인스턴스에 대한 방대한 정보를 JSON 형식으로 반환합니다 (자세한 내용은 온라인 문서를 참조하십시오).\njq를 사용하여 관련 필드만 추출할 수 있습니다.EC2 인스턴스의 가능한 상태는 pending, running, shutting-, terminated, stopping, 그리고 stopped입니다.\n실행 중인(running) 인스턴스에만 파이프라인을 분산할 수 있으므로, 다음과 같이 실행 중이 아닌 인스턴스들은 걸러냅니다.(-r 또는 --raw-output 옵션이 없으면 호스트네임 주위에 큰따옴표가 붙게 됩니다.)\n출력은 hostnames에 저장하여 나중에 parallel에 전달할 수 있게 합니다.언급했듯이 parallel은 원격 머신에 접속하기 위해 ssh103를 고용합니다.\n매번 자격 증명을 입력하지 않고 EC2 인스턴스에 접속하고 싶다면, ~/.ssh/config 파일에 다음과 같은 내용을 추가할 수 있습니다.여러분이 어떤 배포판을 실행 중이냐에 따라 사용자 이름은 ubuntu가 아닐 수 있습니다.","code":"aws ec2 describe-instances | jq '.Reservations[].Instances[] | {public_dns: .PublicDnsName, state: .State.Name}'#! enter=FALSE\nC-C#!literal=FALSE\necho '{' &&\necho '  \"state\": \"running\",' &&\necho '  \"public_dns\": \"ec2-54-88-122-140.compute-1.amazonaws.com\"' &&\necho '}' &&\necho '{' &&\necho '  \"state\": \"stopped\",' &&\necho '  \"public_dns\": null' &&\necho '}' &&aws ec2 describe-instances | jq -r '.Reservations[].Instances[] | select(.State.Name==\"running\") | .PublicDnsName' | tee hostnames#! enter=FALSE\nC-C#! literal=FALSE\necho 'ec2-54-88-122-140.compute-1.amazonaws.com' &&\necho 'ec2-54-88-89-208.compute-1.amazonaws.com'bat ~/.ssh/config"},{"path":"chapter-8-parallel-pipelines.html","id":"원격-머신에서-명령어-실행하기","chapter":"제 8 병렬 파이프라인","heading":"8.4.2 원격 머신에서 명령어 실행하기","text":"분산 처리의 첫 번째 방식은 원격 머신에서 일반 명령어를 실행하는 것입니다.\n우선 각 EC2 인스턴스에서 hostname104 도구를 실행하여 parallel이 제대로 작동하는지 확인해 봅시다.여기서 --sshloginfile 또는 --slf 옵션은 hostnames 파일을 참조하는 데 사용됩니다.\n--nonall 옵션은 parallel에게 매개변수를 사용하지 않고 hostnames 파일에 있는 모든 원격 머신에서 동일한 명령어를 한 번씩 실행하라고 지시합니다.\n기억하십시오. 활용할 원격 머신이 없다면 --slf hostnames를 --sshlogin :으로 바꾸어 명령어가 로컬 머신에서 실행되도록 할 수 있습니다.모든 원격 머신에서 동일한 명령어를 한 번씩 실행하는 데는 머신당 하나의 코어만 있으면 됩니다. 만약 parallel에 전달된 인자 목록을 분산시키려 한다면 잠재적으로 하나 이상의 코어를 사용할 수 있습니다. 코어의 수가 명시적으로 지정되지 않았다면, parallel은 이를 스스로 파악하려고 시도할 것입니다.이 경우에는 두 대의 원격 머신 중 한 대에만 parallel이 설치되어 있습니다.\n그중 하나에서 parallel을 찾을 수 없다는 경고 메시지가 뜹니다.\n그 결과, parallel은 코어의 수를 파악할 수 없어 기본값으로 코어 하나만 사용하게 됩니다.\n이런 경고 메시지를 받으면 다음 네 가지 중 하나를 할 수 있습니다.걱정하지 말고, 머신당 코어 하나만 쓰는 것에 만족하기--jobs 또는 -j 옵션을 통해 각 머신에 대한 작업 수 지정하기hostnames 파일에서 각 호스트네임 앞에 예를 들어 2/ 라고 적어 머신당 사용할 코어 수 지정하기 (이 경우 2개의 코어)패키지 관리자를 사용하여 parallel 설치하기. 예를 들어 원격 머신들이 모두 우분투(Ubuntu)를 실행 중이라면 다음과 같습니다.","code":"parallel --nonall --sshloginfile hostnames hostname#! enter=FALSE\nC-C#! literal=FALSE\necho 'ip-172-31-23-204\\nip-172-31-23-205'parallel --nonall --sshlogin : hostname#! enter=FALSE\nC-C#! literal=FALSE\necho 'data-science-toolbox'alias fake=echo\nseq 2 | parallel --slf hostnames echo 2>&1#! enter=FALSE\nC-C#! literal=FALSE\nfake 'bash: parallel: command not found' &&\nfake -n 'parallel: Warning: Could not figure out number of cpus on' &&\nfake ' ec2-54-88-122-140.compute-1.amazonaws.com (). Using 1.' &&\nfake '1' &&\nfake '2'parallel --nonall --slf hostnames \"sudo apt-get install -y parallel\"#! enter=FALSE\nC-C#! literal=FALSE"},{"path":"chapter-8-parallel-pipelines.html","id":"로컬-데이터를-원격-머신들로-분산하기","chapter":"제 8 병렬 파이프라인","heading":"8.4.3 로컬 데이터를 원격 머신들로 분산하기","text":"분산 처리의 두 번째 방식은 로컬 데이터를 직접 원격 머신들로 분산하는 것입니다.\n여러 대의 원격 머신을 사용하여 처리하고 싶은 아주 큰 데이터셋이 하나 있다고 상상해 봅시다.\n간단하게 1부터 1000까지의 모든 정수를 더해 보겠습니다.\n먼저, hostname과 wc를 사용하여 입력이 실제로 분산되고 있는지, 그리고 각 원격 머신이 받은 입력의 길이(줄 수)는 얼마인지 출력해 확인해 보겠습니다.훌륭합니다. 1000개의 숫자가 100개씩 묶여서 (-N100으로 지정) 골고루 분산되는 것을 볼 수 있습니다.\n이제 이 모든 숫자들을 더할 준비가 되었습니다.여기서는 원격 머신들로부터 돌려받은 10개의 합계를 즉시 다시 합산했습니다.\nparallel 없이 계산했을 때와 결과가 같은지 확인해 봅시다.네, 잘 작동합니다.\n원격 머신에서 실행하고 싶은 더 큰 파이프라인이 있다면, 이를 별도의 스크립트에 담아 parallel을 통해 업로드할 수도 있습니다.\nadd라는 아주 간단한 커맨드 라인 도구를 만들어 이를 시연해 보겠습니다.--basefile 옵션을 사용하면 parallel은 작업을 실행하기 전에 add 파일을 모든 원격 머신에 먼저 업로드합니다.1000개의 숫자를 합산하는 것은 물룬 장난감 수준의 예제일 뿐입니다.\n게다가 이를 로컬에서 수행하는 것이 훨씬 더 빨랐을 것입니다.\n그래도 parallel이 얼마나 믿기 힘들 정도로 강력할 수 있는지 이 예제를 통해 명확해졌기를 바랍니다.","code":"seq 1000 | parallel -N100 --pipe --slf hostnames \"(hostname; wc -l) | paste -sd:\"#! enter=FALSE\nC-C#! literal=FALSE\necho 'ip-172-31-23-204:100' &&\necho 'ip-172-31-23-205:100' &&\necho 'ip-172-31-23-205:100' &&\necho 'ip-172-31-23-204:100' &&\necho 'ip-172-31-23-205:100' &&\necho 'ip-172-31-23-204:100' &&\necho 'ip-172-31-23-205:100' &&\necho 'ip-172-31-23-204:100' &&\necho 'ip-172-31-23-205:100' &&\necho 'ip-172-31-23-204:100'seq 1000 | parallel -N100 --pipe --slf hostnames \"paste -sd+ | bc\" | paste -sd+ | bc#! enter=FALSE\nC-C#! literal=FALSE\necho '500500'seq 1000 | paste -sd+ | bcecho '#!/usr/bin/env bash' > add\necho 'paste -sd+ | bc' >> add\nbat add\nchmod u+x add\nseq 1000 | ./addseq 1000 |\nparallel -N100 --basefile add --pipe --slf hostnames './add' |\n./add #! enter=FALSE\nC-C#! literal=FALSE\necho '500500'"},{"path":"chapter-8-parallel-pipelines.html","id":"원격-머신에서-파일-처리하기","chapter":"제 8 병렬 파이프라인","heading":"8.4.4 원격 머신에서 파일 처리하기","text":"분산 처리의 세 번째 방식은 원격 머신으로 파일을 보내고, 처리한 뒤, 결과를 가져오는 것입니다.\n뉴욕시의 각 구(borough)별로 311 서비스 호출이 얼마나 자주 발생하는지 집계하고 싶다고 가정해 봅시다.\n아직 로컬 머신에 해당 데이터가 없으므로, 먼저 무료 NYC Open Data API에서 데이터를 가져오겠습니다.이제 압축된 JSON 데이터를 포함하는 10개의 파일이 생겼습니다.jq -c '.[]'가 JSON 객체 배열을 평탄화(flatten)하여 한 줄에 하나의 객체가 오도록 하며, 파일당 총 100줄이 되도록 한다는 점에 유의하십시오.\nzcat105을 사용하면 압축된 파일의 내용을 직접 출력할 수 있습니다.JSON의 한 줄이 어떻게 생겼는지 확인해 봅시다.로컬 머신에서 각 구별 서비스 호출 총 횟수를 구하려면 다음과 같은 명령어를 실행할 것입니다.➊ zcat을 사용하여 모든 압축 파일을 확장합니다.\n➋ 각 호출에 대해 jq를 사용하여 구의 이름을 추출합니다.\n➌ 구 이름을 소문자로 변환하고 공백을 언더스코어(_)로 바꿉니다 (awk가 기본적으로 공백을 기준으로 분리하기 때문입니다).\n➍ sort와 uniq를 사용하여 각 구의 발생 횟수를 집계합니다.\n➎ 두 열의 순서를 바꾸고 awk를 사용하여 쉼표로 구분합니다.\n➏ header를 사용하여 헤더를 추가합니다.잠시, 여러분의 머신이 너무 느려서 이 파이프라인을 로컬에서 도저히 수행할 수 없다고 상상해 봅시다.\nparallel을 사용하여 로컬 파일을 원격 머신들로 분산하고, 원격 머신에서 처리를 수행한 뒤 그 결과를 가져올 수 있습니다.➊ 파일 목록을 출력하고 이를 parallel로 파이프합니다.\n➋ jq 바이너리를 각 원격 머신으로 전송합니다. 다행히 jq는 의존성이 없습니다. 이 파일은 --trc 옵션(이는 --cleanup 옵션을 포함합니다)을 지정했으므로 나중에 원격 머신에서 제거됩니다. 파이프라인에서 jq 대신 ./jq를 사용하는 점에 유의하십시오. 파이프라인이 검색 경로(search path)에 있을지도 모르는 버전이 아니라 업로드된 버전을 사용해야 하기 때문입니다.\n➌ 커맨드 라인 인자 --trc {.}.csv는 --transfer --return {.}.csv --cleanup의 약어입니다. (치환 문자열 {.}은 입력 파일명에서 마지막 확장자를 제외한 것으로 치환됩니다.) 여기서 이 옵션은 JSON 파일이 원격 머신으로 전송되고, CSV 파일이 로컬 머신으로 반환되며, 각 작업이 끝난 후 원격 머신에서 두 파일이 모두 삭제됨을 의미합니다.\n➍ 호스트네임 목록을 지정합니다. 로컬에서 직접 시도해보고 싶다면 --slf hostnames 대신 --sshlogin :을 지정할 수도 있습니다.\n➎ awk 표현식의 이스케이프 처리에 유의하십시오. 따옴표 처리는 가끔 까다로울 수 있습니다. 여기서는 달러 기호와 큰따옴표가 이스케이프되었습니다. 따옴표 처리가 너무 혼란스러워진다면, add 도구에서 했던 것처럼 파이프라인을 별도의 커맨드 라인 도구로 만드는 것을 기억하십시오.이 과정 중에 원격 머신 중 하나에서 ls를 실행해 본다면, parallel이 실제로 바이너리 jq, JSON 파일, 그리고 CSV 파일을 전송(하고 정리)하는 것을 볼 수 있을 것입니다.각 CSV 파일은 다음과 같이 생겼습니다.rush106와 tidyverse를 사용하여 각 CSV 파일의 합계를 집계할 수 있습니다.또는 결과 집계에 SQL을 사용하고 싶다면, 5장에서 다룬 csvsql을 사용할 수 있습니다.","code":"seq 0 100 900 | parallel  \"curl -sL 'http://data.cityofnewyork.us/resource/erm2-nwe9.json?\\$limit=100&\\$offset={}' | jq -c '.[]' | gzip > nyc-{#}.json.gz\"l nyc*json.gzzcat nyc-1.json.gz | trimzcat nyc-1.json.gz | head -n 1zcat nyc*json.gz |\njq -r '.borough' |\ntr '[A-Z] ' '[a-z]_' |\nsort | uniq -c | sort -nr |\nawk '{print $2\",\"$1}' |\nheader -a borough,count |\ncsvlookls *.json.gz |\nparallel -v --basefile jq \\\n--trc {.}.csv \\\n--slf hostnames \\\n\"zcat {} | ./jq -r '.borough' | tr '[A-Z] ' '[a-z]_' | sort | uniq -c | awk '{print \\$2\\\",\\\"\\$1}' > {.}.csv\"#! enter=FALSE\nC-C#! literal=FALSEssh $(head -n 1 hostnames) ls#! enter=FALSE\nC-C#! literal=FALSE\necho 'nyc-1.json.csv' &&\necho 'nyc-1.json.gz' &&\necho 'jq' &&cat nyc-1.json.csv #! enter=FALSE\nC-C#! literal=FALSE\necho 'bronx,3' &&\necho 'brooklyn,5' &&\necho 'manhattan,24' &&\necho 'queens,3' &&\necho 'staten_island,2'cat nyc*csv | header -a borough,count |\nrush run -t 'group_by(df, borough) %>% summarize(count = sum(count))' - |\ncsvsort -rc count | csvlookcat nyc*csv | header -a borough,count |\ncsvsql --query 'SELECT borough, SUM(count) AS count FROM stdin GROUP BY borough ORDER BY count DESC' |\ncsvlook"},{"path":"chapter-8-parallel-pipelines.html","id":"요약-7","chapter":"제 8 병렬 파이프라인","heading":"8.5 요약","text":"데이터 과학자로서 여러분은 데이터, 때로는 아주 많은 양의 데이터를 다룹니다.\n이는 가끔 명령어를 여러 번 실행하거나 데이터 집약적인 명령어를 여러 코어에 분산해야 함을 의미합니다.\n이번 장에서 저는 명령어를 병렬화하는 것이 얼마나 쉬운지 보여드렸습니다.\nparallel은 일반적인 커맨드 라인 도구의 속도를 높이고 이를 분산하는 매우 강력하고 유연한 도구입니다.\nparallel은 아주 많은 기능을 제공하며 이번 장에서 저는 그 겉핥기 정도만 할 수 있었습니다.\n다음 장에서는 OSEMN 모델의 네 번째 단계인 데이터 모델링을 다루겠습니다.","code":""},{"path":"chapter-8-parallel-pipelines.html","id":"더-읽어보기-3","chapter":"제 8 병렬 파이프라인","heading":"8.6 더 읽어보기","text":"parallel과 그 주요 옵션들에 대해 기본적인 이해를 마쳤다면, 온라인 튜토리얼을 살펴보실 것을 권장합니다. 입력을 지정하는 다양한 방법, 모든 작업의 로그 유지, 타임아웃, 재개(resume), 그리고 작업 재시도(retry) 방법 등을 배울 수 있습니다. parallel의 제작자인 Ole Tange는 이 튜토리얼에서 이렇게 말합니다. “여러분의 커맨드 라인이 당신을 아주 좋아하게 될 것입니다(command line love ).”","code":""},{"path":"chapter-9-modeling-data.html","id":"chapter-9-modeling-data","chapter":"제 9 데이터 모델링","heading":"제 9 데이터 모델링","text":"이 장에서는 OSEMN 모델의 네 번째 단계인 데이터 모델링을 수행할 것입니다.\n일반적으로 모델은 여러분이 가진 데이터에 대한 추상적이거나 상위 수준의 설명(description)입니다.\n모델링은 개별 데이터 포인트에서 한 걸음 물러나 큰 그림을 본다는 점에서 시각화를 만드는 것과 약간 비슷합니다.시각화는 모양, 위치, 색상으로 특징지어집니다. 우리는 그것들을 직접 눈으로 보고 해석할 수 있습니다.\n반면 모델은 내부적으로 숫자로 특징지어지며, 이는 컴퓨터가 새로운 데이터 포인트에 대한 예측을 수행하는 등의 작업에 모델을 사용할 수 있음을 의미합니다.\n(모델이 어떻게 작동하고 성능이 어떠한지 이해하기 위해 여전히 모델을 시각화할 수 있습니다.)이 장에서 저는 데이터를 모델링하는 데 흔히 사용되는 세 가지 유형의 알고리즘을 살펴볼 것입니다.차원 축소(Dimensionality reduction)회귀(Regression)분류(Classification)이 알고리즘들은 통계학과 머신러닝 분야에서 온 것이므로, 용어를 조금 바꾸어 사용하겠습니다.\n우리가 데이터셋이라고도 불리는 CSV 파일을 가지고 있다고 가정해 봅시다.\n헤더를 제외한 각 행은 하나의 데이터 포인트로 간주됩니다.\n각 데이터 포인트는 하나 이상의 특성(features) 또는 측정된 속성들을 가집니다.\n때로는 데이터 포인트가 레이블(label)을 가지기도 하는데, 이는 일반적으로 판단 결과나 성과를 의미합니다.\n아래에서 와인 데이터셋을 소개할 때 이 개념이 더 구체화될 것입니다.첫 번째 유형의 알고리즘(차원 축소)은 대부분 비지도 학습(unsupervised)입니다. 즉, 데이터셋의 특성만을 기반으로 모델을 생성합니다.\n마지막 두 유형의 알고리즘(회귀 및 분류)은 정의상 지도 학습(supervised) 알고리즘이며, 이는 모델에 레이블 정보도 포함시킨다는 의미입니다.","code":""},{"path":"chapter-9-modeling-data.html","id":"개요-6","chapter":"제 9 데이터 모델링","heading":"9.1 개요","text":"이 장에서 여러분은 다음 방법을 배우게 됩니다.tapkee107를 사용하여 데이터셋의 차원 축소하기vw108를 사용하여 화이트 와인의 품질 예측하기skll109을 사용하여 와인을 레드 또는 화이트로 분류하기이 장은 다음 파일로 시작합니다.이 파일들을 가져오는 방법은 2장에 설명되어 있습니다.\n그 외의 파일들은 커맨드 라인 도구를 사용하여 다운로드하거나 생성한 것들입니다.","code":"cd /data/ch09\nl"},{"path":"chapter-9-modeling-data.html","id":"와인-좀-더-주세요","chapter":"제 9 데이터 모델링","heading":"9.2 와인 좀 더 주세요!","text":"이 장 전반에 걸쳐 ’비뉴 베르드(vinho verde)’라고 불리는 포르투갈 와인의 레드 및 화이트 품종에 대한 와인 감별사들의 기록 데이터셋을 사용할 것입니다.\n각 데이터 포인트는 하나의 와인을 나타냅니다. 각 와인은 11가지 물리화학적 특성에 대해 등급이 매겨져 있습니다: (1) 고정 산도(fixed acidity), (2) 휘발성 산도(volatile acidity), (3) 구연산(citric acid), (4) 잔류 당분(residual sugar), (5) 염화물(chlorides), (6) 유리 이산화황(free sulfur dioxide), (7) 총 이산화황(total sulfur dioxide), (8) 밀도(density), (9) pH, (10) 황산염(sulphates), (11) 알코올(alcohol).\n또한 0(매우 나쁨)에서 10(매우 우수) 사이의 종합 품질 점수가 있는데, 이는 와인 전문가들에 의한 최소 세 번의 평가 결과에 대한 중앙값입니다. 이 데이터셋에 대한 더 자세한 정보는 UCI 머신러닝 저장소에서 확인할 수 있습니다.데이터셋은 화이트 와인용과 레드 와인용 두 개의 파일로 나뉘어 있습니다.\n가장 먼저 할 일은 curl을 사용하여 두 파일을 가져오는 것입니다 (물론 시간이 아까우니 parallel을 사용합니다).세 개의 콜론(:::)은 parallel에 데이터를 전달하는 또 다른 방법입니다.두 파일을 검사하고 줄 수를 세어봅시다.➊ 명확성을 위해 nl을 사용하여 줄 번호를 추가했습니다.\n➋ 헤더 전체를 보기 위해 fold를 사용했습니다.언뜻 보기에 이 데이터는 꽤 깨끗해 보입니다.\n그래도 대부분의 커맨드 라인 도구가 기대하는 형식에 더 잘 부합하도록 정제(scrub)해 보겠습니다.\n구체적으로 다음 작업을 할 것입니다.헤더를 소문자로 변환합니다.세미콜론(;)을 쉼표(,)로 바꿉니다.공백을 언더스코어(_)로 바꿉니다.불필요한 따옴표를 제거합니다.tr 도구를 사용하면 이 모든 작업을 처리할 수 있습니다.\n이번에는 옛 추억을 되살려 루프를 사용하여 두 파일을 처리해 보겠습니다.또한 두 파일을 결합하여 단일 데이터셋을 만들어 봅시다.\ncsvstack110을 사용하여 type이라는 열을 추가할 것입니다. 이 열은 첫 번째 파일의 행에는 “red”, 두 번째 파일의 행에는 “white”라는 값을 갖게 됩니다.➊ 새 열 type은 csvstack에 의해 맨 앞에 배치됩니다.\n➋ 일부 알고리즘은 레이블이 마지막 열에 있다고 가정하므로, xsv를 사용하여 type 열을 맨 뒤로 옮깁니다.대부분의 머신러닝 알고리즘은 결측값(missing values)을 처리하지 못하므로, 이 데이터셋에 결측값이 있는지 확인하는 것이 좋습니다.훌륭합니다!\n만약 결측값이 있었다면 해당 특성의 평균값이나 가장 빈번한 값 등으로 채워 넣을 수 있었을 것입니다.\n대안적으로, 결측값이 하나라도 있는 데이터 포인트를 아예 제거하는 덜 정교한 접근 방식도 있습니다.\n그냥 호기심에, 레드 와인과 화이트 와인의 품질 분포가 어떻게 다른지 살펴봅시다.밀도 그래프를 통해 화이트 와인의 품질이 더 높은 값 쪽으로 분포되어 있음을 알 수 있습니다.\n이것이 화이트 와인이 전반적으로 레드 와인보다 더 좋다는 뜻일까요, 아니면 화이트 와인 전문가들이 레드 와인 전문가들보다 더 쉽게 높은 점수를 준다는 뜻일까요?\n그것은 데이터가 우리에게 말해주지 않는 부분입니다.\n혹시 알코올 농도와 품질 사이에 관계가 있을까요?\nrush를 사용하여 알아봅시다.유레카! 흠흠, 이제 모델링을 계속해 볼까요?","code":"parallel \"curl -sL http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-{}.csv > wine-{}.csv\" ::: red white#!enter=FALSE\nC-C#!literal=FALSEcp /data/.cache/wine-*.csv .< wine-red.csv nl |\nfold |\ntrim\n< wine-white.csv nl | fold | trim\nwc -l wine-{red,white}.csvfor COLOR in red white; do\n< wine-$COLOR.csv tr '[A-Z]; ' '[a-z],_' | tr -d \\\" > wine-${COLOR}-clean.csv\ndonecsvstack -g red,white -n type wine-{red,white}-clean.csv |\nxsv select 2-,1 > wine.csvcsvstat wine.csv --nullsrush run -t 'ggplot(df, aes(x = quality, fill = type)) + geom_density(adjust = 3, alpha = 0.5)' wine.csv > wine-quality.png\ndisplay wine-quality.pngrush plot --x alcohol --y quality --color type --geom smooth wine.csv > wine-alcohol-vs-quality.png\ndisplay wine-alcohol-vs-quality.png"},{"path":"chapter-9-modeling-data.html","id":"tapkee를-사용한-차원-축소","chapter":"제 9 데이터 모델링","heading":"9.3 Tapkee를 사용한 차원 축소","text":"차원 축소의 목표는 고차원의 데이터 포인트를 저차원의 매핑으로 옮기는 것입니다.\n핵심 과제는 유사한 데이터 포인트들이 저차원 매핑에서도 서로 가깝게 유지되도록 하는 것입니다.\n이전 섹션에서 보았듯이, 우리의 와인 데이터셋은 13개의 특성을 포함하고 있습니다.\n시각화하기에 가장 직관적인 두 개의 차원으로 축소를 진행해 보겠습니다.차원 축소는 종종 탐색적 데이터 분석(EDA)의 일부로 간주됩니다.\n플로팅하기에 특성이 너무 많을 때 유용합니다.\n산점도 행렬(scatter-plot matrix)을 만들 수도 있지만, 이는 한 번에 두 개의 특성만 보여줄 수 있습니다.\n또한 다른 머신러닝 알고리즘을 위한 전처리 단계로도 유용합니다.대부분의 차원 축소 알고리즘은 비지도 학습입니다.\n이는 데이터 포인트의 레이블을 사용하지 않고 저차원 매핑을 구축한다는 의미입니다.이 섹션에서는 PCA(주성분 분석, Principal Components Analysis111)와 t-SNE(t-분포 확률적 임베딩, t-distributed Stochastic Neighbor Embedding112)라는 두 가지 기술을 살펴보겠습니다.","code":""},{"path":"chapter-9-modeling-data.html","id":"tapkee-소개","chapter":"제 9 데이터 모델링","heading":"9.3.1 Tapkee 소개","text":"Tapkee는 차원 축소를 위한 C++ 템플릿 라이브러리입니다113.\n이 라이브러리에는 다음을 포함한 많은 차원 축소 알고리즘이 구현되어 있습니다.국소 선형 임베딩(Locally Linear Embedding)Isomap다차원 척도법(Multidimensional Scaling)PCAt-SNE이 알고리즘들에 대한 자세한 정보는 Tapkee 웹사이트에서 찾을 수 있습니다.\nTapkee는 주로 다른 애플리케이션에 포함될 수 있는 라이브러리지만, tapkee라는 커맨드 라인 도구도 제공합니다.\n이를 사용하여 와인 데이터셋의 차원 축소를 수행해 보겠습니다.","code":""},{"path":"chapter-9-modeling-data.html","id":"선형-및-비선형-매핑","chapter":"제 9 데이터 모델링","heading":"9.3.2 선형 및 비선형 매핑","text":"먼저, 각 특성이 동일하게 중요하게 취급되도록 표준화(standardization)를 사용하여 특성의 스케일을 조정하겠습니다.\n일반적으로 머신러닝 알고리즘을 적용할 때 스케일 조정을 하면 더 나은 결과를 얻을 수 있습니다.스케일 조정을 위해 rush와 tidyverse 패키지를 사용합니다.➊ scale() 함수는 수치형 열에만 작동하므로 type 열을 임시로 제거해야 합니다.\n➋ scale() 함수는 데이터 프레임을 받지만 행렬을 반환합니다.\n➌ as_tibble() 함수는 행렬을 다시 데이터 프레임으로 변환합니다.\n➍ 마지막으로 type 열을 다시 추가합니다.이제 두 가지 차원 축소 기술을 적용하고 Rio-scatter를 사용하여 매핑을 시각화해 보겠습니다.➊ type 열을 제외합니다.\n➋ 헤더를 제거합니다.\n➌ PCA를 적용합니다.➊ pc1과 pc2 열이 있는 헤더를 다시 추가합니다.\n➋ type 열을 다시 추가합니다.이제 산점도를 생성할 수 있습니다.동일한 방식으로 t-SNE를 수행해 보겠습니다.➊ type 열을 제외합니다.\n➋ 헤더를 제거합니다.\n➌ t-SNE를 적용합니다.\n➍ x와 y 열이 있는 헤더를 다시 추가합니다.\n➎ type 열을 다시 추가합니다.\n➏ 산점도를 생성합니다.t-SNE가 PCA보다 물리화학적 특성을 기반으로 레드 와인과 화이트 와인을 더 잘 분리하는 것을 볼 수 있습니다.\n이 산점도들은 데이터셋이 특정한 구조를 가지고 있음을 확인시켜 줍니다. 즉, 특성과 레이블 사이에 관계가 있습니다.\n이를 확인했으니 이제 안심하고 지도 학습(supervised machine learning)을 적용해 보겠습니다.\n먼저 회귀 작업을 시작하고 이어서 분류 작업을 진행하겠습니다.","code":"rush run --tidyverse --output wine-scaled.csv \\\n'select(df, -type) %>%\nscale() %>%\nas_tibble() %>%\nmutate(type = df$type)' wine.csv\ncsvlook wine-scaled.csvxsv select '!type' wine-scaled.csv |\nheader -d |\ntapkee --method pca |\ntee wine-pca.txt | trim< wine-pca.txt header -a pc1,pc2 |\npaste -d, - <(xsv select type wine-scaled.csv) |\ntee wine-pca.csv | csvlookrush plot --x pc1 --y pc2 --color type --shape type wine-pca.csv > wine-pca.png\ndisplay wine-pca.pngxsv select '!type' wine-scaled.csv |\nheader -d |\ntapkee --method t-sne |\nheader -a x,y |\npaste -d, - <(xsv select type wine-scaled.csv) |\nrush plot --x x --y y --color type --shape type > wine-tsne.pngdisplay wine-tsne.png"},{"path":"chapter-9-modeling-data.html","id":"vowpal-wabbit을-사용한-회귀","chapter":"제 9 데이터 모델링","heading":"9.4 Vowpal Wabbit을 사용한 회귀","text":"이 섹션에서는 와인의 물리화학적 특성을 기반으로 화이트 와인의 품질을 예측하는 모델을 만들 것입니다.\n품질은 0에서 10 사이의 숫자이므로, 이를 회귀(regression) 작업으로 간주할 수 있습니다.이를 위해 Vowpal Wabbit, 줄여서 vw를 사용할 것입니다.","code":""},{"path":"chapter-9-modeling-data.html","id":"데이터-준비하기","chapter":"제 9 데이터 모델링","heading":"9.4.1 데이터 준비하기","text":"CSV 파일로 작업하는 대신, vw는 자체적인 데이터 형식을 사용합니다.\ncsv2vw114 도구는 이름에서 알 수 있듯이 CSV를 이 형식으로 변환할 수 있습니다.\n--label 옵션은 어떤 열이 레이블을 포함하고 있는지 지정하는 데 사용됩니다.\n결과를 살펴봅시다.이 형식에서 각 줄은 하나의 데이터 포인트입니다.\n줄은 레이블로 시작하고, 그 뒤에 파이프 기호(|)가 오며, 그 다음에는 공백으로 구분된 특성 이름/값 쌍들이 옵니다.\nCSV 형식과 비교했을 때 이 형식이 지나치게 장황해 보일 수도 있지만, 가중치(weights), 태그(tags), 네임스페이스(namespaces), 그리고 희소 특성 표현(sparse feature representation)과 같은 더 많은 유연함을 제공합니다.\n와인 데이터셋에서는 이러한 유연성이 필요하지 않지만, 더 복잡한 문제에 vw를 적용할 때는 유용할 수 있습니다.\n이 기사에서 vw 형식에 대해 더 자세히 설명하고 있습니다.회귀 모델을 만들거나 훈련(train)하고 나면, 이를 사용하여 아직 보지 못한 새로운 데이터 포인트에 대해 예측을 수행할 수 있습니다.\n즉, 모델이 이전에 본 적 없는 와인을 제공하면 그 품질을 예측하거나 테스트(test)할 수 있습니다.\n이러한 예측의 정확도를 적절하게 평가하려면, 훈련에 사용하지 않을 데이터를 일부 따로 떼어놓아야 합니다.\n보통 전체 데이터셋의 80%를 훈련용으로 사용하고 나머지 20%를 테스트용으로 사용합니다.먼저 split115을 사용하여 전체 데이터셋을 동일한 다섯 부분으로 나누어 이 작업을 수행할 수 있습니다.\nwc를 사용하여 각 부분의 데이터 포인트 수를 확인합니다.➊ shuf116 도구는 데이터셋을 무작위로 섞어서 훈련 세트와 테스트 세트가 유사한 품질 분포를 갖도록 보장합니다.이제 첫 번째 부분(즉, 20%)을 테스트 세트인 wine-test.vw로 사용하고, 나머지 네 부분(즉, 80%)을 합쳐서 훈련 세트인 wine-train.vw를 만듭니다.이제 vw를 사용하여 모델을 훈련할 준비가 되었습니다.","code":"csv2vw wine-white-clean.csv --label quality | trimcsv2vw wine-white-clean.csv --label quality |\nshuf |\nsplit -d -n r/5 - wine-part-\nwc -l wine-part-*mv wine-part-00 wine-test.vw\ncat wine-part-* > wine-train.vw\nrm wine-part-*\nwc -l wine-*.vw"},{"path":"chapter-9-modeling-data.html","id":"모델-훈련하기","chapter":"제 9 데이터 모델링","heading":"9.4.2 모델 훈련하기","text":"vw 도구는 매우 많은(거의 400개에 달하는!) 옵션을 제공합니다.\n다행히 효과적으로 사용하기 위해 그 모든 옵션이 필요한 것은 아닙니다.\n여기서 사용하는 옵션들을 설명하기 위해 각 옵션을 별도의 줄에 적어보겠습니다.➊ wine-train.vw 파일이 모델 훈련에 사용됩니다.\n➋ 모델 또는 회귀분석기(regressor)는 wine.model 파일에 저장됩니다.\n➌ 훈련 패스(passes)의 횟수입니다.\n➍ 여러 번의 패스를 수행할 때 캐싱이 필요합니다.\n➎ 3개의 은닉 유닛(hidden units)을 가진 신경망을 사용합니다.\n➏ 모든 입력 특성을 기반으로 이차(quadratic) 특성을 생성하여 사용합니다. 중복되는 특성은 vw에 의해 제거됩니다.\n➐ L2 규제(regularization)를 사용합니다.\n➑ 특성을 저장하는 데 25비트를 사용합니다.이제 회귀 모델을 훈련시켰으니, 이를 사용하여 예측을 해보겠습니다.","code":"vw \\\n--data wine-train.vw \\\n--final_regressor wine.model \\\n--passes 10 \\\n--cache_file wine.cache \\\n--nn 3 \\\n--quadratic :: \\\n--l2 0.000005 \\\n--bit_precision 25"},{"path":"chapter-9-modeling-data.html","id":"모델-테스트하기","chapter":"제 9 데이터 모델링","heading":"9.4.3 모델 테스트하기","text":"모델은 wine.model 파일에 저장되어 있습니다.\n이 모델을 사용하여 예측을 수행하기 위해, 이번에는 다른 옵션들을 사용하여 vw를 다시 실행합니다.➊ wine-test.vw 파일이 모델 테스트에 사용됩니다.\n➋ wine.model 파일에 저장된 모델을 사용합니다.\n➌ 레이블 정보를 무시하고 테스트만 수행합니다.\n➍ 예측 결과는 predictions라는 파일에 저장됩니다.\n➎ 진단 메시지와 진행 상황 업데이트를 출력하지 않습니다.paste를 사용하여 predictions 파일에 담긴 예측값과 wine-test.vw 파일에 담긴 실제값(관측값, observed values)을 결합해 봅시다.\nawk를 사용하여 예측값과 관측값을 비교하고 평균 절대 오차(MAE, Mean Absolute Error)를 계산할 수 있습니다.\nMAE는 화이트 와인의 품질을 예측할 때 vw가 평균적으로 얼마나 차이가 나는지 알려줍니다.예측값은 평균적으로 약 0.6점 정도 차이가 납니다.\nrush plot을 사용하여 관측값과 예측값 사이의 관계를 시각화해 봅시다.모델 훈련에 사용된 옵션들이 조금 복잡해 보일 수 있습니다.\n모든 기본값(default values)을 사용했을 때 vw의 성능이 어떠한지 확인해 봅시다.➊ 회귀 모델 훈련\n➋ 회귀 모델 테스트\n➌ 평균 절대 오차(MAE) 계산기본값을 사용했을 때 MAE가 0.04 더 높게 나타났습니다. 이는 예측 결과가 약간 더 나빠졌음을 의미합니다.이 섹션에서는 vw가 할 수 있는 일의 극히 일부만을 다루었습니다.\n이 도구에 그렇게 많은 옵션이 있는 데는 이유가 있습니다.\n회귀 외에도 이진 분류(binary classification), 다중 클래스 분류(multi-class classification), 강화 학습(reinforcement learning), 그리고 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA) 등을 지원합니다.\n웹사이트에서 더 많은 튜토리얼과 기사를 찾아볼 수 있습니다.","code":"vw \\\n--data wine-test.vw \\\n--initial_regressor wine.model \\\n--testonly \\\n--predictions predictions \\\n--quiet\nbat predictions | trimpaste -d, predictions <(cut -d '|' -f 1 wine-test.vw) |\ntee results.csv |\nawk -F, '{E+=sqrt(($1-$2)^2)} END {print \"MAE: \" E/NR}' |\ncowsay< results.csv header -a \"predicted,observed\" |\nrush plot --x observed --y predicted --geom jitter > wine-regression.png\ndisplay wine-regression.pngvw -d wine-train.vw -f wine2.model --quiet\nvw -data wine-test.vw -i wine2.model -t -p predictions --quiet\npaste -d, predictions <(cut -d '|' -f 1 wine-test.vw) |\nawk -F, '{E+=sqrt(($1-$2)^2)} END {print \"MAE: \" E/NR}'"},{"path":"chapter-9-modeling-data.html","id":"scikit-learn-laboratory를-사용한-분류","chapter":"제 9 데이터 모델링","heading":"9.5 SciKit-Learn Laboratory를 사용한 분류","text":"이 섹션에서는 와인이 레드인지 화이트인지를 예측하는 분류 모델(분류기, classifier)을 훈련시킬 것입니다.\n이 작업에도 vw를 사용할 수 있지만, 다른 도구인 SciKit-Learn Laboratory(SKLL)를 소개해 드리고 싶습니다.\n이름에서 알 수 있듯이, 이 도구는 파이썬에서 널리 쓰이는 머신러닝 패키지인 SciKit-Learn을 기반으로 구축되었습니다.\nSKLL 자체도 파이썬 패키지이며, 커맨드 라인에서 SciKit-Learn을 사용할 수 있게 해주는 run_experiment 도구를 제공합니다.\n저는 패키지 이름과 일치하여 기억하기 쉬운 skll이라는 별칭(alias)을 사용하여 run_experiment를 호출하겠습니다.","code":"alias skll=run_experiment\nskll"},{"path":"chapter-9-modeling-data.html","id":"데이터-준비하기-1","chapter":"제 9 데이터 모델링","heading":"9.5.1 데이터 준비하기","text":"skll은 훈련용 데이터셋과 테스트용 데이터셋이 서로 다른 디렉터리에 있되 같은 파일 이름을 가질 것을 기대합니다.\n예측치(predictions)가 반드시 원래 데이터셋과 같은 순서로 나오지 않으므로, 예측치를 올바른 데이터 포인트와 일치시킬 수 있도록 고유 식별자를 포함하는 id 열을 추가하겠습니다.\n균형 잡힌(balanced) 데이터셋을 만들어 봅시다.➊ 레드 와인의 수를 NUM_RED 변수에 저장합니다.\n➋ 모든 레드 와인과 무작위로 샘플링된 동일한 수의 화이트 와인을 결합합니다.\n➌ nl을 사용하여 각 줄 앞에 “줄 번호”를 추가합니다.\n➍ 적절한 열 이름이 되도록 첫 번째 줄의 “0”을 “id”로 바꿉니다.이제 이 균형 잡힌 데이터셋을 훈련 세트와 테스트 세트로 나눕니다.이제 균형 잡힌 훈련 세트와 테스트 세트가 준비되었으니, 분류기를 만드는 단계를 진행할 수 있습니다.","code":"NUM_RED=\"$(< wine-red-clean.csv wc -l)\"\ncsvstack -n type -g red,white \\\nwine-red-clean.csv \\\n<(< wine-white-clean.csv body shuf | head -n $NUM_RED) |\nbody shuf |\nnl -s, -w1 -v0 |\nsed '1s/0,/id,/' |\ntee wine-balanced.csv | csvlookmkdir -p {train,test}\nHEADER=\"$(< wine-balanced.csv header)\"\n< wine-balanced.csv header -d | shuf | split -d -n r/5 - wine-part-\nwc -l wine-part-*\ncat wine-part-00 | header -a $HEADER > test/features.csv && rm wine-part-00\ncat wine-part-* | header -a $HEADER > train/features.csv && rm wine-part-*\nwc -l t*/features.csv"},{"path":"chapter-9-modeling-data.html","id":"실험-실행하기","chapter":"제 9 데이터 모델링","heading":"9.5.2 실험 실행하기","text":"skll에서 분류기를 훈련시키려면 설정 파일(configuration file)에 실험을 정의해야 합니다.\n이 정의에는 데이터셋을 어디서 찾을지, 어떤 분류기를 사용할지 등을 명시하는 여러 섹션이 포함됩니다.\n여기서 사용할 설정 파일 classify.cfg는 다음과 같습니다.skll을 사용하여 실험을 실행합니다.-l 옵션은 로컬 모드에서 실행할 것을 지정합니다.\nskll은 클러스터에서 실험을 실행하는 기능도 제공합니다.\n실험이 완료되는 데 걸리는 시간은 선택한 알고리즘의 복잡도와 데이터의 크기에 따라 달라집니다.","code":"bat classify.cfgskll -l classify.cfg 2>/dev/null"},{"path":"chapter-9-modeling-data.html","id":"결과-분석하기","chapter":"제 9 데이터 모델링","heading":"9.5.3 결과 분석하기","text":"모든 분류기의 훈련과 테스트가 완료되면, 결과는 output 디렉터리에서 확인할 수 있습니다.skll은 각 분류기에 대해 네 개의 파일을 생성합니다: 로그 파일 한 개, 결과 파일 두 개, 그리고 예측값 파일 한 개입니다.\n다음 SQL 쿼리를 사용하여 알고리즘 이름과 등급별 정확도를 추출해 보겠습니다.여기서 가장 중요한 열은 정확도를 나타내는 accuracy로, 이는 올바르게 분류된 데이터 포인트의 비율을 의미합니다.\n이를 통해 실제로 모든 알고리즘이 매우 좋은 성능을 보이고 있음을 알 수 있습니다.\nRandomForestClassifier가 가장 좋은 성능을 보였고, 그 뒤를 KNeighborsClassifier가 근소한 차이로 따르고 있습니다.각 JSON 파일에는 혼동 행렬(confusion matrix)이 포함되어 있어 각 분류기의 성능에 대한 추가적인 통찰을 제공합니다.\n혼동 행렬은 열이 실제 레이블(레드와 화이트)을 나타내고 행이 예측된 레이블을 나타내는 표입니다.\n대각선 상의 숫자가 높을수록 정확한 예측이 많음을 의미합니다.\njq를 사용하여 각 분류기의 이름과 해당 혼동 행렬을 인쇄할 수 있습니다.혼동 행렬은 특히 분류해야 할 클래스가 세 개 이상일 때 어떤 종류의 오분류가 발생하는지 확인하거나, 각 클래스별로 오분류 비용이 다를 때 특히 도움이 됩니다.사용 관점에서 볼 때, vw와 skll이 서로 다른 두 가지 접근 방식을 취한다는 점이 흥미롭습니다.\nvw는 커맨드 라인 옵션을 사용하는 반면, skll은 별도의 설정 파일이 필요합니다.\n두 방식 모두 장단점이 있습니다.\n커맨드 라인 옵션은 더 즉흥적인 사용을 가능하게 하지만, 설정 파일은 아마도 재현(reproduce)하기가 더 쉽습니다.\n또한 이미 보았듯이, 수많은 옵션과 함께 vw를 호출하는 방식은 스크립트나 Makefile에 쉽게 넣을 수 있습니다.\n반대로, skll이 설정 파일 없이 옵션을 받도록 만드는 것은 비교적 덜 명확합니다.","code":"ls -1 output< output/wine_summary.tsv csvsql --query \"SELECT learner_name, accuracy FROM stdin ORDER BY accuracy DESC\" | csvlook -Ijq -r '.[] | \"\\(.learner_name):\\n\\(.result_table)\\n\"' output/*.json"},{"path":"chapter-9-modeling-data.html","id":"요약-8","chapter":"제 9 데이터 모델링","heading":"9.6 요약","text":"이 장에서는 데이터 모델링에 대해 살펴보았습니다.\n예제를 통해 비지도 학습인 차원 축소, 그리고 지도 학습인 회귀와 분류라는 세 가지 서로 다른 머신러닝 작업에 대해 깊이 파고들어 보았습니다.\n아쉽게도 본격적인 머신러닝 튜토리얼은 이 책의 범위를 벗어납니다.\n머신러닝에 대해 더 배우고 싶은 분들을 위해 다음 섹션에서 몇 권의 책을 추천해 드립니다.\n이것으로 이 책에서 다루는 데이터 과학 OSEMN 모델의 네 번째이자 마지막 단계를 마쳤습니다.\n다음 장은 마지막 중간 막(intermezzo) 장으로, 커맨드 라인을 다른 곳에서 활용하는 방법을 다룰 것입니다.","code":""},{"path":"chapter-9-modeling-data.html","id":"더-읽어보기-4","chapter":"제 9 데이터 모델링","heading":"9.7 더 읽어보기","text":"Sebastian Raschka와 Vahid Mirjalili의 저서 Python Machine Learning은 머신러닝에 대한 포괄적인 개요와 파이썬을 사용한 적용 방법을 제공합니다.Jared Lander의 R Everyone의 후반부 장들에서 R을 사용하여 다양한 머신러닝 작업을 수행하는 방법을 설명합니다.머신러닝에 대해 더 깊이 이해하고 싶다면, Christopher Bishop의 Pattern Recognition Machine Learning과 David MacKay의 Information Theory, Inference, Learning Algorithms를 강력히 추천합니다.t-SNE 알고리즘에 대해 더 자세히 알고 싶다면, Laurens van der Maaten과 Geoffrey Hinton의 원저 논문인 Visualizing Data Using T-SNE를 추천합니다.","code":""},{"path":"chapter-10-polyglot-data-science.html","id":"chapter-10-polyglot-data-science","chapter":"제 10 다국어 데이터 과학","heading":"제 10 다국어 데이터 과학","text":"다국어 사용자(polyglot)란 여러 언어를 말하는 사람을 뜻합니다.\n제가 생각하는 다국어 데이터 과학자는 데이터를 수집, 정제, 탐색, 모델링하기 위해 여러 프로그래밍 언어, 도구 및 기술을 사용하는 사람입니다.커맨드 라인은 이러한 다국어 접근 방식을 장려합니다.\n커맨드 라인은 도구가 유닉스 철학을 따르기만 한다면 해당 도구가 어떤 프로그래밍 언어로 작성되었는지 상관하지 않습니다.\n우리는 4장에서 Bash, Python, R로 커맨드 라인 도구를 만들며 이를 명확히 확인했습니다.\n게다가 우리는 CSV 파일에서 직접 SQL 쿼리를 실행했고 커맨드 라인에서 R 표현식을 실행하기도 했습니다.\n요약하자면, 우리는 이미 충분히 인식하지 못한 채로 다국어 데이터 과학을 수행해 온 것입니다!이 장에서는 이를 뒤집어서 한 걸음 더 나아가 보겠습니다.\n다양한 프로그래밍 언어와 환경에서 커맨드 라인을 활용하는 방법을 보여드리겠습니다.\n솔직히 말해서, 우리는 데이터 과학 커리어 전체를 커맨드 라인에서만 보내지는 않을 것이기 때문입니다.\n저의 경우, 데이터를 분석할 때는 종종 RStudio IDE를 사용하고 무언가를 구현할 때는 파이썬을 자주 사용합니다.\n저는 작업을 완수하는 데 도움이 되는 것이라면 무엇이든 사용합니다.다른 애플리케이션으로 전환하지 않고도 커맨드 라인을 항상 손이 닿는 곳에 둘 수 있다는 사실은 큰 위안이 됩니다.\n별도의 애플리케이션으로 옮겨가서 워크플로우를 끊지 않고도 빠르게 명령어를 실행할 수 있게 해줍니다.\ncurl로 파일을 다운로드하거나, head로 데이터를 검사하거나, git으로 백업을 만들거나, make로 웹사이트를 컴파일하는 등의 작업을 예로 들 수 있습니다.\n일반적으로 평소라면 많은 코드가 필요하거나 커맨드 라 없이는 아예 불가능한 작업들입니다.","code":""},{"path":"chapter-10-polyglot-data-science.html","id":"개요-7","chapter":"제 10 다국어 데이터 과학","heading":"10.1 개요","text":"이 장에서 여러분은 다음 방법을 배우게 됩니다.JupyterLab 및 RStudio IDE 내에서 터미널 실행하기Python 및 R에서 임의의 커맨드 라인 도구와 상호작용하기Apache Spark에서 쉘 명령어를 사용하여 데이터 변환하기이 장은 다음 파일들로 시작합니다.이 파일들을 가져오는 방법은 2장에 설명되어 있습니다.\n그 외의 파일들은 커맨드 라인 도구를 사용하여 다운로드하거나 생성한 것들입니다.","code":"cd /data/ch10\nl"},{"path":"chapter-10-polyglot-data-science.html","id":"jupyter","chapter":"제 10 다국어 데이터 과학","heading":"10.2 Jupyter","text":"프로젝트 Jupyter는 2014년 IPython 프로젝트에서 탄생한 오픈소스 프로젝트로, 모든 프로그래밍 언어에 걸쳐 대화형 데이터 과학 및 과학적 컴퓨팅을 지원하도록 발전해 왔습니다.\nJupyter는 Python, R, Julia, Scala를 포함하여 40개 이상의 프로그래밍 언어를 지원합니다.\n이 섹션에서는 파이썬에 집중하겠습니다.이 프로젝트에는 JupyterLab, Jupyter Notebook, Jupyter Console이 포함됩니다.\n파이썬을 대화형으로 다루는 가장 기본적인 방식인 Jupyter Console부터 시작하겠습니다.\n다음은 커맨드 라인을 활용하는 몇 가지 방법을 보여주는 Jupyter Console 세션입니다.➊ date나 파이썬 패키지 설치를 위한 pip와 같은 임의의 쉘 명령어 및 파이프라인을 실행할 수 있습니다.\n➋ alice.txt의 줄 수를 세기 위한 이 파이썬 코드 줄과 그 아래의 wc 호출을 비교해 보십시오.\n➌ 표준 출력은 문자열 리스트로 반환되므로, total_lines 값을 사용하려면 첫 번째 항목을 가져와서 정수형으로 캐스팅해야 합니다.\n➍ 파일을 다운로드하기 위한 이 셀과 다음 셀을 그 아래의 curl 호출과 비교해 보십시오.\n➎ 중괄호를 사용하여 파이썬 변수를 쉘 명령어의 일부로 사용할 수 있습니다.\n➏ 중괄호 문자를 그대로 사용하고 싶다면 두 번 입력하십시오.\n➐ 파이썬 변수를 표준 입력으로 사용하는 것도 가능하지만, 보시다시피 상당히 까다롭습니다.Jupyter Notebook은 본질적으로 Jupyter Console의 브라우저 기반 버전입니다.\n느낌표(!)와 bash 매직(magic)을 포함하여 커맨드 라인을 활용하는 동일한 방식들을 지원합니다.\n가장 큰 차이점은 노트북에 코드뿐만 아니라 마크업 텍스트, 수식, 데이터 시각화도 포함할 수 있다는 것입니다.\n이러한 이유로 데이터 과학자들 사이에서 매우 인기가 있습니다.\nJupyter Notebook은 별도의 프로젝트이자 환경이지만, 저는 더 완벽한 IDE 기능을 제공하는 JupyterLab을 사용하여 노트북을 작업하는 것을 선호합니다.그림 Figure ??은 JupyterLab의 스크린샷으로, 파일 탐색기(왼쪽), 코드 에디터(가운데), 노트북(오른쪽), 그리고 터미널(아래)을 보여줍니다. 뒤의 세 가지는 모두 커맨드 라인을 활용하는 방법을 보여줍니다.\n코드는 다음 섹션에서 다시 다루겠습니다.\n이 특정 노트북은 방금 논의한 콘솔 세션과 매우 유사합니다.\n터미널은 여러분이 커맨드 라인 도구를 실행할 수 있는 완전한 쉘을 제공합니다.\n이 터미널과 코드, 노트북 사이에는 상호작용이 불가능하다는 점에 유의하십시오.\n따라서 이 터미널은 별도의 터미널 애플리케이션을 열어두는 것과 크게 다르지 않지만, Docker 컨테이너 내부나 원격 서버에서 작업할 때는 여전히 유용합니다.","code":"cat /data/.cache/jupyter-console#!expect_prompt=FALSE\nC-C#!literal=FALSE"},{"path":"chapter-10-polyglot-data-science.html","id":"python","chapter":"제 10 다국어 데이터 과학","heading":"10.3 Python","text":"subprocess 모듈을 사용하면 파이썬에서 커맨드 라인 도구를 실행하고 그들의 표준 입력 및 출력에 연결할 수 있습니다.\n이 모듈은 오래된 os.system() 함수보다 권장되는 방식입니다.\n기본적으로 쉘에서 실행되지는 않지만, run() 함수의 shell 인자를 통해 이를 변경할 수 있습니다.➊ 커맨드 라인을 활용하는 권장되는 방법은 subprocess 모듈의 run() 함수를 사용하는 것입니다.\n➋ filename 파일을 엽니다.\n➌ 전체 텍스트를 단어별로 나눕니다.\n➍ grep 커맨드 라인 도구를 실행하며, 이때 words가 표준 입력으로 전달됩니다.\n➎ 표준 출력은 하나의 긴 문자열로 제공됩니다. 여기서는 pattern의 발생 횟수를 세기 위해 각 줄바꿈 문자를 기준으로 문자열을 나눕니다.이 커맨드 라인 도구는 다음과 같이 사용됩니다.15행의 run 호출의 첫 번째 인자가 문자열 리스트라는 점에 주목하십시오. 첫 번째 항목은 커맨드 라인 도구의 이름이고, 나머지 항목들은 인자들입니다.\n이는 단일 문자열을 전달하는 것과는 다릅니다.\n또한 이는 리다이렉션이나 파이핑과 같은 기능을 가능하게 하는 다른 쉘 구문을 사용할 수 없음을 의미합니다.","code":"bat count.py./count.py alice.txt alice"},{"path":"chapter-10-polyglot-data-science.html","id":"r","chapter":"제 10 다국어 데이터 과학","heading":"10.4 R","text":"R에는 커맨드 라인을 활용하는 여러 가지 방법이 있습니다.아래 예제에서는 R 세션을 시작하고 system2() 함수를 사용하여 이상한 나라의 앨리스(Alice’s Adventures Wonderland)라는 책에서 alice라는 문자열이 몇 번 나타나는지 세어보겠습니다.➊ alice.txt 파일을 읽어옵니다.\n➋ 텍스트를 단어별로 나눕니다.\n➌ grep 커맨드 라인 도구를 호출하여 문자열 alice와 일치하는 줄만 남깁니다. 문자 벡터 words가 표준 입력으로 전달됩니다.\n➍ 문자 벡터 alice의 요소 수를 셉니다.system2()의 단점은 문자 벡터를 커맨드 라인 도구의 표준 입력으로 전달하기 전에 먼저 파일에 기록한다는 점입니다.\n이는 대량의 데이터와 잦은 호출을 처리할 때 문제가 될 수 있습니다.데이터를 디스크에 쓰지 않고 직접 전달하는 명명된 파이프(named pipe)를 사용하는 것이 훨씬 더 효율적입니다.\n이는 pipe()와 fifo() 함수를 통해 가능합니다.\n이 방법을 제안해 준 Jim Hester에게 감사를 표합니다.\n아래 코드는 이를 보여줍니다.➊ fifo() 함수는 out이라는 특별한 선입선출(FIFO) 파일을 생성합니다. 이는 (stdin이나 stdout처럼) 파이프 연결에 대한 참조일 뿐이며, 실제로 디스크에 데이터가 기록되지는 않습니다.\n➋ grep 도구는 b를 포함하는 줄만 남기고 이를 명명된 파이프 out에 씁니다.\n➌ 두 개의 값을 쉘 명령어의 표준 입력으로 씁니다.\n➍ grep에 의해 생성된 표준 출력을 문자 벡터로 읽어옵니다.\n➎ 연결을 관리하고 특별 파일을 삭제합니다.이 방식은 꽤 많은 상용구 코드(연결 생성, 쓰기, 읽기, 정리 등)를 필요로 하므로, 저는 이를 돕기 위한 함수 sh()를 작성했습니다.\nmagrittr 패키지의 파이프 연산자(%>%)를 사용하여 여러 쉘 명령어를 사슬처럼 연결할 수 있습니다.","code":"R --quiet\nlines <- readLines(\"alice.txt\")\nhead(lines)\nwords <- unlist(strsplit(lines, \" \"))\nhead(words)\nalice <- system2(\"grep\", c(\"-i\", \"alice\"), input = words, stdout = TRUE)\nhead(alice)\nlength(alice)out_con <- fifo(\"out\", \"w+\")\nin_con <- pipe(\"grep b > out\")\nwriteLines(c(\"foo\", \"bar\"), in_con)\nreadLines(out_con)\nclose(out_con); close(in_con); unlink(\"out\")library(magrittr)\n\nsh <- function(.data, command) {#! expect_prompt=FALSE\n  temp_file <- tempfile()#! expect_prompt=FALSE\n  out_con <- fifo(temp_file, \"w+\")#! expect_prompt=FALSE\n  in_con <- pipe(paste0(command, \" > \", temp_file))#! expect_prompt=FALSE\n  writeLines(as.character(.data), in_con)#! expect_prompt=FALSE\n  result <- readLines(out_con)#! expect_prompt=FALSE\n  close(out_con)#! expect_prompt=FALSE\n  close(in_con)#! expect_prompt=FALSE\n  unlink(temp_file)#! expect_prompt=FALSE\n  result#! expect_prompt=FALSE\n}\n\nlines <- readLines(\"alice.txt\")\nwords <- unlist(strsplit(lines, \" \"))\n\nsh(words, \"grep -i alice\") %>%#! expect_prompt=FALSE\n  sh(\"wc -l\") %>%#! expect_prompt=FALSE\n  sh(\"cowsay\") %>%#! expect_prompt=FALSE\n  cli::cat_boxx()\n\nq(\"no\")#! expect_prompt=FALSE"},{"path":"chapter-10-polyglot-data-science.html","id":"rstudio","chapter":"제 10 다국어 데이터 과학","heading":"10.5 RStudio","text":"RStudio IDE는 아마도 R을 사용하는 가장 인기 있는 환경일 것입니다.\nRStudio를 열면 가장 먼저 콘솔 탭이 보입니다.터미널 탭은 콘솔 탭 바로 옆에 있습니다.\n터미널 탭은 완전한 쉘을 제공합니다.JupyterLab과 마찬가지로, 이 터미널은 콘솔이나 R 스크립트와 직접 연결되어 있지 않다는 점에 유의하십시오.","code":""},{"path":"chapter-10-polyglot-data-science.html","id":"apache-spark","chapter":"제 10 다국어 데이터 과학","heading":"10.6 Apache Spark","text":"Apache Spark는 클러스터 컴퓨팅 프레임워크입니다.\n데이터를 메모리에 다 올리는 것이 불가능할 때 찾게 되는 강력한 도구입니다.\nSpark 자체는 Scala로 작성되었지만, PySpark를 사용하는 Python이나 SparkR 또는 sparklyr을 사용하는 R을 통해서도 상호작용할 수 있습니다.데이터 처리 및 머신러닝 파이프라인은 일련의 변환(transformations)과 하나의 최종 작업(action)을 통해 정의됩니다.\n이러한 변환 중 하나가 pipe() 변환으로, 이를 통해 전체 데이터셋을 Bash나 Perl 스크립트와 같은 쉘 명령어로 실행할 수 있습니다.\n데이터셋의 항목들이 표준 입력으로 기록되고, 표준 출력은 문자열 RDD로 반환됩니다.아래 세션에서 Spark 쉘을 시작하고 다시 한번 이상한 나라의 앨리스에서 alice의 발생 횟수를 세어보겠습니다.➊ 각 줄이 하나의 요소가 되도록 alice.txt를 읽어옵니다.\n➋ 각 요소를 공백을 기준으로 나눕니다. 즉, 각 줄이 단어들로 나누어집니다.\n➌ 각 파티션을 grep으로 파이핑하여 문자열 alice와 일치하는 요소만 남깁니다.\n➍ 각 파티션을 wc로 파이핑하여 요소의 수를 셉니다.\n➎ 각 파티션에 대해 하나의 카운트 값이 생성됩니다.\n➏ 모든 카운트를 합산하여 최종 카운트를 구합니다. 요소들을 문자열에서 정수형으로 먼저 변환해야 함에 유의하십시오.\n➐ 위의 단계들을 하나의 명령어로 결합한 것입니다.want use custom command-line tool pipeline, need make sure ’s present nodes cluster (known executors).\nOne way specify filename(s) --files option ’re submitting Spark applications using spark-submit.Matei Zaharia와 Bill Chambers(Apache Spark의 원저자)는 그들의 저서 Spark: Definitive Guide에서 “pipe 메소드는 아마도 Spark의 가장 흥미로운 메소드 중 하나일 것”이라고 언급했습니다.\n정말 대단한 찬사입니다!\nApache Spark의 개발자들이 50년 된 기술을 활용할 수 있는 기능을 추가했다는 것은 정말 멋진 일이라고 생각합니다.","code":"alias spark-shell=echo\nspark-shell --master local[6]#!enter=FALSE\nC-C#!literal=FALSE\ncat /data/.cache/spark"},{"path":"chapter-10-polyglot-data-science.html","id":"요약-9","chapter":"제 10 다국어 데이터 과학","heading":"10.7 요약","text":"이 장에서 여러분은 프로그래밍 언어와 다른 환경을 포함하여 다양한 상황에서 커맨드 라인을 활용하는 몇 가지 방법을 배웠습니다.\n커맨드 라인이 진공 상태에서 존재하는 것이 아니라는 점을 깨닫는 것이 중요합니다.\n가장 중요한 것은 작업을 안정적으로 완수할 수 있는 도구들을 사용하는 것이며, 때로는 이들을 조합해서 사용하는 것입니다.이제 네 개의 OSEMN 장과 네 개의 중간 막(intermezzo) 장을 모두 마쳤으니, 이제 마지막 장에서 전체 내용을 마무리하고 결론을 맺을 시간입니다.","code":""},{"path":"chapter-10-polyglot-data-science.html","id":"더-읽어보기-5","chapter":"제 10 다국어 데이터 과학","heading":"10.8 더 읽어보기","text":"커맨드 라인을 사용하지 않고 두 프로그래밍 언어를 직접 통합하는 방법도 있습니다. 예를 들어, R의 reticulate 패키지를 사용하면 파이썬과 직접 인터페이스할 수 있습니다.","code":""},{"path":"chapter-11-conclusion.html","id":"chapter-11-conclusion","chapter":"제 11 결론","heading":"제 11 결론","text":"이 마지막 장으로 책을 마무리하겠습니다.\n먼저 이전 10개 장에서 논의한 내용을 요약하고, 여러분께 세 가지 조언과 함께 우리가 다루었던 관련 주제들을 더 깊이 탐구할 수 있는 자료들을 제안해 드리겠습니다.\n마지막으로 질문이나 의견이 있거나 공유하고 싶은 새로운 커맨드 라인 도구가 있는 경우에 대비하여 저와 연락할 수 있는 몇 가지 방법을 알려드립니다.","code":""},{"path":"chapter-11-conclusion.html","id":"요약해-봅시다","chapter":"제 11 결론","heading":"11.1 요약해 봅시다","text":"이 책은 데이터 과학을 수행하는 데 있어 커맨드 라인을 사용하는 힘을 탐구했습니다.\n상대적으로 젊은 분야인 데이터 과학이 직면한 과제들을 이처럼 오랜 시간 검증된 기술로 해결할 수 있다는 것은 매우 흥미로운 발견이라고 생각합니다.\n이제 여러분이 커맨드 라인으로 무엇을 할 수 있는지 이해하셨기를 바랍니다.\n수많은 커맨드 라인 도구들은 데이터 과학을 구성하는 다양한 작업들에 적합한 온갖 가능성을 제공합니다.데이터 과학에 대한 많은 정의가 존재합니다.\n1장에서 저는 Mason과 Wiggins이 정의한 OSEMN 모델을 소개했습니다. 왜냐하면 매우 구체적인 작업들로 연결되는 아주 실용적인 모델이기 때문입니다.\nOSEMN은 데이터 수집(obtaining), 정제(scrubbing), 탐색(exploring), 모델링(modeling), 해석(interpreting)의 약자입니다. 또한 1장에서는 왜 커맨드 라인이 이러한 데이터 과학 작업들에 매우 적합한지도 설명했습니다.2장에서는 이 책에서 사용된 모든 도구를 얻는 방법을 설명했습니다. 또한 2장에서는 커맨드 라인의 필수적인 도구와 개념들에 대한 소개도 제공했습니다.네 개의 OSEMN 모델 장에서는 커맨드 라인을 사용하여 실용적인 작업들을 수행하는 데 집중했습니다.\n다섯 번째 단계인 데이터 해석(interpreting data)에 대해서는 별도의 장을 할애하지 않았습니다. 솔직히 말해서 커맨드 라인은커녕 컴퓨터조차도 이 단계에서는 거의 도움이 되지 않기 때문입니다.\n하지만 이 주제에 대해 더 읽어볼 만한 몇 가지 포인터를 제공했습니다.네 개의 중간 막(intermezzo) 장에서는 커맨드 라인에서 데이터 과학을 수행하는 데 있어 어느 한 단계에만 국한되지 않는 더 넓은 주제들을 살펴보았습니다.\n4장에서는 한 줄짜리 명령어(one-liners)와 기존 코드를 재사용 가능한 커맨드 라인 도구로 바꾸는 방법을 설명했습니다.\n6장에서는 make라는 도구를 사용하여 데이터 워크플로우를 관리하는 방법을 설명했습니다.\n8장에서는 일반적인 커맨드 라인 도구와 파이프라인을 GNU Parallel을 사용하여 병렬로 실행하는 방법을 보여주었습니다.\n10장에서는 커맨드 라인이 진공 상태에서 존재하는 것이 아니라 다른 프로그래밍 언어나 환경에서도 활용될 수 있음을 보여주었습니다.\n이 중간 막 장들에서 다룬 주제들은 여러분의 데이터 워크플로우 어느 지점에서도 적용될 수 있습니다.데이터 과학을 수행하는 데 사용할 수 있고 관련이 있는 모든 커맨드 라인 도구를 보여주는 것은 불가능합니다.\n새로운 도구들이 매일 만들어지고 있기 때문입니다.\n이제 이해하셨겠지만, 이 책은 도구의 전수 조사를 제공하기보다는 커맨드 라인을 사용한다는 아이디어 자체에 더 중점을 두었습니다.","code":""},{"path":"chapter-11-conclusion.html","id":"세-가지-조언","chapter":"제 11 결론","heading":"11.2 세 가지 조언","text":"여러분은 아마도 이 장들을 읽고 코드 예제들을 따라 하는 데 꽤 많은 시간을 보냈을 것입니다.\n이러한 투자의 효과를 극대화하고 여러분이 데이터 과학 워크플로우에 커맨드 라인을 계속해서 통합할 가능성을 높이기 위해, 저는 여러분께 세 가지 조언을 드리고 싶습니다: (1) 인내심을 갖고(patient), (2) 창의적이며(creative), (3) 실용적이 되십시오(practical). 다음 세 개의 하위 섹션에서 각 조언에 대해 자세히 설명하겠습니다.","code":""},{"path":"chapter-11-conclusion.html","id":"인내심을-가지십시오","chapter":"제 11 결론","heading":"11.2.1 인내심을 가지십시오","text":"제가 드릴 수 있는 첫 번째 조언은 인내심을 가지라는 것입니다.\n커맨드 라인에서 데이터를 다루는 것은 프로그래밍 언어를 사용하는 것과 다르며, 따라서 다른 사고방식이 필요합니다.또한, 커맨드 라인 도구들 자체에도 독특한 특징이나 불일치하는 부분이 있습니다.\n이는 부분적으로 이 도구들이 수십 년에 걸쳐 수많은 사람에 의해 개발되었기 때문입니다.\n혹시라도 그 수많은 옵션에 대해 당황하게 된다면, 잊지 말고 --help, man, tldr 또는 자주 사용하는 검색 엔진을 활용해 보세요.여전히, 특히 처음에는 좌절감을 느낄 수 있습니다.\n저를 믿으세요, 커맨드 라인과 그 도구들을 사용하는 연습을 하다 보면 점점 더 능숙해질 것입니다.\n커맨드 라인은 수십 년 동안 존재해 왔으며, 앞으로도 수십 년 동안 계속 존재할 것입니다.\n그것은 투자할 가치가 충분합니다.","code":""},{"path":"chapter-11-conclusion.html","id":"창의적이-되십시오","chapter":"제 11 결론","heading":"11.2.2 창의적이 되십시오","text":"그와 관련된 두 번째 조언은 창의적이 되라는 것입니다.\n커맨드 라인은 매우 유연합니다.\n커맨드 라인 도구들을 결합함으로써, 여러분이 생각하는 것보다 더 많은 일을 해낼 수 있습니다.저는 여러분이 즉시 프로그래밍 언어로 되돌아가지 않기를 권장합니다.\n그리고 프로그래밍 언어를 사용해야 할 때는, 그 코드가 어떤 방식으로든 일반화되거나 재사용될 수 있을지 생각해 보세요.\n만약 그렇다면 4장에서 논의한 단계들을 따라 그 코드로 여러분만의 커맨드 라인 도구를 만드는 것을 고려해 보세요.\n여러분의 도구가 다른 사람들에게도 유익할 것이라고 믿는다면 오픈 소스로 만드는 한 걸음을 더 나아갈 수도 있습니다.\n커맨드 라인에서 수행하는 방법을 알고 있는 단계가 있지만, 현재 작업 중인 주요 프로그래밍 언어나 환경의 편안함을 떠나고 싶지 않을 수도 있습니다.\n그럴 때는 10장에 나열된 접근 방식 중 하나를 사용할 수도 있습니다.","code":""},{"path":"chapter-11-conclusion.html","id":"실용적이-되십시오","chapter":"제 11 결론","heading":"11.2.3 실용적이 되십시오","text":"세 번째 조언은 실용적이 되라는 것입니다.\n실용적이 되는 것은 창의적이 되는 것과 관련이 있지만, 별도의 설명이 필요합니다.\n이전 하위 섹션에서 저는 즉시 프로그래밍 언어로 되돌아가지 말라고 언급했습니다.\n물론 커맨드 라인에도 한계는 있습니다.\n책 전반에 걸쳐 저는 커맨드 라인이 데이터 과학을 수행하는 데 있어 보조적인(companion) 접근 방식으로 간주되어야 한다고 강조해 왔습니다.저는 커맨드 라인에서 데이터 과학을 수행하기 위한 네 단계를 논의했습니다.\n실제로 커맨드 라인의 적용 가능성은 1단계가 4단계보다 더 높습니다.\n여러분은 당면한 작업에 가장 잘 맞는 접근 방식을 사용해야 합니다.\n그리고 워크플로우의 어느 지점에서든 접근 방식들을 혼합하여 사용하는 것도 완벽하게 괜찮습니다.\n10장에서 보여드렸듯이, 커맨드 라인은 다른 접근 방식, 프로그래밍 언어, 통계 환경과 통합되는 데 매우 훌륭합니다.\n각 접근 방식에는 일정한 트레이드오프(trade-)가 있으며, 커맨드 라인에 능숙해지는 과정 중 일부는 언제 어떤 방식을 사용할지 배우는 것입니다.결론적으로, 여러분이 인내심을 갖고 창의적이며 실용적이 될 때, 커맨드 라인은 여러분을 더 효율적이고 생산적인 데이터 과학자로 만들어 줄 것입니다.","code":""},{"path":"chapter-11-conclusion.html","id":"앞으로-어디로-나아가야-할까요","chapter":"제 11 결론","heading":"11.3 앞으로 어디로 나아가야 할까요?","text":"이 책은 커맨드 라인과 데이터 과학의 접점에 관한 것이므로, 많은 관련 주제가 맛보기로만 다루어졌습니다.\n이제 이러한 주제들을 더 깊이 탐구하는 것은 여러분의 몫입니다.\n다음 하위 섹션들은 참고할 만한 주제 리스트와 권장 자료들을 제공합니다.","code":""},{"path":"chapter-11-conclusion.html","id":"커맨드-라인","chapter":"제 11 결론","heading":"11.4 커맨드 라인","text":"Linux Command Line: Complete Introduction, 2nd Edition William E. Shotts, Jr. (Starch Press, 2019)Unix Power Tools, 3rd Edition Jerry Peek, Shelley Powers, Tim O’Reilly, Mike Loukides (O’Reilly Media, 2002)Learning Vi Vim Editors, 7th Edition Arnold Robbins, Elbert Hannah, Linda Lamb (O’Reilly Media, 2008)","code":""},{"path":"chapter-11-conclusion.html","id":"쉘-프로그래밍","chapter":"제 11 결론","heading":"11.4.1 쉘 프로그래밍","text":"Classic Shell Scripting Arnold Robbins Nelson H.F. Beebe (O’Reilly Media, 2005)Wicked Cool Shell Scripts, 2nd Edition Dave Taylor Brandon Perry (Starch Press, 2017)Bash Cookbook Carl Albing JP Vossen (O’Reilly Media, 2018)","code":""},{"path":"chapter-11-conclusion.html","id":"python-r-그리고-sql","chapter":"제 11 결론","heading":"11.4.2 Python, R, 그리고 SQL","text":"Learn Python 3 Hard Way Zed . Shaw (Addison-Wesley Professional, 2017)Python Data Analysis, 2nd Edition Wes McKinney (O’Reilly Media, 2017)Data Science Scratch, 2nd Edition Joel Grus (O’Reilly Media, 2019)R Data Science Garrett Grolemund Hadley Wickham (O’Reilly Media, 2016)R Everyone, 2nd edition Jared Lander (Addison-Wesley Professional, 2017)Sams Teach SQL 10 Minutes Day, 5th Edition Ben Forta (Sams, 2020)","code":""},{"path":"chapter-11-conclusion.html","id":"api","chapter":"제 11 결론","heading":"11.4.3 API","text":"Mining Social Web, 3rd Edition Matthew . Russell Mikhail Klassen (O’Reilly Media, 2019)Data Source Handbook Pete Warden (O’Reilly Media, 2011)","code":""},{"path":"chapter-11-conclusion.html","id":"머신러닝","chapter":"제 11 결론","heading":"11.4.4 머신러닝","text":"Python Machine Learning, 3rd Edition Sebastian Raschka Vahid Mirjalili (Packt Publishing, 2019)Pattern Recognition Machine Learning Christopher M. Bishop (Springer, 2006)Information Theory, Inference, Learning Algorithms David MacKay (Cambridge University Press, 2003)","code":""},{"path":"chapter-11-conclusion.html","id":"연락하기","chapter":"제 11 결론","heading":"11.5 연락하기","text":"이 책은 커맨드 라인과 수많은 도구를 만든 많은 사람이 없었다면 세상에 나올 수 없었을 것입니다.\n데이터 과학을 위한 현재의 커맨드 라인 도구 생태계는 커뮤니티의 노력의 결과라고 해도 과언이 아닙니다.\n저는 여러분께 사용할 수 있는 수많은 커맨드 라인 도구 중 극히 일부만을 보여드릴 수 있었습니다.\n새로운 도구들은 매일 만들어지고 있으며, 어쩌면 여러분도 언젠가 직접 도구를 만드실지도 모릅니다.\n그런 경우, 여러분의 이야기를 듣고 싶습니다.\n질문, 의견 또는 제안 사항이 있을 때 언제든지 연락해 주시면 감사하겠습니다.\n저와 연락할 수 있는 몇 가지 방법은 다음과 같습니다.이메일: jeroen@jeroenjanssens.com트위터: @jeroenhjanssens책 웹사이트: https://datascienceatthecommandline.com/책 GitHub 저장소: https://github.com/jeroenjanssens/data-science---command-line감사합니다.","code":""},{"path":"커맨드-라인-도구-목록.html","id":"커맨드-라인-도구-목록","chapter":"커맨드 라인 도구 목록","heading":"커맨드 라인 도구 목록","text":"이 장은 이 책에서 논의된 모든 커맨드 라인 도구들에 대한 개요입니다.\n여기에는 바이너리 실행 파일, 인터프리터 스크립트, 그리고 Z Shell 빌트인(builtins) 및 키워드가 포함됩니다.\n각 커맨드 라인 도구에 대해 가능한 경우 그리고 적절한 경우 다음과 같은 정보가 제공됩니다.커맨드 라인에 입력할 실제 명령어설명책에서 사용된 버전해당 버전이 출시된 연도주요 저자더 많은 정보를 찾을 수 있는 웹사이트도움말을 얻는 방법사용 예시여기에 나열된 모든 커맨드 라인 도구는 Docker 이미지에 포함되어 있습니다.\n설정 방법은 2장을 참조하십시오.\n오픈 소스 소프트웨어를 인용하는 것은 간단하지 않으며, 일부 정보가 누락되었거나 정확하지 않을 수 있음에 유의하십시오.","code":""},{"path":"커맨드-라인-도구-목록.html","id":"alias","chapter":"커맨드 라인 도구 목록","heading":"alias","text":"별칭을 정의하거나 표시합니다.\nalias는 Z shell 빌트인입니다.","code":"type alias\nman zshbuiltins | grep -A 10 alias#!enter=FALSE\nC-C#!literal=FALSE\nalias l\nalias python=python3"},{"path":"커맨드-라인-도구-목록.html","id":"awk","chapter":"커맨드 라인 도구 목록","heading":"awk","text":"패턴 스캐닝 및 텍스트 처리 언어입니다.\nawk (버전 1.3.4), Mike D. Brennan 및 Thomas E. Dickey 제작 (2019).\n더 많은 정보: https://invisible-island.net/mawk.","code":"type awk\nman awk#!enter=FALSE\nC-C#!literal=FALSE\nseq 5 | awk '{sum+=$1} END {print sum}'"},{"path":"커맨드-라인-도구-목록.html","id":"aws","chapter":"커맨드 라인 도구 목록","heading":"aws","text":"AWS 서비스를 관리하기 위한 통합 도구입니다.\naws (버전 2.1.32), Amazon Web Services 제작 (2021).\n더 많은 정보: https://aws.amazon.com/cli.","code":"type aws\naws --help#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"bash","chapter":"커맨드 라인 도구 목록","heading":"bash","text":"GNU Bourne-SHell입니다.\nbash (버전 5.0.17), Brian Fox 및 Chet Ramey 제작 (2019).\n더 많은 정보: https://www.gnu.org/software/bash.","code":"type bash\nman bash#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"bat","chapter":"커맨드 라인 도구 목록","heading":"bat","text":"구문 강조 및 Git 연동 기능이 있는 cat 클론입니다.\nbat (버전 0.18.0), David Peter 제작 (2021).\n더 많은 정보: https://github.com/sharkdp/bat.","code":"type bat\nman bat#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"bc","chapter":"커맨드 라인 도구 목록","heading":"bc","text":"임의 정밀도 계산기 언어입니다.\nbc (버전 1.07.1), Philip . Nelson 제작 (2017).\n더 많은 정보: https://www.gnu.org/software/bc.","code":"type bc\nman bc#!enter=FALSE\nC-C#!literal=FALSE\nbc -l <<< 'e(1)'"},{"path":"커맨드-라인-도구-목록.html","id":"body","chapter":"커맨드 라인 도구 목록","heading":"body","text":"첫 번째 줄을 제외한 모든 줄에 명령을 적용합니다.\nbody (버전 0.1), Jeroen Janssens 제작 (2021).\n더 많은 정보: https://github.com/jeroenjanssens/dsutils.","code":"type body\nseq 10 | header -a 'values' | body shuf"},{"path":"커맨드-라인-도구-목록.html","id":"cat","chapter":"커맨드 라인 도구 목록","heading":"cat","text":"파일들을 연결하여 표준 출력으로 인쇄합니다.\ncat (버전 8.30), Torbjorn Granlund 및 Richard M. Stallman 제작 (2018).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type cat\nman cat#!enter=FALSE\nC-C#!literal=FALSE\ncat *.log > all.log#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"cd","chapter":"커맨드 라인 도구 목록","heading":"cd","text":"쉘 작업 디렉토리를 변경합니다.\ncd는 Z shell 빌트인입니다.","code":"type cd\nman zshbuiltins | grep -A 10 cd#!enter=FALSE\nC-C#!literal=FALSE\ncd ~\npwd\ncd ..\npwd\ncd /data/ch01"},{"path":"커맨드-라인-도구-목록.html","id":"chmod","chapter":"커맨드 라인 도구 목록","heading":"chmod","text":"파일 모드 비트를 변경합니다.\nchmod (버전 8.30), David MacKenzie 및 Jim Meyering 제작 (2018).\n저는 4장에서 도구를 실행 가능하게 만들기 위해 chmod를 사용했습니다.\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type chmod\nman chmod#!enter=FALSE\nC-C#!literal=FALSE\nchmod u+x script.sh#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"cols","chapter":"커맨드 라인 도구 목록","heading":"cols","text":"일부 열에 명령을 적용합니다.\ncols (버전 0.1), Jeroen Janssens 제작 (2021).\n더 많은 정보: https://github.com/jeroenjanssens/dsutils.","code":"type cols"},{"path":"커맨드-라인-도구-목록.html","id":"column","chapter":"커맨드 라인 도구 목록","heading":"column","text":"목록을 여러 열로 나열합니다.\ncolumn (버전 2.36.1), Karel Zak 제작 (2021).\n더 많은 정보: https://www.kernel.org/pub/linux/utils/util-linux.","code":"type column"},{"path":"커맨드-라인-도구-목록.html","id":"cowsay","chapter":"커맨드 라인 도구 목록","heading":"cowsay","text":"말하는 소를 출력합니다. (설정 가능)\ncowsay (버전 3.0.3), Tony Monroe 제작 (1999).\n더 많은 정보: https://github.com/tnalpgge/rank-amateur-cowsay.","code":"type cowsay\nman cowsay#!enter=FALSE\nC-C#!literal=FALSE\necho 'The command line is awesome!' | cowsay"},{"path":"커맨드-라인-도구-목록.html","id":"cp","chapter":"커맨드 라인 도구 목록","heading":"cp","text":"파일 및 디렉토리를 복사합니다.\ncp (버전 8.30), Torbjorn Granlund, David MacKenzie 및 Jim Meyering 제작 (2018).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type cp\nman cp#!enter=FALSE\nC-C#!literal=FALSE\ncp -r ~/Downloads/*.xlsx /data#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"csv2vw","chapter":"커맨드 라인 도구 목록","heading":"csv2vw","text":"CSV를 Vowpal Wabbit 형식으로 변환합니다.\ncsv2vw (버전 0.1), Jeroen Janssens 제작 (2021).\n더 많은 정보: https://github.com/jeroenjanssens/dsutils.","code":"type csv2vw"},{"path":"커맨드-라인-도구-목록.html","id":"csvcut","chapter":"커맨드 라인 도구 목록","heading":"csvcut","text":"CSV 파일을 필터링하고 자릅니다.\ncsvcut (버전 1.0.5), Christopher Groskopf 제작 (2020).\n더 많은 정보: https://csvkit.rtfd.org.","code":"type csvcut\ncsvcut --help#!enter=FALSE\nC-C#!literal=FALSE\ncsvcut -c bill,tip /data/ch05/tips.csv | trim"},{"path":"커맨드-라인-도구-목록.html","id":"csvgrep","chapter":"커맨드 라인 도구 목록","heading":"csvgrep","text":"CSV 파일을 검색합니다.\ncsvgrep (버전 1.0.5), Christopher Groskopf 제작 (2020).\n더 많은 정보: https://csvkit.rtfd.org.","code":"type csvgrep\ncsvgrep --help#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"csvjoin","chapter":"커맨드 라인 도구 목록","heading":"csvjoin","text":"지정된 열을 기준으로 CSV 파일을 병합합니다. (SQL 스타일 조인)\ncsvjoin (버전 1.0.5), Christopher Groskopf 제작 (2020).\n더 많은 정보: https://csvkit.rtfd.org.","code":"type csvjoin\ncsvjoin --help#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"csvlook","chapter":"커맨드 라인 도구 목록","heading":"csvlook","text":"콘솔에서 CSV 파일을 Markdown 호환 고정폭 표로 렌더링합니다.\ncsvlook (버전 1.0.5), Christopher Groskopf 제작 (2020).\n더 많은 정보: https://csvkit.rtfd.org.","code":"type csvlook\ncsvlook --help#!enter=FALSE\nC-C#!literal=FALSE\ncsvlook /data/ch05/tips.csv"},{"path":"커맨드-라인-도구-목록.html","id":"csvquote","chapter":"커맨드 라인 도구 목록","heading":"csvquote","text":"일반 유닉스 유틸리티가 CSV 데이터와 올바르게 작동하도록 설정합니다.\ncsvquote (버전 0.1), Dan Brown 제작 (2018).\n더 많은 정보: https://github.com/dbro/csvquote.","code":"type csvquote"},{"path":"커맨드-라인-도구-목록.html","id":"csvsort","chapter":"커맨드 라인 도구 목록","heading":"csvsort","text":"CSV 파일을 정렬합니다.\ncsvsort (버전 1.0.5), Christopher Groskopf 제작 (2020).\n더 많은 정보: https://csvkit.rtfd.org.","code":"type csvsort\ncsvsort --help#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"csvsql","chapter":"커맨드 라인 도구 목록","heading":"csvsql","text":"CSV 파일에 대해 SQL 문을 실행합니다.\ncsvsql (버전 1.0.5), Christopher Groskopf 제작 (2020).\n더 많은 정보: https://csvkit.rtfd.org.","code":"type csvsql\ncsvsql --help#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"csvstack","chapter":"커맨드 라인 도구 목록","heading":"csvstack","text":"여러 CSV 파일의 행을 쌓아 올립니다.\ncsvstack (버전 1.0.5), Christopher Groskopf 제작 (2020).\n더 많은 정보: https://csvkit.rtfd.org.","code":"type csvstack\ncsvstack --help#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"csvstat","chapter":"커맨드 라인 도구 목록","heading":"csvstat","text":"CSV 파일의 각 열에 대한 기술 통계를 인쇄합니다.\ncsvstat (버전 1.0.5), Christopher Groskopf 제작 (2020).\n더 많은 정보: https://csvkit.rtfd.org.","code":"type csvstat\ncsvstat --help#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"curl","chapter":"커맨드 라인 도구 목록","heading":"curl","text":"URL을 전송합니다.\ncurl (버전 7.68.0), Daniel Stenberg 제작 (2016).\n더 많은 정보: https://curl.haxx.se.","code":"type curl\nman curl#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"cut","chapter":"커맨드 라인 도구 목록","heading":"cut","text":"파일의 각 줄에서 섹션을 제거합니다.\ncut (버전 8.30), David M. Ihnat, David MacKenzie 및 Jim Meyering 제작 (2019).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type cut\nman cut#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"display","chapter":"커맨드 라인 도구 목록","heading":"display","text":"X 서버에 이미지 또는 이미지 시퀀스를 표시합니다.\ndisplay (버전 6.9.10-23), ImageMagick Studio LLC 제작 (2019).\n더 많은 정보: https://imagemagick.org.","code":"type display"},{"path":"커맨드-라인-도구-목록.html","id":"dseq","chapter":"커맨드 라인 도구 목록","heading":"dseq","text":"날짜 시퀀스를 생성합니다.\ndseq (버전 0.1), Jeroen Janssens 제작 (2021).\n더 많은 정보: https://github.com/jeroenjanssens/dsutils.","code":"type dseq\ndseq 3"},{"path":"커맨드-라인-도구-목록.html","id":"echo","chapter":"커맨드 라인 도구 목록","heading":"echo","text":"텍스트 한 줄을 표시합니다.\necho (버전 8.30), Brian Fox 및 Chet Ramey 제작 (2019).\n리터럴 텍스트를 다음 도구의 표준 입력으로 사용하는 데 유용합니다.\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type echo\nman echo#!enter=FALSE\nC-C#!literal=FALSE\necho Hippopotomonstrosesquippedaliophobia | cowsay\necho -n Hippopotomonstrosesquippedaliophobia | wc -c"},{"path":"커맨드-라인-도구-목록.html","id":"env","chapter":"커맨드 라인 도구 목록","heading":"env","text":"수정된 환경에서 프로그램을 실행합니다.\nenv (버전 8.30), Richard Mlynarik, David MacKenzie 및 Assaf Gordon 제작 (2018).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type env\nman env#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"export","chapter":"커맨드 라인 도구 목록","heading":"export","text":"쉘 변수에 대해 내보내기(export) 속성을 설정합니다. 쉘 변수를 다른 커맨드 라인 도구에서 사용할 수 있게 만드는 데 유용합니다.\nexport는 Z shell 빌트인입니다.","code":"type export\nman zshbuiltins | grep -A 10 export#!enter=FALSE\nC-C#!literal=FALSE\nexport PATH=\"$PATH:$HOME/bin\""},{"path":"커맨드-라인-도구-목록.html","id":"fc","chapter":"커맨드 라인 도구 목록","heading":"fc","text":"대화형 기록 메커니즘을 제어합니다.\nfc는 Z shell 빌트인입니다.\n저는 4장에서 명령어를 nano에서 편집하기 위해 fc를 사용했습니다.","code":"type fc\nman zshbuiltins | grep -A 10 '^ *fc '#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"find","chapter":"커맨드 라인 도구 목록","heading":"find","text":"디렉토리 계층 구조에서 파일을 검색합니다.\nfind (버전 4.7.0), Eric B. Decker, James Youngman 및 Kevin Dalley 제작 (2019).\n더 많은 정보: https://www.gnu.org/software/findutils.","code":"type find\nman find#!enter=FALSE\nC-C#!literal=FALSE\nfind /data -type f -name '*.csv' -size -3"},{"path":"커맨드-라인-도구-목록.html","id":"fold","chapter":"커맨드 라인 도구 목록","heading":"fold","text":"지정된 너비에 맞춰 각 입력 줄을 감쌉니다(wrap).\nfold (버전 8.30), David MacKenzie 제작 (2020).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type fold\nman fold#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"for","chapter":"커맨드 라인 도구 목록","heading":"for","text":"목록의 각 멤버에 대해 명령을 실행합니다.\nfor는 Z shell 빌트인입니다.\n8장에서 저는 대신 parallel을 사용할 때의 이점에 대해 논의합니다.","code":"type for\nman zshmisc | grep -EA 10 '^ *for '#!enter=FALSE\nC-C#!literal=FALSE\nfor i in {A..C} \"It's easy as\" {1..3}; do echo $i; done"},{"path":"커맨드-라인-도구-목록.html","id":"fx","chapter":"커맨드 라인 도구 목록","heading":"fx","text":"대화형 JSON 뷰어입니다.\nfx (버전 20.0.2), Anton Medvedev 제작 (2020).\n더 많은 정보: https://github.com/antonmedv/fx.","code":"type fx\nfx --help#!enter=FALSE\nC-C#!literal=FALSE\necho '[1,2,3]' | fx 'this.map(x => x * 2)'"},{"path":"커맨드-라인-도구-목록.html","id":"git","chapter":"커맨드 라인 도구 목록","heading":"git","text":"분산 버전 관리 시스템입니다.\ngit (버전 2.25.1), Linus Torvalds 및 Junio C. Hamano 제작 (2021).\n더 많은 정보: https://git-scm.com.","code":"type git\nman git#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"grep","chapter":"커맨드 라인 도구 목록","heading":"grep","text":"패턴과 일치하는 줄을 출력합니다.\ngrep (버전 3.4), Jim Meyering 제작 (2019).\n더 많은 정보: https://www.gnu.org/software/grep.","code":"type grep\nman grep#!enter=FALSE\nC-C#!literal=FALSE\nseq 100 | grep 3 | wc -l"},{"path":"커맨드-라인-도구-목록.html","id":"gron","chapter":"커맨드 라인 도구 목록","heading":"gron","text":"JSON을 검색(grep) 가능하게 만듭니다.\ngron (버전 0.6.1), Tom Hudson 제작 (2021).\n더 많은 정보: https://github.com/TomNomNom/gron.","code":"type gron\nman gron#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"head","chapter":"커맨드 라인 도구 목록","heading":"head","text":"파일의 앞부분을 출력합니다.\nhead (버전 8.30), David MacKenzie 및 Jim Meyering 제작 (2019).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type head\nman head#!enter=FALSE\nC-C#!literal=FALSE\nseq 100 | head -n 5"},{"path":"커맨드-라인-도구-목록.html","id":"header","chapter":"커맨드 라인 도구 목록","heading":"header","text":"헤더 라인을 추가, 교체 및 삭제합니다.\nheader (버전 0.1), Jeroen Janssens 제작 (2021).\n더 많은 정보: https://github.com/jeroenjanssens/dsutils.","code":"type header"},{"path":"커맨드-라인-도구-목록.html","id":"history","chapter":"커맨드 라인 도구 목록","heading":"history","text":"GNU History 라이브러리입니다.\nhistory (버전 8.1), Brian Fox 및 Chet Ramey 제작 (2020).\n더 많은 정보: https://www.gnu.org/software/bash.","code":"type history"},{"path":"커맨드-라인-도구-목록.html","id":"hostname","chapter":"커맨드 라인 도구 목록","heading":"hostname","text":"시스템의 호스트 이름을 표시하거나 설정합니다.\nhostname (버전 3.23), Peter Tobias, Bernd Eckenfels 및 Michael Meskes 제작 (2021).\n더 많은 정보: https://sourceforge.net/projects/net-tools/.","code":"type hostname\nman hostname#!enter=FALSE\nC-C#!literal=FALSE\nhostname\nhostname -i"},{"path":"커맨드-라인-도구-목록.html","id":"in2csv","chapter":"커맨드 라인 도구 목록","heading":"in2csv","text":"일반적이지만 덜 멋진 테이블 데이터 형식을 CSV로 변환합니다.\nin2csv (버전 1.0.5), Christopher Groskopf 제작 (2020).\n더 많은 정보: https://csvkit.rtfd.org.","code":"type in2csv\nin2csv --help#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"jq","chapter":"커맨드 라인 도구 목록","heading":"jq","text":"커맨드 라인용 JSON 프로세서입니다.\njq (버전 1.6), Stephen Dolan 제작 (2021).\n더 많은 정보: https://stedolan.github.com/jq.","code":"type jq\nman jq#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"json2csv","chapter":"커맨드 라인 도구 목록","heading":"json2csv","text":"JSON을 CSV로 변환합니다.\njson2csv (버전 1.2.1), Jehiah Czebotar 제작 (2019).\n더 많은 정보: https://github.com/jehiah/json2csv.","code":"type json2csv\njson2csv --help#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"l","chapter":"커맨드 라인 도구 목록","heading":"l","text":"디렉토리 내용을 긴 형식(long format)으로 나열합니다. 디렉토리는 파일보다 먼저 그룹화되고, 파일 크기는 읽기 쉬운 단위로 표시되며, 접근 권한도 포함됩니다.\nl 제작자 미상 (1999).","code":"type l\ncd /data/ch03\nls\nl"},{"path":"커맨드-라인-도구-목록.html","id":"less","chapter":"커맨드 라인 도구 목록","heading":"less","text":"more의 반대입니다. (파일 내용을 페이지 단위로 보여주는 유틸리티)\nless (버전 551), Mark Nudelman 제작 (2019).\n더 많은 정보: https://www.greenwoodsoftware.com/less.","code":"type less\nman less#!enter=FALSE\nC-C#!literal=FALSE\nless README#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"ls","chapter":"커맨드 라인 도구 목록","heading":"ls","text":"디렉토리 내용을 나열합니다.\nls (버전 8.30), Richard M. Stallman 및 David MacKenzie 제작 (2019).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type ls\nman ls#!enter=FALSE\nC-C#!literal=FALSE\nls /data"},{"path":"커맨드-라인-도구-목록.html","id":"make","chapter":"커맨드 라인 도구 목록","heading":"make","text":"컴퓨터 프로그램을 유지 관리하기 위한 프로그램입니다.\nmake (버전 4.3), Stuart . Feldman 제작 (2020).\n더 많은 정보: https://www.gnu.org/software/make.","code":"type make\nman make#!enter=FALSE\nC-C#!literal=FALSE\nmake sandwich#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"man","chapter":"커맨드 라인 도구 목록","heading":"man","text":"시스템 참조 매뉴얼에 대한 인터페이스입니다. (도움말 시스템)\nman (버전 2.9.1), John W. Eaton 및 Colin Watson 제작 (2020).\n더 많은 정보: https://nongnu.org/man-db.","code":"type man\nman man#!enter=FALSE\nC-C#!literal=FALSE\nman excel"},{"path":"커맨드-라인-도구-목록.html","id":"mkdir","chapter":"커맨드 라인 도구 목록","heading":"mkdir","text":"디렉토리를 생성합니다.\nmkdir (버전 8.30), David MacKenzie 제작 (2019).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type mkdir\nman mkdir#!enter=FALSE\nC-C#!literal=FALSE\nmkdir -p /data/ch{01..10}#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"mv","chapter":"커맨드 라인 도구 목록","heading":"mv","text":"파일을 이동하거나 이름을 변경합니다.\nmv (버전 8.30), Mike Parker, David MacKenzie 및 Jim Meyering 제작 (2020).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type mv\nman mv#!enter=FALSE\nC-C#!literal=FALSE\nmv results{,.bak}#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"nano","chapter":"커맨드 라인 도구 목록","heading":"nano","text":"Pico에서 영감을 받은 Nano’s ANOther 에디터입니다.\nnano (버전 5.4), Benno Schulenberg 외 다수 제작 (2020).\n더 많은 정보: https://nano-editor.org.","code":"type nano"},{"path":"커맨드-라인-도구-목록.html","id":"nl","chapter":"커맨드 라인 도구 목록","heading":"nl","text":"파일의 줄에 번호를 매깁니다.\nnl (버전 8.30), Scott Bartram 및 David MacKenzie 제작 (2020).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type nl\nman nl#!enter=FALSE\nC-C#!literal=FALSE\nnl /data/ch05/alice.txt | head"},{"path":"커맨드-라인-도구-목록.html","id":"parallel","chapter":"커맨드 라인 도구 목록","heading":"parallel","text":"동일 노드 또는 원격 노드에서 병렬로 작업을 빌드하고 실행합니다.\nparallel (버전 20161222), Ole Tange 제작 (2016).\n더 많은 정보: https://www.gnu.org/software/parallel.","code":"type parallel\nman parallel#!enter=FALSE\nC-C#!literal=FALSE\nseq 3 | parallel \"echo Processing file {}.csv\""},{"path":"커맨드-라인-도구-목록.html","id":"paste","chapter":"커맨드 라인 도구 목록","heading":"paste","text":"파일의 줄을 병합합니다.\npaste (버전 8.30), David M. Ihnat 및 David MacKenzie 제작 (2019).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type paste\nman paste#!enter=FALSE\nC-C#!literal=FALSE\npaste -d, <(seq 5) <(dseq 5)\nseq 5 | paste -sd+"},{"path":"커맨드-라인-도구-목록.html","id":"pbc","chapter":"커맨드 라인 도구 목록","heading":"pbc","text":"병렬 bc(병렬 계산기)입니다.\npbc Jeroen Janssens 제작 (2021).\n더 많은 정보: https://github.com/jeroenjanssens/dsutils.","code":"type pbc\nseq 3 | pbc '{1}^2'"},{"path":"커맨드-라인-도구-목록.html","id":"pip","chapter":"커맨드 라인 도구 목록","heading":"pip","text":"Python 패키지를 설치하고 관리하기 위한 도구입니다.\npip (버전 20.0.2), PyPA 제작 (2020).\n더 많은 정보: https://pip.pypa.io.","code":"type pip\nman pip#!enter=FALSE\nC-C#!literal=FALSE\npip install pandas#!enter=FALSE\nC-C#!literal=FALSE\npip freeze | grep sci"},{"path":"커맨드-라인-도구-목록.html","id":"pup","chapter":"커맨드 라인 도구 목록","heading":"pup","text":"커맨드 라인에서 HTML을 파싱합니다.\npup (버전 0.4.0), Eric Chiang 제작 (2016).\n더 많은 정보: https://github.com/EricChiang/pup.","code":"type pup\npup --help#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"pwd","chapter":"커맨드 라인 도구 목록","heading":"pwd","text":"작업 디렉토리의 이름을 인쇄합니다.\npwd (버전 8.30), Jim Meyering 제작 (2019).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type pwd\nman pwd#!enter=FALSE\nC-C#!literal=FALSE\ncd ~\npwd"},{"path":"커맨드-라인-도구-목록.html","id":"python-1","chapter":"커맨드 라인 도구 목록","heading":"python","text":"인터프리터 방식의 대화형 객체 지향 프로그래밍 언어입니다.\npython (버전 3.8.5), Python Software Foundation 제작 (2021).\n더 많은 정보: https://www.python.org.","code":"type python\nman python#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"r-1","chapter":"커맨드 라인 도구 목록","heading":"R","text":"통계 컴퓨팅을 위한 언어 및 환경입니다.\nR (버전 4.0.4), R Foundation Statistical Computing 제작 (2021).\n더 많은 정보: https://www.r-project.org.","code":"type R\nman R#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"rev","chapter":"커맨드 라인 도구 목록","heading":"rev","text":"각 줄의 문자 순서를 반대로 바꿉니다.\nrev (버전 2.36.1), Karel Zak 제작 (2021).\n더 많은 정보: https://www.kernel.org/pub/linux/utils/util-linux.","code":"type rev\necho 'Satire: Veritas' | rev\necho 'Ça va?' | rev | cut -c 2- | rev"},{"path":"커맨드-라인-도구-목록.html","id":"rm","chapter":"커맨드 라인 도구 목록","heading":"rm","text":"파일이나 디렉토리를 제거합니다.\nrm (버전 8.30), Paul Rubin 외 다수 제작 (2019).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type rm\nman rm#!enter=FALSE\nC-C#!literal=FALSE\nrm *.old#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"rush","chapter":"커맨드 라인 도구 목록","heading":"rush","text":"쉘에서 실행하는 R 한 줄짜리 명령어 도구입니다.\nrush (버전 0.1), Jeroen Janssens 제작 (2021).\n더 많은 정보: https://github.com/jeroenjanssens/rush.","code":"type rush\nrush --help#!enter=FALSE\nC-C#!literal=FALSE\nrush run '6*7'\nrush run --tidyverse 'filter(starwars, species == \"Human\") %>% select(name)'"},{"path":"커맨드-라인-도구-목록.html","id":"sample","chapter":"커맨드 라인 도구 목록","heading":"sample","text":"특정 확률, 지정된 지연 시간 또는 특정 지속 시간 동안 표준 입력의 줄을 필터링합니다.\nsample (버전 0.2.4), Jeroen Janssens 제작 (2021).\n더 많은 정보: https://github.com/jeroenjanssens/sample.","code":"type sample\nsample --help#!enter=FALSE\nC-C#!literal=FALSE\nseq 1000 | sample -r 0.01 | trim 5"},{"path":"커맨드-라인-도구-목록.html","id":"scp","chapter":"커맨드 라인 도구 목록","heading":"scp","text":"OpenSSH 보안 파일 복사 도구입니다.\nscp (버전 1:8.2p1-4ubuntu0.2), Timo Rinne 및 Tatu Ylonen 제작 (2019).\n더 많은 정보: https://www.openssh.com.","code":"type scp\nman scp#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"sed","chapter":"커맨드 라인 도구 목록","heading":"sed","text":"텍스트를 필터링하고 변환하기 위한 스트림 에디터입니다.\nsed (버전 4.7), Jay Fenlason 외 다수 제작 (2018).\n더 많은 정보: https://www.gnu.org/software/sed.","code":"type sed\nman sed#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"seq","chapter":"커맨드 라인 도구 목록","heading":"seq","text":"숫자 시퀀스를 인쇄합니다.\nseq (버전 8.30), Ulrich Drepper 제작 (2019).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type seq\nman seq#!enter=FALSE\nC-C#!literal=FALSE\nseq 3\nseq 10 5 20"},{"path":"커맨드-라인-도구-목록.html","id":"servewd","chapter":"커맨드 라인 도구 목록","heading":"servewd","text":"간단한 HTTP 서버를 사용하여 현재 작업 디렉토리를 서비스합니다.\nservewd (버전 0.1), Jeroen Janssens 제작 (2021).\n더 많은 정보: https://github.com/jeroenjanssens/dsutils.","code":"type servewd\nservewd --help#!enter=FALSE\nC-C#!literal=FALSE\ncd /data && servewd 8000#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"shuf","chapter":"커맨드 라인 도구 목록","heading":"shuf","text":"무작위 순열을 생성합니다.\nshuf (버전 8.30), Paul Eggert 제작 (2019).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type shuf\nman shuf#!enter=FALSE\nC-C#!literal=FALSE\necho {a..z} | tr ' ' '\\n' | shuf | trim 5\nshuf -i 1-100 | trim 5"},{"path":"커맨드-라인-도구-목록.html","id":"skll","chapter":"커맨드 라인 도구 목록","heading":"skll","text":"SciKit-Learn Laboratory입니다.\nskll (버전 2.5.0), Educational Testing Service 제작 (2021).\n실제 도구는 run_experiment입니다. 저는 기억하기 더 쉽기 때문에 별칭 skll을 사용합니다.\n더 많은 정보: https://skll.readthedocs.org.","code":"type skll\nskll --help#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"sort","chapter":"커맨드 라인 도구 목록","heading":"sort","text":"텍스트 파일의 줄을 정렬합니다.\nsort (버전 8.30), Mike Haertel 및 Paul Eggert 제작 (2019).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type sort\nman sort#!enter=FALSE\nC-C#!literal=FALSE\necho '3\\n7\\n1\\n3' | sort"},{"path":"커맨드-라인-도구-목록.html","id":"split","chapter":"커맨드 라인 도구 목록","heading":"split","text":"파일을 여러 조각으로 나눕니다.\nsplit (버전 8.30), Torbjorn Granlund 및 Richard M. Stallman 제작 (2019).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type split\nman split#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"sponge","chapter":"커맨드 라인 도구 목록","heading":"sponge","text":"표준 입력을 흡수하여 파일에 씁니다.\nsponge (버전 0.65), Colin Watson 및 Tollef Fog Heen 제작 (2021).\n단일 파이프라인에서 동일한 파일을 읽고 쓰고 싶을 때 유용합니다.\n더 많은 정보: https://joeyh.name/code/moreutils.","code":"type sponge"},{"path":"커맨드-라인-도구-목록.html","id":"sql2csv","chapter":"커맨드 라인 도구 목록","heading":"sql2csv","text":"데이터베이스에서 SQL 쿼리를 실행하고 결과를 CSV 파일로 출력합니다.\nsql2csv (버전 1.0.5), Christopher Groskopf 제작 (2020).\n더 많은 정보: https://csvkit.rtfd.org.","code":"type sql2csv\nsql2csv --help#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"ssh","chapter":"커맨드 라인 도구 목록","heading":"ssh","text":"OpenSSH 원격 로그인 클라이언트입니다.\nssh (버전 1:8.2p1-4ubuntu0.2), Tatu Ylonen 외 다수 제작 (2020).\n더 많은 정보: https://www.openssh.com.","code":"type ssh\nman ssh#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"sudo","chapter":"커맨드 라인 도구 목록","heading":"sudo","text":"다른 사용자로 명령을 실행합니다.\nsudo (버전 1.8.31), Todd C. Miller 제작 (2019).\n더 많은 정보: https://www.sudo.ws.","code":"type sudo\nman sudo#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"tail","chapter":"커맨드 라인 도구 목록","heading":"tail","text":"파일의 마지막 부분을 출력합니다.\ntail (버전 8.30), Paul Rubin 외 다수 제작 (2019).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type tail\nman tail#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"tapkee","chapter":"커맨드 라인 도구 목록","heading":"tapkee","text":"효율적인 차원 축소 라이브러리입니다.\ntapkee (버전 1.2), Sergey Lisitsyn 외 다수 제작 (2013).\n더 많은 정보: http://tapkee.lisitsyn..","code":"type tapkee\ntapkee --help#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"tar","chapter":"커맨드 라인 도구 목록","heading":"tar","text":"아카이빙 유틸리티입니다.\ntar (버전 1.30), John Gilmore 및 Jay Fenlason 제작 (2014).\n더 많은 정보: https://www.gnu.org/software/tar.","code":"type tar\nman tar#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"tee","chapter":"커맨드 라인 도구 목록","heading":"tee","text":"표준 입력에서 읽어 표준 출력과 파일에 씁니다.\ntee (버전 8.30), Mike Parker 외 다수 제작 (2019).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type tee\nman tee#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"telnet","chapter":"커맨드 라인 도구 목록","heading":"telnet","text":"TELNET 프로토콜에 대한 사용자 인터페이스입니다.\ntelnet (버전 0.17), Mats Erik Andersson 외 다수 제작 (1999).\n더 많은 정보: http://www.hcs.harvard.edu/~dholland/computers/netkit.html.","code":"type telnet"},{"path":"커맨드-라인-도구-목록.html","id":"tldr","chapter":"커맨드 라인 도구 목록","heading":"tldr","text":"콘솔 명령에 대한 협업 치트 시트입니다.\ntldr (버전 3.3.7), Owen Voke 제작 (2021).\n더 많은 정보: https://tldr.sh.","code":"type tldr\ntldr --help#!enter=FALSE\nC-C#!literal=FALSE\ntldr tar | trim"},{"path":"커맨드-라인-도구-목록.html","id":"tr","chapter":"커맨드 라인 도구 목록","heading":"tr","text":"문자를 변환하거나 삭제합니다.\ntr (버전 8.30), Jim Meyering 제작 (2018).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type tr\nman tr#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"tree","chapter":"커맨드 라인 도구 목록","heading":"tree","text":"디렉토리 내용을 트리 형식으로 나열합니다.\ntree (버전 1.8.0), Steve Baker 제작 (2018).\n더 많은 정보: https://launchpad.net/ubuntu/+source/tree.","code":"type tree\nman tree#!enter=FALSE\nC-C#!literal=FALSE\ntree / | trim"},{"path":"커맨드-라인-도구-목록.html","id":"trim","chapter":"커맨드 라인 도구 목록","heading":"trim","text":"출력을 지정된 높이와 너비로 자릅니다.\ntrim Jeroen Janssens 제작 (2021).\n더 많은 정보: https://github.com/jeroenjanssens/dsutils.","code":"type trim\necho {a..z}-{0..9} | fold | trim 5 60"},{"path":"커맨드-라인-도구-목록.html","id":"ts","chapter":"커맨드 라인 도구 목록","heading":"ts","text":"입력에 타임스탬프를 추가합니다.\nts (버전 0.65), Joey Hess 제작 (2021).\n더 많은 정보: https://joeyh.name/code/moreutils.","code":"type ts\necho seq 5 | sample -d 500 | ts"},{"path":"커맨드-라인-도구-목록.html","id":"type","chapter":"커맨드 라인 도구 목록","heading":"type","text":"커맨드 라인 도구의 유형과 위치를 표시합니다.\ntype은 Z shell 빌트인입니다.","code":"type type\nman zshbuiltins | grep -A 10 '^ *type '#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"uniq","chapter":"커맨드 라인 도구 목록","heading":"uniq","text":"중복된 줄을 보고하거나 누락시킵니다.\nuniq (버전 8.30), Richard M. Stallman 및 David MacKenzie 제작 (2019).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type uniq\nman uniq#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"unpack","chapter":"커맨드 라인 도구 목록","heading":"unpack","text":"일반적인 파일 형식의 압축을 해제합니다.\nunpack (버전 0.1), Patrick Brisbin 제작 (2013).\n더 많은 정보: https://github.com/jeroenjanssens/dsutils.","code":"type unpack"},{"path":"커맨드-라인-도구-목록.html","id":"unrar","chapter":"커맨드 라인 도구 목록","heading":"unrar","text":"rar 아카이브에서 파일을 추출합니다.\nunrar (버전 0.0.1), Ben Asselstine 외 다수 제작 (2014).\n더 많은 정보: http://home.gna.org/unrar.","code":"type unrar\nman unrar#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"unzip","chapter":"커맨드 라인 도구 목록","heading":"unzip","text":"ZIP 아카이브의 압축 파일을 나열, 테스트 및 추출합니다.\nunzip (버전 6.0), Samuel H. Smith 외 다수 제작 (2009).\n더 많은 정보: http://www.info-zip.org/pub/infozip.","code":"type unzip\nman unzip#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"vw","chapter":"커맨드 라인 도구 목록","heading":"vw","text":"온라인 학습을 위한 빠른 머신러닝 라이브러리입니다.\nvw (버전 8.10.1), John Langford 제작 (2021).\n더 많은 정보: https://vowpalwabbit.org.","code":"type vw\nvw --help --quiet#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"wc","chapter":"커맨드 라인 도구 목록","heading":"wc","text":"각 파일의 줄 바꿈, 단어 및 바이트 수를 인쇄합니다.\nwc (버전 8.30), Paul Rubin 및 David MacKenzie 제작 (2019).\n더 많은 정보: https://www.gnu.org/software/coreutils.","code":"type wc\nman wc#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"which","chapter":"커맨드 라인 도구 목록","heading":"which","text":"명령어의 위치를 찾습니다.\n(버전 0.1) 제작자 미상 (2016).\n더 많은 정보: .","code":"type which\nman which#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"xml2json","chapter":"커맨드 라인 도구 목록","heading":"xml2json","text":"xml-mapping을 사용하여 XML 입력을 JSON 출력으로 변환합니다.\nxml2json (버전 0.0.3), François Parmentier 제작 (2016).\n더 많은 정보: https://github.com/parmentf/xml2json.","code":"type xml2json"},{"path":"커맨드-라인-도구-목록.html","id":"xmlstarlet","chapter":"커맨드 라인 도구 목록","heading":"xmlstarlet","text":"커맨드 라인용 XML/XSLT 툴킷입니다.\nxmlstarlet (버전 1.6.1), Dagobert Michelsen 외 다수 제작 (2019).\n더 많은 정보: https://sourceforge.net/projects/xmlstar.","code":"type xmlstarlet\nman xmlstarlet#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"xsv","chapter":"커맨드 라인 도구 목록","heading":"xsv","text":"Rust로 작성된 빠른 CSV 커맨드 라인 툴킷입니다.\nxsv (버전 0.13.0), Andrew Gallant 제작 (2018).\n더 많은 정보: https://github.com/BurntSushi/xsv.","code":"type xsv\nxsv --help#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"zcat","chapter":"커맨드 라인 도구 목록","heading":"zcat","text":"파일의 압축을 해제하여 표준 출력으로 연결합니다.\nzcat (버전 1.10), Paul Eggert 제작 (2021).\n더 많은 정보: https://www.nongnu.org/zutils/zutils.html.","code":"type zcat\nman zcat#!enter=FALSE\nC-C#!literal=FALSE"},{"path":"커맨드-라인-도구-목록.html","id":"zsh","chapter":"커맨드 라인 도구 목록","heading":"zsh","text":"Z 쉘입니다.\nzsh (버전 5.8), Paul Falstad 및 Peter Stephenson 제작 (2020).\n더 많은 정보: https://www.zsh.org.","code":"type zsh\nman zsh#!enter=FALSE\nC-C#!literal=FALSE"}]
