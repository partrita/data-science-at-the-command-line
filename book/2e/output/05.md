---
suppress-bibliography: true
---







# 데이터 정제하기 {#chapter-5-scrubbing-data}

두 장 전, 데이터 과학을 위한 OSEMN 모델의 첫 번째 단계에서 우리는 다양한 소스로부터 데이터를 *획득(obtaining)*하는 방법을 살펴보았습니다.
이번 장은 두 번째 단계인 데이터를 *정제(scrubbing)*하는 것에 관한 모든 내용을 다룹니다.
데이터를 얻자마자 즉시 *탐색(exploring)*하거나 *모델링(modeling)*을 계속할 수 있는 경우는 매우 드뭅니다.
데이터에 먼저 약간의 청소, 즉 정제가 필요한 이유는 수없이 많습니다.

우선, 데이터가 원하는 형식이 아닐 수 있습니다.
예를 들어, API에서 JSON 데이터를 얻었지만 시각화를 만들기 위해 CSV 형식이 필요할 수 있습니다.
다른 일반적인 형식으로는 일반 텍스트(plain text), HTML, XML 등이 있습니다.
대부분의 커맨드 라인 도구는 한두 가지 형식만 지원하므로, 데이터를 한 형식에서 다른 형식으로 변환할 수 있는 능력을 갖추는 것이 중요합니다.

데이터가 원하는 형식이 되었더라도 결측치(missing values), 불일치(inconsistencies), 이상한 문자, 또는 불필요한 부분과 같은 문제가 여전히 남아 있을 수 있습니다.
필터를 적용하고, 값을 바꾸고, 여러 파일을 결합함으로써 이를 해결할 수 있습니다.
커맨드 라인은 이러한 종류의 변환에 특히 잘 어울립니다. 사용 가능한 전문 도구가 많고, 그중 대부분은 대량의 데이터를 처리할 수 있기 때문입니다.
이 장에서는 `grep` [@grep]과 `awk` [@awk] 같은 고전적인 도구부터 `jq` [@jq]와 `pup` [@pup] 같은 최신 도구까지 다룰 것입니다.

가끔은 동일한 커맨드 라인 도구를 사용하여 여러 작업을 수행하거나, 여러 도구가 사용하여 동일한 작업을 수행할 수도 있습니다.
이 장은 커맨드 라인 도구 자체를 깊게 파고들기보다는 문제나 레시피(recipe)에 초점을 맞춘 요리책(cookbook)과 같은 구조로 되어 있습니다.


## 개요

<!-- #TODO: SHOULD: Review the list below once the chapter is complete -->

이 장에서 여러분은 다음 내용을 배우게 됩니다.

- 데이터를 한 형식에서 다른 형식으로 변환하기
- CSV에 직접 SQL 쿼리 적용하기
- 행(line) 필터링하기
- 값 추출 및 교체하기
- 열(column) 분할, 병합, 추출하기
- 여러 파일 결합하기

이 장은 다음 파일들로 시작합니다.


``` console
cd /data/ch05
l
```

이 파일들을 얻는 방법은 [2장](#chapter-2-getting-started)에 설명되어 있습니다.
그 외의 파일들은 커맨드 라인 도구를 사용하여 다운로드하거나 생성한 것들입니다.

실제 변환 작업을 살펴보기 전에, 커맨드 라인에서 작업할 때 이러한 변환이 얼마나 도처에 널려 있는지 보여드리고 싶습니다.


## 변환, 어디에나 있는 변환들

[1장](#chapter-1-introduction)에서 실제 데이터 과학의 OSEMN 모델 단계들이 선형적으로만 따라가지는 않는다고 언급했습니다.
이러한 맥락에서, 정제(scrubbing)가 OSEMN 모델의 두 번째 단계이긴 하지만, 단지 획득한 데이터만이 정제가 필요한 것은 아니라는 점을 알아두셨으면 합니다.
이 장에서 배울 변환 기술들은 파이프라인의 어느 부분에서든, 그리고 OSEMN 모델의 어느 단계에서든 유용할 수 있습니다.
일반적으로 한 커맨드 라인 도구가 다음 도구에서 즉시 사용 가능한 출력을 생성한다면, 파이프 연산자(`|`)를 사용하여 두 도구를 연결할 수 있습니다.
그렇지 않다면, 파이프라인 중간에 변환 도구를 삽입하여 데이터에 변환을 먼저 적용해야 합니다.

이를 더 구체적으로 만들기 위해 예시를 하나 들어보겠습니다.
여러분에게 *fizzbuzz* 시퀀스의 처음 100개 항목이 있고([4장](#chapter-4-creating-command-line-tools) 참조), *fizz*, *buzz*, *fizzbuzz*라는 단어가 얼마나 자주 나타나는지 막대 그래프로 시각화하고 싶다고 가정해 봅시다.
아직 익숙하지 않은 도구가 사용되더라도 걱정하지 마세요. 나중에 모두 자세히 다룰 것입니다.

먼저 시퀀스를 생성하여 데이터를 얻고 이를 *fb.seq*에 씁니다.


``` console
seq 100 |
/data/ch04/fizzbuzz.py |
tee fb.seq | trim
```
<1> 사용자 정의 도구인 `fizzbuzz.py`는 [4장](#chapter-4-creating-command-line-tools)에서 가져온 것입니다.

그런 다음 `grep`을 사용하여 *fizz* 또는 *buzz* 패턴과 일치하는 행만 남기고, `sort`와 `uniq` [@uniq]를 사용하여 각 단어의 빈도를 셉니다.


``` console
grep -E "fizz|buzz" fb.seq | # <1>
sort | uniq -c | sort -nr > fb.cnt # <2>
bat -A fb.cnt
```
<1> 이 정규 표현식은 *fizzbuzz*와도 일치합니다.
<2> `sort`와 `uniq`를 이런 식으로 사용하는 것은 행의 개수를 세고 내림차순으로 정렬하는 일반적인 방법입니다. 개수를 추가하는 것은 `-c` 옵션입니다.

`sort`가 두 번 사용된 점에 주목하세요. 첫 번째는 `uniq`가 정렬된 데이터를 입력으로 기대하기 때문이고, 두 번째는 개수를 숫자순으로 정렬하기 위함입니다.
어떤 의미에서 이는 미묘하긴 하지만 중간 변환 과정입니다.

다음 단계는 `rush` [@rush]를 사용하여 빈도를 시각화하는 것입니다.
하지만 `rush`는 입력 데이터가 CSV 형식일 것으로 예상하므로, 그전에 명확한 변환 과정이 필요합니다.
`awk`를 사용하면 헤더를 추가하고 두 필드의 순서를 바꾸고 쉼표를 삽입하는 작업을 한 번에 수행할 수 있습니다.


``` console
< fb.cnt awk 'BEGIN { print "value,count" } { print $2","$1 }' > fb.csv
bat fb.csv
csvlook fb.csv
```

이제 `rush`를 사용하여 막대 그래프를 만들 준비가 되었습니다.
결과는 \@ref(fig:fb-image)를 확인하세요.
(`rush`의 이 구문은 [7장](#chapter-7-exploring-data)에서 자세히 다룰 것입니다.)


``` console
rush plot -x value -y count --geom col --height 2 fb.csv > fb.png
display fb.png
```


이 예제는 약간 인위적이긴 하지만 커맨드 라인에서 작업할 때 흔히 나타나는 패턴을 보여줍니다.
데이터를 획득하거나 시각화하거나 모델을 학습시키는 등의 핵심 도구들은 파이프라인으로 연결되기 위해 중간 변환을 필요로 하는 경우가 많습니다.
그런 의미에서 파이프라인을 작성하는 것은 핵심 조각들이 서로 맞물리기 위해 도우미 조각들을 필요로 하는 퍼즐을 맞추는 것과 같습니다.

이제 데이터 정제의 중요성을 확인했으니, 실제 변환 기술들에 대해 배울 준비가 되었습니다.


## 일반 텍스트 (Plain Text)

엄밀히 말하면, *일반 텍스트*는 사람이 읽을 수 있는 문자들의 시퀀스와 선택적으로 탭이나 줄바꿈 같은 특정 유형의 제어 문자들을 의미합니다[@plaintext].
로그, 전자책, 이메일, 소스 코드 등이 그 예입니다.
일반 텍스트는 바이너리 데이터에 비해 다음과 같은 많은 이점을 가집니다[@pragmaticprogrammer].

- 어떤 텍스트 에디터로도 열고 수정하고 저장할 수 있습니다.
- 자기 기술적(self-describing)이며 이를 생성한 애플리케이션으로부터 독립적입니다.
- 데이터를 처리하는 데 추가적인 지식이나 애플리케이션이 필요하지 않으므로 다른 데이터 형식보다 더 오래 살아남을 것입니다.

하지만 가장 중요한 점은, 유닉스 철학이 일반 텍스트를 커맨드 라인 도구 간의 보편적인 인터페이스로 간주한다는 점입니다[@raymond2003art].
즉, 대부분의 도구는 일반 텍스트를 입력으로 받고 일반 텍스트를 출력으로 내보냅니다.

이것만으로도 제가 일반 텍스트부터 시작할 이유는 충분합니다.
이 장에서 다룰 다른 형식들인 CSV, JSON, XML, HTML 또한 사실 일반 텍스트입니다.
우선은 일반 텍스트가 (CSV처럼) 명확한 표 구조나 (JSON, XML, HTML처럼) 중첩된 구조를 가지고 있지 않다고 가정하겠습니다.
이 장의 뒷부분에서 이러한 형식들을 다루기 위해 특별히 설계된 도구들을 소개하겠습니다.

<!-- #TODO: SHOULD: What to do with this part? -->
<!-- Although the tools in this section can also be applied to these other formats (because they're still text), -->
<!-- keep in mind that the tools treat the data as plain text, and don't interpret the tabular or nested structure. -->
<!-- Sometimes you can get away with this,  -->



### 행 필터링하기

첫 번째 정제 작업은 행을 필터링하는 것입니다.
이는 입력 데이터에서 각 행을 유지할지 버릴지 평가하는 것을 의미합니다.


#### 위치에 기반한 필터링

행을 필터링하는 가장 간단한 방법은 위치에 기반하는 것입니다.
이는 파일의 첫 10행을 검사하고 싶거나, 다른 커맨드 라인 도구의 출력에서 특정 행을 추출하고 싶을 때 유용할 수 있습니다.
위치 기반 필터링을 설명하기 위해 10개의 행을 가진 더미 파일을 만들어 봅시다.


``` console
seq -f "Line %g" 10 | tee lines
```

`head` [@head], `sed` [@sed], 또는 `awk`를 사용하여 처음 3행을 인쇄할 수 있습니다.


``` console
< lines head -n 3
< lines sed -n '1,3p'
< lines awk 'NR <= 3'
```
<1> `awk`에서 *NR*은 지금까지 확인한 총 입력 레코드 수를 나타냅니다.

마찬가지로 `tail` [@tail]을 사용하여 마지막 3행을 인쇄할 수 있습니다.


``` console
< lines tail -n 3
```

이 작업에도 `sed`나 `awk`를 사용할 수 있지만 `tail`이 훨씬 빠릅니다.
처음 3행을 제거하는 방법은 다음과 같습니다.


``` console
< lines tail -n +4
< lines sed '1,3d'
< lines sed -n '1,3!p'
```

`tail`에서는 출력하고 싶은 행 번호에 1을 더해서 지정해야 한다는 점에 유의하세요.
인쇄를 시작하고 싶은 행이라고 생각하면 됩니다.
`head`를 사용하여 마지막 3행을 지울 수도 있습니다.


``` console
< lines head -n -3
```

`sed`, `awk`, 또는 `head`와 `tail`의 조합을 사용하여 특정 행들을 인쇄할 수 있습니다.
여기서는 4, 5, 6행을 인쇄합니다.


``` console
< lines sed -n '4,6p'
< lines awk '(NR>=4) && (NR<=6)'
< lines head -n 6 | tail -n 3
```

`sed`에서 시작 지점과 단계를 지정하거나 `awk`에서 머듈로(modulo) 연산자를 사용하여 홀수 행만 인쇄할 수 있습니다.


``` console
< lines sed -n '1~2p'
< lines awk 'NR%2'
```

짝수 행 인쇄도 비슷한 방식으로 작동합니다.


``` console
< lines sed -n '0~2p'
< lines awk '(NR+1)%2'
```

<!-- #TODO: SHOULD: Mention somewhere in the book that Linux doesn't care about file extensions -->

\BeginKnitrBlock{rmdnote}<div class="rmdnote">이 예제들의 상당수는 작음 기호(`<`) 뒤에 파일 이름을 붙여서 시작합니다.
이렇게 하면 파이프라인을 왼쪽에서 오른쪽으로 읽을 수 있기 때문입니다.
이는 제 개인적인 선호임을 알아두세요.
파일의 내용을 파이프로 넘기기 위해 `cat`을 사용할 수도 있습니다.
또한, 많은 커맨드 라인 도구는 파일 이름을 인자로 직접 받기도 합니다.</div>\EndKnitrBlock{rmdnote}


#### 패턴에 기반한 필터링

가끔은 내용에 따라 행을 유지하거나 버리고 싶을 때가 있습니다.
행을 필터링하는 표준적인 커맨드 라인 도구인 `grep`을 사용하면 특정 패턴이나 정규 표현식과 일치하는 모든 행을 인쇄할 수 있습니다.
예를 들어, *이상한 나라의 앨리스*에서 모든 장(chapter)의 제목을 추출해 보겠습니다.


``` console
< alice.txt grep -i chapter
```
<1> `-i` 옵션은 대소문자를 구분하지 않고 매칭하도록 지정합니다.

정규 표현식을 지정할 수도 있습니다.
예를 들어, *The*로 시작하는 제목만 인쇄하고 싶다면 다음과 같이 합니다.


``` console
< alice.txt grep -E '^CHAPTER (.*)\. The'
```

정규 표현션을 활성화하려면 `-E` 옵션을 지정해야 한다는 점에 유의하세요.
그렇지 않으면 `grep`은 패턴을 리터럴 문자열로 해석하여 검색 결과가 나오지 않을 가능성이 큽니다.


``` console
< alice.txt grep '^CHAPTER (.*)\. The'
```

`-v` 옵션을 사용하면 일치 항목을 반전시켜서 패턴과 일치하지 *않는* 행들을 인쇄합니다.
아래의 정규 표현식은 공백 문자만 포함된 행과 일치합니다.
따라서 이를 반전 시키고 `wc -l`을 사용하면 비어있지 않은 행의 개수를 셀 수 있습니다.


``` console
< alice.txt grep -Ev '^\s$' | wc -l
```


#### 무작위성에 기반한 필터링

데이터 파이프라인을 구성하는 과정에서 데이터 양이 아주 많다면 파이프라인을 디버깅하는 것이 번거로울 수 있습니다.
그럴 때 데이터에서 더 작은 샘플을 생성하는 것이 유용할 수 있습니다.
이때 `sample` [@sample]이 요긴하게 쓰입니다.
`sample`의 주요 목적은 입력 데이터에서 각 행마다 일정 비율만큼만 출력하여 데이터의 부분 집합을 얻는 것입니다.





``` console
seq -f "Line %g" 1000 | sample -r 1%
```

여기서는 모든 입력 행이 인쇄될 확률이 1%입니다.
이 백분율은 분수(예: `1/100`)나 확률(예: `0.01`)로도 지정할 수 있습니다.

`sample`은 파이프라인을 디버깅할 때 유용한 두 가지 다른 용도가 있습니다.
첫째, 출력에 약간의 지연을 추가할 수 있습니다.
이는 입력이 지속적인 스트림일 때(예: [3장](#chapter-3-obtaining-data)에서 본 위키피디아 스트림), 데이터가 너무 빨리 들어와서 무슨 일이 일어나고 있는지 확인하기 어려울 때 유용합니다.
둘째, `sample`에 타이머를 설정하여 진행 중인 프로세스를 수동으로 종료할 필요가 없게 할 수 있습니다.
예를 들어, 각 행이 인쇄될 때 1초의 지연을 추가하고 5초 동안만 실행하려면 다음과 같이 입력합니다.


``` console
seq -f "Line %g" 1000 | sample -r 1% -d 1000 -s 5 | ts
```
<1> `ts`[@ts] 도구는 각 행의 맨 앞에 타임스탬프를 추가합니다.

불필요한 계산을 방지하기 위해 파이프라인에서 가능한 한 앞부분에 `sample`을 배치하도록 하세요.
사실 이 논리는 `head`나 `tail`처럼 데이터를 줄여주는 모든 커맨드 라인 도구에 적용됩니다.
파이프라인이 제대로 작동한다는 확신이 들면 파이프라인에서 이를 제거하면 됩니다.


### 값 추출하기

앞의 예제에서 실제 장 제목을 추출하려면, `grep`의 출력을 `cut`으로 파이핑하는 간단한 접근 방식을 사용할 수 있습니다.


``` console
grep -i chapter alice.txt | cut -d ' ' -f 3-
```

여기서 `cut`으로 전달된 각 행은 공백을 기준으로 필드(field)로 나뉘고, 세 번째 필드부터 마지막 필드까지 인쇄됩니다.
필드의 총 개수는 입력 행마다 다를 수 있습니다.
`sed`를 사용하면 훨씬 더 복잡한 방식으로 동일한 작업을 수행할 수 있습니다.


``` console
sed -rn 's/^CHAPTER ([IVXLCDM]{1,})\. (.*)$/\2/p' alice.txt | trim 3
```

(출력이 동일하므로 3행으로 잘랐습니다.) 이 방식은 정규 표현식과 후방 참조(back reference)를 사용합니다.
여기서 `sed`는 `grep`이 하던 작업까지 도맡았습니다.
더 간단한 방법으로는 해결되지 않을 정도로 복잡한 상황에서만 이 방식을 사용하는 것을 권장합니다.
예를 들어, *chapter*라는 단어가 단순히 새 장의 시작을 알리는 용도뿐만 아니라 텍스트 자체의 일부로 쓰인 경우가 있을 때 그렇습니다.
물론 이를 해결할 수 있는 다양한 수준의 복잡한 방법이 있겠지만, 여기서는 매우 엄격한 접근 방식을 예로 보여 드린 것입니다.
실제로 중요한 과제는 복잡성과 유연성 사이에서 균형을 잘 잡는 파이프라인을 고안하는 것입니다.

`cut`은 문자 위치를 기준으로도 나눌 수 있다는 점을 언급할 가치가 있습니다.
이는 입력 행마다 동일한 위치의 문자 집합을 추출(또는 제거)하고 싶을 때 유용합니다.


``` console
grep -i chapter alice.txt | cut -c 9-
```

`grep`은 `-o` 옵션을 사용하여 모든 일치 항목을 별도의 행으로 출력하는 훌륭한 기능을 가지고 있습니다.


``` console
< alice.txt grep -oE '\w{2,}' | trim
```

그렇다면 *a*로 시작하고 *e*로 끝나는 모든 단어의 데이터셋을 만들고 싶다면 어떻게 해야 할까요?
당연히 이를 위한 파이프라인도 있습니다.


``` console
< alice.txt tr '[:upper:]' '[:lower:]' |
grep -oE '\w{2,}' |
grep -E '^a.*e$' |
sort | uniq | sort -nr | trim
```
<1> 여기서 텍스트를 소문자로 만들기 위해 `tr`을 사용합니다. 다음 절에서 `tr`을 더 자세히 살펴보겠습니다.

두 개의 `grep` 명령어를 하나로 합칠 수도 있었겠지만, 이 경우에는 이전 파이프라인을 재사용하고 조정하는 것이 더 쉽다고 판단했습니다.
일을 완수하기 위해 실용적인 태도를 취하는 것은 전혀 부끄러운 일이 아닙니다!


### 값 교체 및 삭제하기

*반전(translate)*의 약자인 커맨드 라인 도구 `tr` [@tr]을 사용하여 개별 문자를 교체하거나 삭제할 수 있습니다.
예를 들어, 공백을 다음과 같이 언더스코어로 바꿀 수 있습니다.


``` console
echo 'hello world!' | tr ' ' '_'
```

둘 이상의 문자를 교체해야 한다면 다음과 같이 조합할 수 있습니다.


``` console
echo 'hello world!' | tr ' !' '_?'
```

`tr`에 `-d` 인자를 지정하여 개별 문자를 삭제할 수도 있습니다.


``` console
echo 'hello world!' | tr -d ' !'
echo 'hello world!' | tr -d -c '[a-z]'
```

이 경우 위 두 명령어는 동일한 결과를 냅니다.
하지만 두 번째 명령어는 두 가지 추가 기능을 사용합니다.
대괄호와 하이픈(`[-]`)을 사용하여 문자 *범위*(모든 소문자)를 지정했고, `-c` 옵션은 그에 대한 보집합(complement)을 사용함을 나타냅니다.
즉, 이 명령어는 소문자만 남깁니다.
`tr`을 사용하여 텍스트를 대문자로 변환할 수도 있습니다.


``` console
echo 'hello world!' | tr '[a-z]' '[A-Z]'
echo 'hello world!' | tr '[:lower:]' '[:upper:]'
```

하지만 ASCII가 아닌 문자를 번역해야 한다면 `tr`이 작동하지 않을 수 있습니다. `tr`은 단일 바이트 문자에서만 작동하기 때문입니다. 그런 경우에는 대신 `sed`를 사용해야 합니다.


``` console
echo 'hello world!' | tr '[a-z]' '[A-Z]'
echo 'hallo wêreld!' | tr '[a-z]' '[A-Z]'
echo 'hallo wêreld!' | tr '[:lower:]' '[:upper:]'
echo 'hallo wêreld!' | sed 's/[[:lower:]]*/\U&/g'
echo 'helló világ' | tr '[:lower:]' '[:upper:]'
echo 'helló világ' | sed 's/[[:lower:]]*/\U&/g'
```

<!-- #TODO: SHOULD: Give a proper intro about sed -->

개별 문자 이상의 것을 조작해야 한다면 `sed`가 유용하다는 사실을 알게 될 것입니다.
이미 *alice.txt*에서 장 제목을 추출하는 `sed` 예제를 보셨습니다.
추출, 삭제, 교체는 사실 `sed`에서 모두 동일한 작업입니다.
단지 서로 다른 정규 표현식을 지정할 뿐입니다.
예를 들어, 단어를 바꾸거나 반복된 공백을 제거하고 앞부분의 공백을 제거하려면 다음과 같이 합니다.


``` console
echo ' hello     world!' |
sed -re 's/hello/bye/' |
sed -re 's/\s+/ /g' |
sed -re 's/\s+//'
```
<1> *hello*를 *bye*로 바꿉니다.
<2> 임의의 공백을 하나의 공백으로 바꿉니다. 플래그 `g`는 글로벌(global)의 약자로, 동일한 행에서 동일한 치환을 여러 번 적용할 수 있음을 의미합니다.
<3> 여기서는 `g` 플래그를 지정하지 않았으므로 맨 앞의 공백만 제거합니다.

이전의 `grep` 예제와 마찬가지로, 이 세 개의 `sed` 명령어는 하나로 합쳐질 수 있습니다.


``` console
echo ' hello     world!' |
sed -re 's/hello/bye/;s/\s+/ /g;s/\s+//'
```
글쎄요, 여러분은 어느 쪽이 읽기 더 편하신가요?


## CSV



<!-- #TODO: MUST: ### Canonical Format -->

<!-- The `csvkit` documentation[@csvkit] opens with: [...] CSV, the king of tabular file formats. -->

<!-- Canonical format. The format that we want to convert to. -->
<!-- Two properties. it's plain text and it has a rectangular shape, meaning in consists of rows and columns. -->

<!-- As a format, -->

<!-- As a shape, -->

<!-- Even kings have their flaws. -->
<!-- CSV definitely has its flaws. No meta data. No rules regarding quoting, the delimiter, header. -->
<!-- Writing a robust CSV parser is really hard. Nevertheless, CSV has many advantages. -->

<!-- CSV, which is the main format I'll be working with in this chapter, is actually not the easiest format to work with. Many CSV datasets are broken or incompatible with each other because there is no standard syntax, unlike XML and JSON. -->

<!-- Export from Database, spreadsheets, rectangular. -->


<!-- - It's still plain text, so can be read, edited, and even created in any text editor. -->
<!-- - CSV can be imported by many programming languages such as Python, R, and JavaScript and many software such as Excel, Tableau, and Power BI. -->
<!-- - Related to the previous point, but worth mentioning separately: a CSV directly translates to a data frame in Python (with the pandas package), R, and Julia. This means that can immediately continue scrubbing, exploring, and modeling in those languages. -->
<!-- - Speaking of modeling, most machine learning algorithms expect data to be in a rectangular format, or more precisely, a matrix of numerical values where each row is a data point and each column is a feature. More on this in Chapter 9. -->



<!-- makes assumptions, same number of fields per row. -->
<!-- flat. not nested. -->
<!-- CVS is not nested structure -->
<!-- If you build an API, use JSON -->
<!-- If you -->
<!-- You rarely have to covert CSV to anything else when you're doing data analysis. -->


### 본문(Bodies)과 헤더(Headers)와 열(Columns)

일반 텍스트를 정제하기 위해 사용했던 `tr`이나 `grep` 같은 커맨드 라인 도구가 CSV에 항상 적용될 수 있는 것은 아닙니다.
그 이유는 이러한 커맨드 라인 도구들이 헤더, 본문, 열에 대한 개념이 없기 때문입니다.
만약 `grep`을 사용하여 행을 필터링하면서 항상 출력에 헤더를 포함하고 싶다면 어떻게 해야 할까요?
혹은 `tr`을 사용하여 다른 열은 건드리지 않고 특정 열의 값만 대문자로 바꾸고 싶다면 어떻게 해야 할까요?

이를 위한 여러 단계의 우회 방법이 있긴 하지만 매우 번거롭습니다.
더 좋은 방법이 있습니다.
일반적인 커맨드 라인 도구를 CSV에 활용하기 위해 `body` [@body], `header` [@header], `cols` [@cols]라는 이름의 세 가지 도구를 소개하겠습니다.

첫 번째 도구인 `body`부터 시작해 보겠습니다.
`body`를 사용하면 CSV 파일의 헤더를 제외한 나머지 부분, 즉 본문에 임의의 커맨드 라인 도구를 적용할 수 있습니다.
예를 들어 다음과 같습니다.


``` console
echo -e "value\n7\n2\n5\n3" | body sort -n
```

이 도구는 CSV 파일의 헤더가 한 줄이라고 가정합니다.
작동 방식은 다음과 같습니다.

- 표준 입력에서 한 줄을 가져와 *\$header*라는 변수에 저장합니다.
- 헤더를 출력합니다.
- 표준 입력에 남은 데이터에 대해 `body`에 전달된 모든 커맨드 라인 인자를 실행합니다.

또 다른 예입니다.
다음 CSV 파일의 행 개수를 세고 싶다고 가정해 봅시다.


``` console
seq 5 | header -a count
```

`wc -l`을 사용하면 모든 행의 개수를 셀 수 있습니다.


``` console
seq 5 | header -a count | wc -l
```

본문의 행만 고려하고 싶다면(즉, 헤더를 제외한 모든 것), `body`를 추가합니다.


``` console
seq 5 | header -a count | body wc -l
```

헤더는 계산에 포함되지 않으며 출력에 다시 인쇄됩니다.

두 번째 도구인 `header`를 사용하면 CSV 파일의 헤더를 조작할 수 있습니다.
인자를 제공하지 않으면 CSV 파일의 헤더가 인쇄됩니다.


``` console
< tips.csv header
```

이는 `head -n 1`과 같습니다.
헤더가 두 줄 이상인 경우(권장되지는 않음) `-n 2`를 지정할 수 있습니다.
CSV 파일에 헤더를 추가할 수도 있습니다.


``` console
seq 5 | header -a count
```

이는 `echo "count" | cat - <(seq 5)`와 동일합니다.
헤더 삭제는 `-d` 옵션으로 수행합니다.


``` console
< iris.csv header -d | trim
```

이는 `tail -n +2`와 비슷하지만 기억하기 조금 더 쉽습니다.
헤더 교체는 `-r` 옵션으로 수행합니다. 이는 내부적으로 헤더를 먼저 삭제한 다음 추가하는 방식입니다. 여기서는 `body`와 결합해 보겠습니다.


``` console
seq 5 | header -a line | body wc -l | header -r count
```

마지막으로, `body` 도구가 본문에 하는 것처럼 헤더에만 명령을 적용할 수 있습니다.
예를 들어 다음과 같습니다.


``` console
seq 5 | header -a line | header -e "tr '[a-z]' '[A-Z]'"
```

세 번째 도구는 `cols`로, 특정 열의 부분 집합에만 명령을 적용할 수 있게 해줍니다.
예를 들어, tips 데이터셋에서 다른 열과 헤더는 건드리지 않고 day 열의 값만 대문자로 바꾸고 싶다면 다음과 같이 `cols`와 `body`를 조합하여 사용합니다.


``` console
< tips.csv cols -c day body "tr '[a-z]' '[A-Z]'" | head -n 5 | csvlook
```

`header -e`, `body`, `cols`에 여러 커맨드 라인 도구와 인자를 명령어로 전달할 때 따옴표 처리가 까다로울 수 있다는 점에 유의하세요.
이런 문제가 발생하면 별도의 커맨드 라인 도구를 만들어서 그 명령어를 전달하는 것이 가장 좋습니다.

결론적으로, CSV 데이터를 위해 특별히 제작된 커맨드 라인 도구를 사용하는 것이 일반적으로 바람직하지만, `body`, `header`, `cols`를 사용하면 필요한 경우 고전적인 커맨드 라인 도구들을 CSV 파일에도 적용할 수 있습니다.


### CSV에서 SQL 쿼리 수행하기

만약 이번 장에서 언급된 커맨드 라인 도구들이 충분한 유연성을 제공하지 못한다면, 커맨드 라인에서 데이터를 정제하는 또 다른 접근 방식이 있습니다.
`csvsql` [@csvsql] 도구는 CSV 파일에서 직접 SQL 쿼리를 실행할 수 있게 해줍니다.
SQL은 데이터 정제 작업을 정의하는 강력한 언어이며, 개별 커맨드 라인 도구를 사용하는 것과는 매우 다른 방식입니다.

\BeginKnitrBlock{rmdnote}<div class="rmdnote">만약 데이터가 원래 관계형 데이터베이스에서 온 것이라면, 가능하면 그 데이터베이스에서 SQL 쿼리를 실행한 다음 결과를 CSV로 추출하는 것이 좋습니다. [3장](#chapter-3-obtaining-data)에서 논의했듯이 이를 위해 `sql2csv` 도구를 사용할 수 있습니다. 데이터베이스에서 CSV 파일로 먼저 내보낸 다음 다시 SQL을 적용하는 것은 속도가 느릴 뿐만 아니라, CSV 데이터로부터 열 유형(column types)이 올바르게 유추되지 않을 가능성도 있기 때문입니다.</div>\EndKnitrBlock{rmdnote}

아래의 정제 작업들에는 `csvsql`을 포함한 몇 가지 솔루션을 소개하겠습니다. 기본적인 명령어는 다음과 같습니다.


``` console
seq 5 | header -a val | csvsql --query "SELECT SUM(val) AS sum FROM stdin"
```

표준 입력을 `csvsql`로 전달하면 테이블 이름은 *stdin*이 됩니다.
열의 유형은 데이터로부터 자동으로 유추됩니다.
나중에 CSV 파일 결합 섹션에서 보겠지만, 여러 개의 CSV 파일을 지정할 수도 있습니다.
<!-- #TODO: MUST: Reference SQLite -->
`csvsql`은 SQL 표준과 약간의 차이가 있는 SQLite 방식을 사용한다는 점을 유념하세요.
SQL은 일반적으로 다른 솔루션보다 장황하긴 하지만 훨씬 더 유연합니다.
만약 이미 SQL로 정제 문제를 해결하는 방법을 알고 있다면 커맨드 라인에서도 이를 사용하지 않을 이유가 없습니다.


### 열 추출 및 재정렬하기

<!-- #TODO: MUST: Replace csvcut with xsv select -->

`csvcut` [@csvcut] 도구를 사용하여 열을 추출하고 재정렬할 수 있습니다.
예를 들어, Iris 데이터셋에서 숫자 값이 포함된 열만 남기고 가운데 두 열의 순서를 바꾸려면 다음과 같이 합니다.


``` console
< iris.csv csvcut -c sepal_length,petal_length,sepal_width,petal_width | csvlook
```

또는 `-C`(complement) 옵션을 사용하여 제외할 열을 지정할 수도 있습니다.


``` console
< iris.csv csvcut -C species | csvlook
```

이 경우 포함된 열들은 원래 순서를 유지합니다.
열 이름 대신 1부터 시작하는 열 인덱스를 지정할 수도 있습니다.
예를 들어, 홀수 번째 열만 선택하는 것도 가능합니다.


``` console
echo 'a,b,c,d,e,f,g,h,i\n1,2,3,4,5,6,7,8,9' |
csvcut -c $(seq 1 2 9 | paste -sd,)
```

만약 어떤 값에도 쉼표가 포함되어 있지 않다는 확신이 있다면 `cut`을 사용하여 열을 추출할 수도 있습니다.
하지만 `cut`은 다음 명령어에서 볼 수 있듯이 열의 순서를 바꾸지 못한다는 점에 주의하세요.


``` console
echo 'a,b,c,d,e,f,g,h,i\n1,2,3,4,5,6,7,8,9' | cut -d, -f 5,1,3
```

보시다시피 `-f` 옵션으로 열을 어떤 순서로 지정하든 상관없습니다. `cut`을 사용하면 항상 원래 순서대로 나타납니다.
완전성을 위해, Iris 데이터셋의 숫자 열을 추출하고 재정렬하는 SQL 방식도 살펴보겠습니다.


``` console
< iris.csv csvsql --query "SELECT sepal_length, petal_length, "\
"sepal_width, petal_width FROM stdin" | head -n 5 | csvlook
```

### 행 필터링하기

CSV 파일에서 행(rows)을 필터링하는 것이 일반 텍스트 파일에서 행(lines)을 필터링하는 것과 다른 점은, 특정 열의 값에만 기반하여 필터링하고 싶을 수 있다는 것입니다.
위치 기반 필터링은 기본적으로 동일하지만, CSV 파일의 첫 번째 행이 대개 헤더라는 점을 고려해야 합니다.
헤더를 유지하고 싶다면 항상 `body` 도구를 사용할 수 있다는 점을 기억하세요.

<!-- #TODO: #MUST use xsv slice and xsv search -->


``` console
seq 5 | sed -n '3,5p'
seq 5 | header -a count | body sed -n '3,5p'
```

특정 열 내의 특정 패턴에 따라 필터링하려는 경우 `csvgrep` [@csvgrep], `awk`, 또는 `csvsql`을 사용할 수 있습니다.
예를 들어, 일행의 수(party size)가 5명 미만인 모든 영수증을 제외하려면 다음과 같이 합니다.


``` console
csvgrep -c size -i -r "[1-4]" tips.csv
```

`awk`와 `csvsql`은 숫자 비교도 수행할 수 있습니다.
예를 들어, 토요일이나 일요일에 40달러가 넘는 모든 영수증을 가져오려면 다음과 같이 합니다.


``` console
< tips.csv awk -F, 'NR==1 || ($1 > 40.0) && ($5 ~ /^S/)'
```

`csvsql` 솔루션은 더 장황하지만 인덱스 대신 열 이름을 사용하므로 더 견고합니다.


``` console
csvsql --query "SELECT * FROM tips WHERE bill > 40 AND day LIKE 'S%'" tips.csv
```

SQL의 *WHERE* 절은 날짜나 집합에 대해 작동할 수 있고 복잡한 절의 조합을 형성할 수 있기 때문에, 그 유연성은 다른 커맨드 라인 도구들이 쉽게 따라가기 어렵습니다.


### 열 병합하기

열 병합은 관심 있는 값이 여러 열에 분산되어 있을 때 유용합니다.
날짜(연, 월, 일이 별도의 열인 경우)나 이름(이름과 성이 별도의 열인 경우)에서 이런 일이 발생할 수 있습니다.
두 번째 상황을 살펴보겠습니다.

<!-- #TODO: Must use composers.csv -->

입력 CSV는 작곡가 목록입니다.
여러분의 과제는 이름(first name)과 성(last name)을 합쳐서 전체 이름(full name)을 만드는 것이라고 가정해 봅시다.
이 과제를 해결하기 위해 `sed`, `awk`, `cols` + `tr`, `csvsql`의 네 가지 접근 방식을 제시하겠습니다.
먼저 입력 CSV를 살펴보겠습니다.


``` console
csvlook -I names.csv
```

첫 번째 방식인 `sed`는 두 개의 문(statement)을 사용합니다.
첫 번째는 헤더를 교체하는 것이고, 두 번째는 두 번째 행부터 적용되는 후방 참조를 포함한 정규 표현식입니다.


``` console
< names.csv sed -re '1s/.*/id,full_name,born/g;2,$s/(.*),(.*),(.*),(.*)/\1,\3 \2,\4/g' |
csvlook -I
```

`awk` 접근 방식은 다음과 같습니다.


``` console
< names.csv awk -F, 'BEGIN{OFS=","; print "id,full_name,born"} {if(NR > 1) {print $1,$3" "$2,$4}}' |
csvlook -I
```

`tr`과 결합한 `cols` 방식입니다.


``` console
< names.csv |
cols -c first_name,last_name tr \",\" \" \" |
header -r full_name,id,born |
csvcut -c id,full_name,born |
csvlook -I
```

`csvsql`은 쿼리를 실행하기 위해 SQLite 데이터베이스를 사용하며, `||`는 문자열 연결(concatenation)을 의미한다는 점에 유의하세요.


``` console
< names.csv csvsql --query "SELECT id, first_name || ' ' || last_name "\
"AS full_name, born FROM stdin" | csvlook -I
```

만약 *last\_name*에 쉼표가 포함되어 있다면 어떻게 될까요? 명확성을 위해 가공되지 않은 입력 CSV를 살펴보겠습니다.


<!-- Ludwig van Beethoven enters the party -->
<!-- ```{console} -->
<!-- echo 'Ludwig,"Beethoven, van",1770,"Bonn,\nGermany"' >> composers.csv -->
<!-- ``` -->
<!-- bat -A composers.csv -->
<!-- //# cat composers.csv | csvquote | tr -d $'\x1f'   # comma (unit separator in unicode) -->
<!-- //# cat composers.csv | csvquote | tr $'\x1e' ' '  # new line (record separator in unicode) -->


``` console
cat names-comma.csv
```

글쎄요, 처음 세 가지 방식은 모두 각기 다른 이유로 실패하는 것 같군요. 오직 `csvsql`만이 first\_name과 full\_name을 제대로 결합할 수 있습니다.


``` console
< names-comma.csv sed -re '1s/.*/id,full_name,born/g;2,$s/(.*),(.*),(.*),(.*)/\1,\3 \2,\4/g' | tail -n 1
```


``` console
< names-comma.csv awk -F, 'BEGIN{OFS=","; print "id,full_name,born"} {if(NR > 1) {print $1,$3" "$2,$4}}' | tail -n 1
```


``` console
< names-comma.csv | cols -c first_name,last_name tr \",\" \" \" |
header -r full_name,id,born | csvcut -c id,full_name,born | tail -n 1
```


``` console
< names-comma.csv csvsql --query "SELECT id, first_name || ' ' || last_name AS full_name, born FROM stdin" | tail -n 1
```


``` console
< names-comma.csv rush run -t 'unite(df, full_name, first_name, last_name, sep = " ")' - | tail -n 1
```

잠깐만요! 저 마지막 명령어는 무엇인가요? R인가요? 사실 그렇습니다.
`rush`라는 커맨드 라인 도구를 통해 실행된 R 코드입니다. 지금 말씀드릴 수 있는 것은 이 방식 또한 두 개의 열을 병합하는 데 성공했다는 점입니다.
이 멋진 도구에 대해서는 나중에 다시 다루겠습니다.


<!-- ```{console} -->
<!-- shuf -ri 0-9999 -n 24 | nl -v0 -w1 -s, | header -a hour,value > 2021-01-01.csv -->
<!-- shuf -ri 0-9999 -n 24 | nl -v0 -w1 -s, | header -a hour,value > 2021-01-02.csv -->
<!-- shuf -ri 0-9999 -n 24 | nl -v0 -w1 -s, | header -a hour,value > 2021-01-03.csv -->
<!-- head -n 3 2021-*.csv -->
<!-- ``` -->

<!-- ```{console} -->
<!-- csvstack --group-name date --filenames 2021-*.csv | sed 's/\.csv,/,/' | csvlook -->
<!-- ``` -->

<!-- ```{console} -->
<!-- awk 'NR==1 {H=$0; print "date,"$0} $0!=H {print FILENAME","$0}' 2021-01-0*.csv | csvlook -->
<!-- ``` -->


#### 가로로 결합하기

나란히 붙이고 싶은 세 개의 CSV 파일이 있다고 가정해 봅시다. 파이프라인 중간에 `csvcut`의 결과를 저장하기 위해 `tee` [@tee]를 사용합니다.


``` console
< tips.csv csvcut -c bill,tip | tee bills.csv | head -n 3 | csvlook
< tips.csv csvcut -c day,time | tee datetime.csv |
head -n 3 | csvlook -I
< tips.csv csvcut -c sex,smoker,size | tee customers.csv |
head -n 3 | csvlook
```

행들이 서로 일치한다고 가정하면, `paste` [@paste]를 사용하여 파일들을 하나로 합칠 수 있습니다.


``` console
paste -d, {bills,customers,datetime}.csv | head -n 3 | csvlook -I
```

여기서 커맨드 라인 인자 `-d`는 `paste`가 구분자로 쉼표를 사용하도록 지시합니다.


#### 결합(Joining)

가끔은 데이터를 세로로나 가로로 단순히 이어 붙이는 것만으로는 합칠 수 없습니다.
특히 관계형 데이터베이스의 경우, 중복을 최소화하기 위해 데이터가 여러 테이블(또는 파일)에 분산되어 있는 경우가 있습니다.
Iris 데이터셋에 세 가지 붓꽃 종류에 대한 더 많은 정보(예: USDA 식별자)를 추가하여 확장하고 싶다고 가정해 봅시다.
마침 이러한 식별자가 포함된 별도의 CSV 파일이 있습니다.


``` console
csvlook irismeta.csv
```

이 데이터셋과 Iris 데이터셋의 공통점은 *species* 열입니다.
`csvjoin` [@csvjoin]을 사용하여 두 데이터셋을 결합할 수 있습니다.


``` console
csvjoin -c species iris.csv irismeta.csv | csvcut -c sepal_length,sepal_width,species,usda_id | sed -n '1p;49,54p' | csvlook
```

물론 `csvsql`을 사용하여 SQL 방식으로 접근할 수도 있습니다. 평소와 같이 조금 더 길지만, 잠재적으로 훨씬 더 유연합니다.


``` console
csvsql --query 'SELECT i.sepal_length, i.sepal_width, i.species, m.usda_id FROM iris i JOIN irismeta m ON (i.species = m.species)' iris.csv irismeta.csv | sed -n '1p;49,54p' | csvlook
```

## XML/HTML 및 JSON 작업하기

이 섹션에서는 데이터를 한 형식에서 다른 형식으로 변환할 수 있는 몇 가지 커맨드 라인 도구를 시연하겠습니다.
데이터를 변환하는 데에는 두 가지 이유가 있습니다.

첫째, 많은 시각화 및 머신러닝 알고리즘이 표 형식의 데이터를 요구하기 때문에, 데이터를 데이터베이스 테이블이나 스프레드시트와 같은 표 형태로 만들어야 하는 경우가 많습니다.
CSV는 본래 표 형식이지만, JSON과 HTML/XML 데이터는 깊게 중첩된 구조를 가질 수 있습니다.

둘째, `cut`이나 `grep` 같은 고전적인 커맨드 라인 도구들은 대부분 일반 텍스트를 대상으로 작동합니다.
텍스트는 커맨드 라인 도구 간의 보편적인 인터페이스로 간주되기 때문입니다.
게다가 다른 형식들은 비교적 나중에 등장했습니다. 이러한 각 형식을 일반 텍스트로 취급하면 고전적인 커맨드 라인 도구들을 다른 형식에도 적용할 수 있습니다.

가끔은 구조화된 데이터에 고전적인 도구들을 적용하여 문제를 해결할 수 있습니다.
예를 들어, 아래의 JSON 데이터를 일반 텍스트로 취급하여 `sed`를 사용해 *gender* 속성을 *sex*로 바꿀 수 있습니다.


``` console
sed -e 's/"gender":/"sex":/g' users.json | jq | trim
```

다른 많은 커맨드 라인 도구와 마찬가지로 `sed`는 데이터의 구조를 활용하지 않습니다.
구조를 활용하는 도구(아래에서 설명할 `jq` 등)를 사용하거나, 데이터를 먼저 CSV와 같은 표 형식으로 변환한 다음 적절한 커맨드 라인 도구를 적용하는 것이 더 좋습니다.

실제 사용 사례를 통해 XML/HTML 및 JSON을 CSV로 변환하는 과정을 보여드리겠습니다.
여기서 사용할 커맨드 라인 도구는 `curl`, `pup` [@pup], `xml2json` [@xml2json], `jq`, `json2csv` [@json2csv]입니다.

위키피디아에는 방대한 정보가 담겨 있습니다. 이 정보의 상당 부분은 표(table)로 정리되어 있으며, 이는 데이터셋으로 간주될 수 있습니다.
예를 들어, [이 페이지](http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_border/area_ratio)에는 국가 및 영토 목록과 함께 국경 길이, 면적, 그리고 그 둘 사이의 비율이 포함되어 있습니다.

이 데이터를 분석하는 데 관심이 있다고 가정해 봅시다. 이 섹션에서는 필요한 모든 단계와 그에 해당하는 명령어를 안내해 드리겠습니다. 세세한 부분까지 모두 설명하지는 않으므로 모든 것을 즉시 이해하지 못할 수도 있습니다. 걱정하지 마세요. 핵심 내용은 파악하실 수 있을 것입니다. 이 섹션의 목적은 커맨드 라인의 활용 능력을 보여주는 것임을 기억하세요. 이 섹션에서 사용된 모든 도구와 개념(그 이상)은 이후 장에서 설명될 것입니다.

관심 있는 데이터셋은 HTML에 포함되어 있습니다.
여러분의 목표는 작업하기 편한 형태의 데이터셋 표현을 얻는 것입니다.
가장 먼저 할 일은 `curl`을 사용하여 HTML을 다운로드하는 것입니다.


``` console
curl -sL 'http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_border/area_ratio' > wiki.html
```

HTML은 *wiki.html*이라는 파일로 저장됩니다.
처음 10행이 어떻게 생겼는지 확인해 봅시다.


``` console
< wiki.html trim
```

제대로 된 것 같습니다.
관심 있는 루트 HTML 요소가 *wikitable* 클래스를 가진 *&lt;table&gt;*이라는 것을 알아냈다고 가정해 봅시다.
그러면 `grep`을 사용하여 관심 있는 부분을 살펴볼 수 있습니다(`-A` 옵션은 일치하는 행 이후에 인쇄할 행의 수를 지정합니다).


``` console
grep wikitable -A 21 wiki.html
```

이제 실제 국가들과 그 값들이 보입니다.
다음 단계는 HTML 파일에서 필요한 요소를 추출하는 것입니다.
이를 위해 `pup`을 사용할 수 있습니다.


``` console
< wiki.html pup 'table.wikitable tbody' | tee table.html | trim
```

`pup`에 전달된 표현식은 CSS 선택자(selector)입니다.
이 구문은 보통 웹 페이지의 스타일을 지정하는 데 사용되지만, HTML에서 특정 요소를 선택하는 데에도 사용할 수 있습니다.
이 경우 *wikitable* 클래스를 가진 *table*의 *tbody*를 선택하려고 합니다.
그다음은 XML(및 HTML)을 JSON으로 변환하는 `xml2json`입니다.


``` console
< table.html xml2json > table.json
jq . table.json | trim 20
```

HTML을 JSON으로 변환하는 이유는 JSON 데이터를 처리하는 매우 강력한 도구인 `jq`가 있기 때문입니다.
다음 명령어는 JSON 데이터의 특정 부분을 추출하고 우리가 작업할 수 있는 형태로 재구성합니다.


``` console
< table.json jq -r '.tbody.tr[1:][] | [.td[]["$t"]] | @csv' | header -a rank,country,border,surface,ratio > countries.csv
```

이제 데이터가 작업하기 편한 형태가 되었습니다.
위키피디아 페이지에서 CSV 데이터셋을 얻기까지 꽤 많은 단계를 거쳤습니다.
하지만 위의 모든 명령어를 하나로 합치면 실제로는 매우 간결하고 표현력이 풍부하다는 것을 알 수 있습니다.


``` console
csvlook --max-column-width 28 countries.csv
```

이로써 XML/HTML에서 JSON을 거쳐 CSV로 변환하는 시연을 마칩니다.
`jq`는 훨씬 더 많은 작업을 수행할 수 있고 XML 데이터를 다루는 전문 도구들도 존재하지만, 제 경험상 가능한 한 빨리 데이터를 CSV 형식으로 변환하는 것이 잘 작동하는 경향이 있습니다.
이렇게 하면 특정 도구보다는 범용적인 커맨드 라인 도구에 능숙해지는 데 더 많은 시간을 할애할 수 있습니다.


## 요약

이 장에서는 데이터를 청소하거나 정제하는 과정을 살펴보았습니다.
보셨듯이 데이터의 모든 지저분함을 마법처럼 없애주는 단일 도구는 없습니다. 원하는 결과를 얻기 위해 여러 가지 서로 다른 도구들을 결합해야 하는 경우가 많습니다.
`cut`이나 `sort` 같은 고전적인 커맨드 라인 도구들은 구조화된 데이터를 해석할 수 없다는 점을 기억하세요.
다행히 JSON이나 XML 같은 데이터 형식을 CSV 같은 다른 데이터 형식으로 변환해 주는 도구들이 있습니다.
다음 장은 다시 한 번 막간을 이용한 장으로, `make`를 사용하여 프로젝트를 관리하는 방법을 보여드리겠습니다.
[7장](#chapter-7-exploring-data)에서 데이터 탐색과 시각화를 시작하는 것이 너무 기다려진다면 이 장은 건너뛰어도 좋습니다.


## 더 읽어보기

- `awk`에 대해 더 많이 설명할 수 있었다면 좋았을 것입니다. `awk`는 매우 강력한 도구이자 프로그래밍 언어입니다. 시간을 내어 배워보시길 강력히 추천합니다. 좋은 자료로는 Doherty와 Robbins가 쓴 *sed & awk* 책과 온라인 [GNU Awk User's Guide](https://www.gnu.org/software/gawk/manual/gawk.html)가 있습니다.
- 이 장의 몇 군데에서 정규 표현식을 사용했습니다. 아쉽게도 정규 표현식에 대한 튜토리얼은 이 책의 범위를 벗어납니다. 정규 표현식은 많은 서로 다른 도구에서 사용될 수 있으므로 배워두시는 것을 추천합니다. 좋은 책으로는 Jan Goyvaerts와 Steven Levithan이 쓴 *Regular Expressions Cookbook*이 있습니다.
