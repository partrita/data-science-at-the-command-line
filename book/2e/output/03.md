---
suppress-bibliography: true
---







# 데이터 획득하기 {#chapter-3-obtaining-data}

이 장에서는 OSEMN 모델의 첫 번째 단계인 데이터 획득(Obtaining)을 다룹니다.
데이터 없이는 데이터 과학을 할 수 없기 때문입니다.
데이터 과학 문제를 해결하는 데 필요한 데이터가 이미 존재한다고 가정한다면, 여러분의 첫 번째 과제는 그 데이터를 여러분이 작업할 수 있는 형식으로 컴퓨터(혹은 가능하면 Docker 컨테이너 내부)에 가져오는 것입니다.

유닉스 철학에 따르면 텍스트는 범용 인터페이스입니다.
거의 모든 커맨드 라인 도구가 텍스트를 입력으로 받고, 텍스트를 출력으로 생성하거나 두 가지 모두를 수행합니다.
이것이 커맨드 라인 도구들이 서로 아주 잘 어울려 작동할 수 있는 주요 원인입니다.
하지만 앞으로 보겠지만, 텍스트 하나만 해도 여러 가지 형식이 있을 수 있습니다.

데이터는 서버에서 다운로드하거나, 데이터베이스에 쿼리하거나, 웹 API에 연결하는 등 여러 가지 방법으로 얻을 수 있습니다.
때로는 데이터가 압축된 형태이거나 마이크로소프트 엑셀 스프레드시트와 같은 바이너리 형식으로 오기도 합니다.
이 장에서는 커맨드 라인에서 이를 해결하는 데 도움이 되는 여러 도구를 다룹니다: `curl`[@curl], `in2csv`[@in2csv], `sql2csv`[@sql2csv], `tar`[@tar] 등이 포함됩니다.


## 개요

이 장에서 여러분은 다음 내용을 배우게 됩니다.

- 로컬 파일을 Docker 이미지로 복사하기
- 인터넷에서 데이터 다운로드하기
- 파일 압축 풀기
- 스프레드시트에서 데이터 추출하기
- 관계형 데이터베이스 쿼리하기
- 웹 API 호출하기

이 장은 다음 파일들로 시작합니다.


``` console
cd /data/ch03
l
```

이 파일들을 얻는 방법은 [2장](#chapter-2-getting-started)에 설명되어 있습니다.
그 외의 파일들은 커맨드 라인 도구를 사용하여 다운로드하거나 생성한 것들입니다.


## 로컬 파일을 Docker 컨테이너로 복사하기

필요한 파일이 이미 여러분의 컴퓨터에 있는 경우가 흔합니다.
이 절에서는 그 파일들을 Docker 컨테이너 안으로 가져오는 방법을 설명합니다.

[2장](#chapter-2-getting-started)에서 말씀드렸듯이 Docker 컨테이너는 격리된 가상 환경입니다.
다행히 한 가지 예외가 있는데, Docker 컨테이너 안팎으로 파일을 전송할 수 있다는 점입니다.
여러분이 `docker run`을 실행했던 로컬 디렉터리가 Docker 컨테이너 내부의 한 디렉터리로 매핑됩니다.
그 디렉터리는 */data*라고 부릅니다.
참고로 이곳은 홈 디렉터리(*/home/dst*)가 아닙니다.

로컬 컴퓨터에 있는 파일을 커맨드 라인 도구로 다루고 싶다면, 해당 파일을 매핑된 디렉터리로 복사하거나 이동하기만 하면 됩니다.
여러분의 '다운로드' 디렉터리에 *logs.csv*라는 파일이 있다고 가정해 봅시다.

윈도우를 사용 중이라면 명령 프롬프트나 PowerShell을 열고 다음 두 명령어를 실행합니다.

```powershell
> cd %UserProfile%\Downloads
> copy logs.csv MyDataScienceToolbox\
```

리눅스나 macOS를 사용 중이라면 터미널을 열고 운영체제의 쉘에서(Docker 컨테이너 내부가 아님) 다음 명령어를 실행합니다.


``` console
cp ~/Downloads/logs.csv ~/my-data-science-toolbox#! enter=FALSE
```



윈도우 파일 탐색기나 macOS의 Finder와 같은 그래픽 파일 관리자를 사용하여 파일을 해당 디렉터리로 드래그 앤 드롭할 수도 있습니다.


## 인터넷에서 다운로드하기

인터넷은 의심할 여지 없이 흥미로운 데이터를 위한 가장 큰 자원 저장소입니다.
커맨드 라인 도구 `curl`은 인터넷에서 데이터를 다운로드할 때 커맨드 라인의 맥가이버 칼과 같은 존재라고 할 수 있습니다.


### `curl` 소개

URL(uniform resource locator) 주소로 접속하면 브라우저는 다운로드한 데이터를 해석합니다.
예를 들어 브라우저는 HTML 파일을 렌더링하고, 비디오 파일을 자동으로 재생하며, PDF 파일을 보여줍니다.
하지만 `curl`로 URL에 접근하면 데이터를 다운로드하여 기본적으로 표준 출력으로 인쇄합니다.
`curl`은 데이터를 해석하지 않지만, 다행히 다른 커맨드 라인 도구를 사용하여 데이터를 추가로 처리할 수 있습니다.

`curl`을 실행하는 가장 쉬운 방법은 URL을 커맨드 라인 인자로 지정하는 것입니다.
위키백과에서 문서를 하나 다운로드해 봅시다.


``` console
curl "https://en.wikipedia.org/wiki/List_of_windmills_in_the_Netherlands" |
trim
```
<1> 기억하세요. `trim`은 출력 결과가 책에 잘 어울리게 들어가도록 하기 위해서만 사용합니다.

보시다시피 `curl`은 위키백과 서버가 반환한 가공되지 않은(raw) HTML을 다운로드합니다. 아무런 해석 과정 없이 전체 내용이 즉시 표준 출력으로 인쇄됩니다.
URL을 보면 이 문서가 네덜란드의 모든 풍차 목록을 담고 있을 것 같지만, 보아하니 풍차가 너무 많아서 각 주(province)마다 별도의 페이지가 있는 것 같습니다. 놀랍군요.

기본적으로 `curl`은 다운로드 속도와 예상 완료 시간을 보여주는 진행 표시기(progress meter)를 출력합니다.
이 출력은 표준 출력이 아닌 별도의 채널인 *표준 오류*로 전달되므로, 파이프라인에 다른 도구를 추가하더라도 방해되지 않습니다.
매우 큰 파일을 다운로드할 때는 이 정보가 유용할 수 있지만, 대개는 번거롭게 느껴지므로 저는 `-s` 옵션을 지정하여 이 출력을 끕니다(silence).


``` console
curl -s "https://en.wikipedia.org/wiki/List_of_windmills_in_Friesland" |
pup -n 'table.wikitable tr' # <1>
```
<1> 웹사이트 스크래핑에 유용한 도구인 `pup`[@pup]에 대해서는 [5장](#chapter-5-scrubbing-data)에서 더 자세히 다루겠습니다.

보세요, 프리슬란트 주에만 자그마치 234개의 풍차가 있다고 하네요!


### 파일로 저장하기

`-O` 옵션을 추가하여 `curl`이 출력을 파일로 저장하게 할 수 있습니다.
파일 이름은 URL의 마지막 부분을 기반으로 결정됩니다.


``` console
curl -s "https://en.wikipedia.org/wiki/List_of_windmills_in_Friesland" -O
l
```

그 파일 이름이 마음에 들지 않는다면 `-o` 옵션과 함께 파일 이름을 지정하거나, 직접 출력을 파일로 리다이렉션할 수 있습니다.


``` console
curl -s "https://en.wikipedia.org/wiki/List_of_windmills_in_Friesland" > friesland.html
```


### 다른 프로토콜들

`curl`은 총 [20개 이상의 프로토콜](https://ec.haxx.se/protocols/protocols-curl)을 지원합니다.
FTP(File Transfer Protocol) 서버에서 다운로드할 때도 동일한 방식으로 `curl`을 사용합니다.
여기서는 *ftp.gnu.org*에서 *welcome.msg* 파일을 다운로드해 보겠습니다.


``` console
curl -s "ftp://ftp.gnu.org/welcome.msg" | trim
```

지정된 URL이 디렉터리라면 `curl`은 해당 디렉터리의 내용을 나열합니다.
URL이 비밀번호로 보호되어 있다면 `-u` 옵션을 사용하여 사용자 이름과 비밀번호를 지정할 수 있습니다.

각종 사전과 용어 정의에 접근할 수 있게 해주는 DICT 프로토콜은 어떨까요?
Collaborative International Dictionary of English에 따른 "windmill"의 정의는 다음과 같습니다.


``` console
curl -s "dict://dict.org/d:windmill" | trim
```

하지만 인터넷에서 데이터를 다운로드할 때 사용하는 프로토콜은 대부분 HTTP일 것이므로, URL은 *http://* 또는 *https://*로 시작할 것입니다.


### 리다이렉션 따라가기 (Following Redirects)

*http://bit.ly/* 또는 *http://t.co/*로 시작하는 단축 URL에 접근하면 브라우저는 자동으로 올바른 위치로 리다이렉션해 줍니다.
하지만 `curl`에서는 리다이렉션되도록 `-L` 또는 `--location` 옵션을 지정해야 합니다.
그렇지 않으면 다음과 같은 결과를 얻게 될 수도 있습니다.


``` console
curl -s "https://bit.ly/2XBxvwK"
```

때로는 우리가 방금 사용한 URL처럼 아무런 응답도 받지 못할 때가 있습니다.


``` console
curl -s "https://youtu.be/dQw4w9WgXcQ"
```

`-I` 또는 `--head` 옵션을 지정하면 `curl`은 응답의 HTTP 헤더라고 불리는 부분만 가져옵니다. 이를 통해 서버가 반환한 상태 코드(status code)와 기타 정보를 확인할 수 있습니다.


``` console
curl -sI "https://youtu.be/dQw4w9WgXcQ" | trim
```

첫 번째 줄은 프로토콜과 HTTP 상태 코드를 보여주는데, 이 경우 303입니다.
또한 이 URL이 리다이렉션되는 위치(location)도 확인할 수 있습니다.
`curl`이 예상한 결과를 주지 않을 때 헤더를 검사하고 상태 코드를 확인하는 것은 유용한 디버깅 방법입니다.
다른 흔한 HTTP 상태 코드로는 404(찾을 수 없음)와 403(금지됨)이 있습니다.
위키백과에는 [모든 HTTP 상태 코드](http://en.wikipedia.org/wiki/List_of_HTTP_status_codes)를 나열한 페이지가 있습니다.

요약하자면, `curl`은 인터넷에서 데이터를 다운로드하는 데 유용한 커맨드 라인 도구입니다.
가장 자주 쓰이는 세 가지 옵션은 진행 표시기를 끄는 `-s`, 사용자 이름과 비밀번호를 지정하는 `-u`, 그리고 자동으로 리다이렉션을 따라가는 `-L`입니다.
더 자세한 정보는 매뉴얼 페이지를 참고하세요(아마 머리가 어지러울 수 있습니다).


``` console
man curl | trim 20
```


## 파일 압축 풀기

원본 데이터셋이 매우 크거나 여러 파일의 모음인 경우, 압축된 아카이브 형태일 수 있습니다.
텍스트 파일의 단어들이나 JSON 파일의 키(key)들과 같이 반복되는 값이 많은 데이터셋은 특히 압축하기에 적합합니다.

압축 아카이브의 일반적인 확장자는 *.tar.gz*, *.zip*, *.rar* 등입니다.
이들의 압축을 풀기 위해서는 각각 `tar`, `unzip` [@unzip], `unrar` [@unrar] 도구를 사용합니다.
(그리 흔하지는 않지만 다른 도구가 필요한 확장자들도 몇 가지 더 있습니다.)

*.tar.gz*("지집 타볼(gzipped tarball)"이라고 읽습니다)를 예로 들어 봅시다.
*logs.tar.gz*라는 아카이브를 추출하기 위해 다음 명령어를 사용할 수 있습니다.


``` console
tar -xzf logs.tar.gz # <1> #! enter=FALSE
```
<1> 제가 여기서 한 것처럼 이 세 개의 짧은 옵션을 결합하는 것이 일반적이지만, `-x -z -f`와 같이 따로 지정할 수도 있습니다.
사실 많은 커맨드 라인 도구들이 단일 문자로 구성된 옵션들을 결합해서 쓸 수 있도록 허용합니다.

과연 `tar`는 수많은 커맨드 라인 인자로 악명이 높습니다.
여기서 세 옵션 `-x`, `-z`, `-f`는 각각 `tar`가 아카이브에서 파일을 *추출(extract)*하고, 압축 해제 알고리즘으로 *gzip*을 사용하며, *logs.tar.gz* 파일을 사용하도록 지정합니다.



하지만 이 아카이브에 아직 익숙하지 않으므로 내용을 먼저 확인하는 것이 좋습니다.
이는 `-x` 옵션 대신 `-t` 옵션으로 할 수 있습니다.


``` console
tar -tzf logs.tar.gz | trim
```

이 아카이브에는 많은 파일이 들어 있는 것 같고, 디렉터리 안에 들어 있지는 않네요.
현재 디렉터리를 깨끗하게 유지하기 위해 우선 `mkdir`로 새 디렉터리를 만들고, `-C` 옵션을 사용해 그곳에 파일들을 추출하는 것이 좋습니다.


``` console
mkdir logs
tar -xzf logs.tar.gz -C logs
```

파일 수와 내용 일부를 확인해 봅시다.


``` console
ls logs | wc -l
cat logs/* | trim
```

아주 좋군요.
여러분이 이 로그 파일들을 정제하고 탐색하고 싶어 한다는 점은 이해하지만, 그 내용은 나중에 [5장](#chapter-5-scrubbing-data)과 [7장](#chapter-7-exploring-data)에서 다루겠습니다.

시간이 지나면 이 옵션들에 익숙해지겠지만, 편리할 수 있는 대안적인 옵션을 하나 보여드리고 싶습니다.
서로 다른 커맨드 라인 도구와 그 옵션들을 일일이 기억하는 대신, 여러 가지 형식을 알아서 풀어주는 `unpack` [@unpack]이라는 편리한 스크립트가 있습니다.
`unpack`은 압축을 풀고자 하는 파일의 확장자를 보고 적절한 커맨드 라인 도구를 호출합니다.
이제 동일한 파일의 압축을 풀기 위해 다음을 실행하면 됩니다.


``` console
unpack logs.tar.gz #! enter=FALSE
```




## 마이크로소프트 엑셀 스프레드시트를 CSV로 변환하기

많은 사람에게 마이크로소프트 엑셀은 소규모 데이터셋을 다루고 계산을 수행하는 직관적인 방법을 제공합니다.
그 결과, 엄청나게 많은 데이터가 마이크로소프트 엑셀 스프레드시트에 내포되어 있습니다.
이러한 스프레드시트는 파일 확장자에 따라 독자적인 바이너리 형식(*.xls*)이나 압축된 XML 파일들의 모음(*.xlsx*)으로 저장됩니다.
두 경우 모두 데이터가 대부분의 커맨드 라인 도구에서 즉시 사용하기에 적합하지 않습니다.
단지 이런 방식으로 저장되어 있다는 이유만으로 그 가치 있는 데이터셋들을 사용할 수 없다면 참 안타까운 일일 것입니다.

특히 커맨드 라인을 막 시작했을 때는 스프레드시트를 마이크로소프트 엑셀이나 리브레오피스(LibreOffice) Calc 같은 오픈 소스 프로그램으로 열어서 수동으로 CSV로 내보내고 싶은 유혹을 느낄 수 있습니다.
일회성 해결책으로는 괜찮을 수 있지만, 여러 파일로 확장하기 어렵고 자동화할 수 없다는 단점이 있습니다.
게다가 서버에서 작업할 때는 그런 애플리케이션을 사용할 수 없을 가능성이 큽니다.
저를 믿으세요, 곧 익숙해질 것입니다.

다행히 마이크로소프트 엑셀 스프레드시트를 CSV 파일로 변환해 주는 `in2csv`라는 커맨드 라인 도구가 있습니다.
CSV는 쉼표로 구분된 값(comma-separated values)을 의미합니다.
CSV는 공식적인 사양이 부족하기 때문에 다루기가 까다로울 수 있습니다.
Yakov Shafranovich는 CSV 형식을 다음 세 가지 포인트로 정의합니다.[@shafranovich2005common]

1.  각 레코드는 줄바꿈(`␊`)으로 구분되어 별도의 줄에 위치합니다. 예를 들어 닌자 거북이에 대한 중요한 정보가 담긴 다음 CSV 파일을 봅시다.


``` console
bat -A tmnt-basic.csv # <1>
```
<1> `-A` 옵션은 `bat`이 공백, 탭, 줄바꿈 같은 출력되지 않는 모든 문자를 보여주게 합니다.

2.  파일의 마지막 레코드에는 끝 줄바꿈이 있을 수도 있고 없을 수도 있습니다. 예를 들면 다음과 같습니다.


``` console
bat -A tmnt-missing-newline.csv
```

3.  파일의 첫 번째 줄에 일반 레코드 줄과 동일한 형식의 헤더가 나타날 수 있습니다. 이 헤더는 파일의 필드에 대응하는 이름들을 포함하며, 파일의 나머지 부분에 있는 레코드들과 동일한 수의 필드를 가져야 합니다. 예를 들면 다음과 같습니다.


``` console
bat -A tmnt-with-header.csv
```

보시다시피 CSV는 기본적으로 그리 읽기 편하지 않습니다.
데이터를 `csvlook` [@csvlook]이라는 도구로 파이프하면 보기 좋은 표 형식으로 포맷해 줍니다.
만약 *tmnt-missing-newline.csv*처럼 CSV 데이터에 헤더가 없다면 `-H` 옵션을 추가해야 합니다. 그렇지 않으면 첫 번째 줄이 헤더로 해석됩니다.


``` console
csvlook tmnt-with-header.csv
csvlook tmnt-basic.csv
csvlook -H tmnt-missing-newline.csv # <1>
```
<1> `-H` 옵션은 CSV 파일에 헤더가 없음을 지정합니다.

네덜란드의 연례 라디오 프로그램인 [Top 2000](https://www.top2000nl.com)의 역대 인기곡 2000곡이 담긴 스프레드시트를 사용하여 `in2csv`를 시연해 보겠습니다.
데이터를 추출하려면 다음과 같이 `in2csv`를 호출합니다.


``` console
curl "https://www.nporadio2.nl/data/download/TOP-2000-2020.xlsx" > top2000.xlsx
in2csv top2000.xlsx | tee top2000.csv | trim
```

Danny Vera가 누구죠? 가장 인기 있는 노래는 당연히 *Bohemian Rhapsody*여야 합니다.
음, 적어도 Queen은 Top 2000에 아주 많이 등장하니 불평할 수는 없겠네요.


``` console
csvgrep top2000.csv --columns ARTIEST --regex '^Queen$' | csvlook -I
```
<1> `--regex` 옵션 뒤에 오는 값은 정규 표현식(regular expression, 줄여서 regex)입니다. 패턴을 정의하기 위한 특별한 구문입니다. 여기서는 아티스트 이름이 정확히 "Queen"인 것만 매칭하고 싶어서, 'ARTIEST' 컬럼 값의 시작과 끝을 나타내는 캐럿(`^`)과 달러 기호(`$`)를 사용했습니다.

참고로 `in2csv`, `csvgrep`, `csvlook` 도구들은 CSV 데이터를 다루기 위한 커맨드 라인 도구 모음인 CSVkit의 일부입니다.

파일 형식은 확장자에 의해 자동으로 결정되며, 이 경우 *.xlsx*입니다.
만약 데이터를 `in2csv`로 파이프한다면 형식을 명시적으로 지정해야 합니다.

스프레드시트는 여러 개의 워크시트를 포함할 수 있습니다.
`in2csv`는 기본적으로 첫 번째 워크시트를 추출합니다.
다른 워크시트를 추출하려면 `--sheet` 옵션에 워크시트 이름을 전달해야 합니다.
워크시트 이름이 무엇인지 확실하지 않다면 모든 워크시트의 이름을 출력하는 `--names` 옵션을 사용할 수 있습니다.
여기서 *top2000.xlsx*에는 *Blad1*(네덜란드어로 *Sheet1*이라는 뜻)이라는 이름의 시트가 하나만 있음을 확인할 수 있습니다.


``` console
in2csv --names top2000.xlsx
```


## 관계형 데이터베이스 쿼리하기

많은 기업이 데이터를 관계형 데이터베이스에 저장합니다.
스프레드시트와 마찬가지로, 커맨드 라인에서 그 데이터를 얻을 수 있다면 정말 좋을 것입니다.

관계형 데이터베이스의 예로는 MySQL, PostgreSQL, SQLite 등이 있습니다.
이 데이터베이스들은 모두 인터페이스하는 방식이 조금씩 다릅니다.
어떤 것들은 커맨드 라인 도구나 인터페이스를 제공하지만, 그렇지 않은 것들도 있습니다.
게다가 사용법과 출력 형식 면에서 일관성이 떨어지기도 합니다.

다행히 CSVkit 스위트의 일부인 `sql2csv`라는 커맨드 라인 도구가 있습니다.
이 도구는 MySQL, Oracle, PostgreSQL, SQLite, Microsoft SQL Server, Sybase를 포함한 다양한 데이터베이스와 공통된 인터페이스를 통해 작동합니다.
`sql2csv`의 출력은 이름에서 알 수 있듯이 CSV 형식입니다.

우리는 *`SELECT`* 쿼리를 실행하여 관계형 데이터베이스에서 데이터를 얻을 수 있습니다.
(`sql2csv`는 *`INSERT`*, *`UPDATE`*, *`DELETE`* 쿼리도 지원하지만, 이 장의 목적에는 맞지 않습니다.)

`sql2csv`에는 두 가지 인자가 필요합니다: 데이터베이스 URL을 지정하는 `--db`(일반적인 형식은 `dialect+driver://username:password@host:port/database`입니다)와 *`SELECT`* 쿼리를 포함하는 `--query`입니다.
예를 들어, R의 표준 데이터셋이 담긴 SQLite 데이터베이스[^3]가 있다면, 다음과 같이 *mtcars* 테이블의 모든 행을 선택하고 *mpg* 컬럼을 기준으로 정렬할 수 있습니다.


``` console
sql2csv --db 'sqlite:///r-datasets.db' \
--query 'SELECT row_names AS car, mpg FROM mtcars ORDER BY mpg' | csvlook
```

이 SQLite 데이터베이스는 로컬 파일이므로 사용자 이름, 비밀번호, 호스트를 지정할 필요가 없습니다.
회사의 데이터베이스를 쿼리하고 싶다면 당연히 접근 방법과 권한이 필요할 것입니다.


## 웹 API 호출하기

앞 절에서는 인터넷에서 파일을 다운로드하는 방법을 설명했습니다.
인터넷에서 데이터를 얻는 또 다른 방법은 웹 API(Application Programming Interface)를 통하는 것입니다.
제공되는 API의 수는 점점 더 빠르게 늘어나고 있으며, 이는 우리 데이터 과학자들에게 흥미로운 데이터가 많아짐을 의미합니다.

웹 API는 웹사이트처럼 예쁜 레이아웃으로 보여주기 위한 것이 아닙니다.
대신 대부분의 웹 API는 JSON이나 XML 같은 구조화된 형식으로 데이터를 반환합니다.
구조화된 형식의 데이터를 가지면 `jq`와 같은 다른 도구로 데이터를 쉽게 처리할 수 있다는 장점이 있습니다.
예를 들어 George R.R. Martin의 가상 세계에 대한 풍부한 정보를 담고 있는 API of Ice and Fire(얼음과 불의 노래 API)는 다음과 같은 JSON 구조로 데이터를 반환합니다.


``` console
curl -s "https://anapioficeandfire.com/api/characters/583" | jq '.'
```
<1> 스포일러 주의: 이 데이터는 완전히 최신 상태가 아닐 수 있습니다.

데이터를 보기 좋게 표시하기 위해 커맨드 라인 도구 `jq`로 파이프했습니다.
`jq`에는 정제와 탐색을 위한 훨씬 더 많은 가능성이 있으며, 이는 [5장](#chapter-5-scrubbing-data)과 [7장](#chapter-7-exploring-data)에서 살펴보겠습니다.


### 인증 (Authentication)

일부 웹 API는 데이터를 사용하기 전에 인증(즉, 신원 증명)을 요구합니다.
이를 수행하는 몇 가지 방법이 있습니다.
어떤 웹 API는 API 키를 사용하고, 어떤 것들은 OAuth 프로토콜을 사용합니다.
헤드라인과 뉴스 기사를 제공하는 독립 자원인 News API가 훌륭한 예입니다.
API 키 없이 이 API에 접근하면 어떤 일이 일어나는지 봅시다.


``` console
curl -s "http://newsapi.org/v2/everything?q=linux" | jq .
```

음, 예상했던 결과군요.
참고로 물음표 뒤의 부분은 쿼리 매개변수를 전달하는 곳입니다.
API 키를 지정해야 하는 곳이기도 하죠.
제 API 키를 비밀로 유지하고 싶으므로, 명령어 치환(command substitution)을 사용하여 */data/.secret/newsapi.org_apikey* 파일을 읽어서 입력하겠습니다.


``` console
curl -s "http://newsapi.org/v2/everything?q=linux&apiKey=$(< /data/.secret/newsapi.org_apikey)" |
jq '.' | trim 30
```
여러분만의 API 키는 [News API 웹사이트](https://newsapi.org)에서 얻을 수 있습니다.


### 스트리밍 API (Streaming APIs)

일부 웹 API는 데이터를 스트리밍 방식으로 반환합니다.
즉, 한 번 연결하면 연결이 끊길 때까지 데이터가 쉬지 않고 쏟아져 들어옵니다.
유명한 예로는 전 세계에서 전송되는 모든 트윗을 끊임없이 스트리밍하는 트위터의 "파이어호스(firehose)"가 있습니다.
다행히 대부분의 커맨드 라인 도구들도 스트리밍 방식으로 작동합니다.



예를 들어 위키미디어의 스트리밍 API 중 하나를 10초 동안 샘플링해 봅시다.


``` console
curl -s "https://stream.wikimedia.org/v2/stream/recentchange" |
sample -s 10 > wikimedia-stream-sample #! enter=FALSE
```



이 특정 API는 위키백과와 위키미디어의 다른 속성들에 가해진 모든 변경 사항을 반환합니다.
커맨드 라인 도구 `sample`은 10초 후에 연결을 닫는 데 사용됩니다.
**`Ctrl-C`**를 눌러 인터럽트를 보냄으로써 연결을 수동으로 닫을 수도 있습니다.
출력은 *wikimedia-stream-sample* 파일에 저장됩니다.
`trim`을 사용해 살짝 엿볼까요?


``` console
< wikimedia-stream-sample trim
```

`sed`와 `jq`를 조금 사용해서 이 데이터를 정제하면 영어 위키백과에서 일어나는 변경 사항들을 엿볼 수 있습니다.


``` console
< wikimedia-stream-sample sed -n 's/^data: //p' |
jq 'select(.type == "edit" and .server_name == "en.wikipedia.org") | .title'
```
<1> 이 `sed` 표현식은 *`data: `*로 시작하는 줄만 출력하고 세미콜론 뒤의 부분만 인쇄하는데, 이것이 마침 JSON입니다.
<2> 이 `jq` 표현식은 특정 *`type`*과 *`server_name`*을 가진 JSON 객체의 *`title`* 키를 인쇄합니다.

스트리밍 이야기가 나와서 말인데, `telnet` [@telnet]을 사용해 *스타워즈 에피소드 4: 새로운 희망*을 무료로 스트리밍할 수 있다는 사실을 알고 계셨나요?




``` console
telnet towel.blinkenlights.nl#! enter=FALSE
```

그리고 시간이 좀 지나면, 한 솔로가 먼저 쐈다는 것을 알게 됩니다!




``` console
 cat towel.blinkenlights.nl
```

물론 데이터로서 좋은 소스는 아니겠지만, 머신러닝 모델을 훈련시키면서 고전 명작을 즐기는 것도 나쁘지 않죠[^starwars].


## 요약

축하합니다, OSEMN 모델의 첫 번째 단계를 마쳤습니다.
다운로드부터 관계형 데이터베이스 쿼리까지 데이터를 얻는 다양한 방법을 배웠습니다.
다음 장은 막간 장으로, 여러분만의 커맨드 라인 도구를 만드는 방법을 가르쳐 드릴 것입니다.
데이터 정제에 대해 배우고 싶어 참을 수 없다면, 이 장을 건너뛰고 [5장](#chapter-5-scrubbing-data) (OSEMN 모델의 두 번째 단계)으로 바로 가셔도 좋습니다.


## 더 읽을거리

- 연습할 데이터셋을 찾고 계신가요? GitHub 저장소 [*Awesome Public Datasets*](https://github.com/awesomedata/awesome-public-datasets)에는 공개적으로 사용 가능한 수백 개의 고품질 데이터셋 목록이 있습니다.
- 아니면 API로 연습하고 싶으신가요? GitHub 저장소 [*Public APIs*](https://github.com/public-apis/public-apis)에는 흥미로운 무료 API들이 많이 나열되어 있습니다. [City Bikes](http://api.citybik.es/v2/)와 [The One API](https://the-one-api.dev/)는 제가 좋아하는 API들입니다.
- 관계형 데이터베이스에서 데이터를 얻기 위해 SQL 쿼리를 작성하는 것은 중요한 기술입니다. Ben Forta의 저서 *SQL in 10 Minutes a Day*의 처음 15개 장은 *`SELECT`* 문과 필터링, 그룹화, 정렬 기능을 잘 가르쳐 줍니다.

[^3]: [GitHub](https://github.com/r-dbi/RSQLite/blob/master/inst/db/datasets.sqlite)에서 이용 가능합니다.
[^starwars]: 누군가 아카이브 메모리에서 삭제하여 서버에 연결할 수 없다면, 언제든 [YouTube의 `telnet` 세션 녹화본](https://www.youtube.com/results?search_query=towel.blinkenlights.nl)을 감상할 수 있습니다.
