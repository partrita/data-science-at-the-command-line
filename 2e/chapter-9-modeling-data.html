<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>장9 데이터 모델링 | 커맨드 라인에서 시작하는 데이터 과학, 2판</title>
<meta name="author" content="Jeroen Janssens">
<meta name="description" content="이 장에서는 OSEMN 모델의 네 번째 단계인 데이터 모델링을 수행할 것입니다. 일반적으로 모델은 여러분이 가진 데이터에 대한 추상적이거나 상위 수준의 설명(description)입니다. 모델링은 개별 데이터 포인트에서 한 걸음 물러나 큰 그림을 본다는 점에서 시각화를 만드는 것과 약간 비슷합니다. 시각화는 모양, 위치, 색상으로 특징지어집니다....">
<meta name="generator" content="bookdown 0.46 with bs4_book()">
<meta property="og:title" content="장9 데이터 모델링 | 커맨드 라인에서 시작하는 데이터 과학, 2판">
<meta property="og:type" content="book">
<meta property="og:url" content="https://datascienceatthecommandline.com/chapter-9-modeling-data.html">
<meta property="og:image" content="https://datascienceatthecommandline.com/og.png">
<meta property="og:description" content="이 장에서는 OSEMN 모델의 네 번째 단계인 데이터 모델링을 수행할 것입니다. 일반적으로 모델은 여러분이 가진 데이터에 대한 추상적이거나 상위 수준의 설명(description)입니다. 모델링은 개별 데이터 포인트에서 한 걸음 물러나 큰 그림을 본다는 점에서 시각화를 만드는 것과 약간 비슷합니다. 시각화는 모양, 위치, 색상으로 특징지어집니다....">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="장9 데이터 모델링 | 커맨드 라인에서 시작하는 데이터 과학, 2판">
<meta name="twitter:description" content="이 장에서는 OSEMN 모델의 네 번째 단계인 데이터 모델링을 수행할 것입니다. 일반적으로 모델은 여러분이 가진 데이터에 대한 추상적이거나 상위 수준의 설명(description)입니다. 모델링은 개별 데이터 포인트에서 한 걸음 물러나 큰 그림을 본다는 점에서 시각화를 만드는 것과 약간 비슷합니다. 시각화는 모양, 위치, 색상으로 특징지어집니다....">
<meta name="twitter:image" content="https://datascienceatthecommandline.com/twitter.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/Source_Sans_Pro-0.4.10/font.css" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Fira%20Mono:wght@400;600&amp;display=swap" rel="stylesheet">
<script src="libs/bs3compat-0.10.0/transition.js"></script><script src="libs/bs3compat-0.10.0/tabs.js"></script><script src="libs/bs3compat-0.10.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#d42d2d">
<meta name="apple-mobile-web-app-title" content="Data Science at the Command Line">
<meta name="application-name" content="Data Science at the Command Line">
<meta name="msapplication-TileColor" content="#b91d47">
<meta name="theme-color" content="#ffffff">
<script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-43246574-3', 'auto');
      ga('send', 'pageview');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="dsatcl2e.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-2 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">본문으로 건너뛰기</a>

    <div class="d-flex align-items-start justify-content-between">
      <img id="cover" class="d-none d-lg-block" src="images/cover-small.png"><h1 class="d-lg-none">
        <a href="index.html" title="">커맨드 라인에서 시작하는 데이터 과학, 2판</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">목차 보기</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="검색" aria-label="Search">
</form>
      <nav aria-label="Table of contents"><h2>목차</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">환영합니다</a></li>
<li><a class="" href="%EC%B6%94%EC%B2%9C%EC%9D%98-%EA%B8%80.html">추천의 글</a></li>
<li><a class="" href="%EC%84%9C%EB%AC%B8.html">서문</a></li>
<li><a class="" href="chapter-1-introduction.html"><span class="header-section-number">1</span> 서론</a></li>
<li><a class="" href="chapter-2-getting-started.html"><span class="header-section-number">2</span> 시작하기</a></li>
<li><a class="" href="chapter-3-obtaining-data.html"><span class="header-section-number">3</span> 데이터 획득하기</a></li>
<li><a class="" href="chapter-4-creating-command-line-tools.html"><span class="header-section-number">4</span> 커맨드 라인 도구 만들기</a></li>
<li><a class="" href="chapter-5-scrubbing-data.html"><span class="header-section-number">5</span> 데이터 정제하기</a></li>
<li><a class="" href="chapter-6-project-management-with-make.html"><span class="header-section-number">6</span> Make으로 프로젝트 관리하기</a></li>
<li><a class="" href="chapter-7-exploring-data.html"><span class="header-section-number">7</span> 데이터 탐색하기</a></li>
<li><a class="" href="chapter-8-parallel-pipelines.html"><span class="header-section-number">8</span> 병렬 파이프라인</a></li>
<li><a class="active" href="chapter-9-modeling-data.html"><span class="header-section-number">9</span> 데이터 모델링</a></li>
<li><a class="" href="chapter-10-polyglot-data-science.html"><span class="header-section-number">10</span> 다국어 데이터 과학</a></li>
<li><a class="" href="chapter-11-conclusion.html"><span class="header-section-number">11</span> 결론</a></li>
<li><a class="" href="%EC%BB%A4%EB%A7%A8%EB%93%9C-%EB%9D%BC%EC%9D%B8-%EB%8F%84%EA%B5%AC-%EB%AA%A9%EB%A1%9D.html">커맨드 라인 도구 목록</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/partrita/data-science-at-the-command-line">책 저장소 보기 <i class=""></i></a></p>
        </div>

        <div>
          <a id="course-signup" href="/#course">Embrace the Command Line</a>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="chapter-9-modeling-data" class="section level1" number="9">
<h1>
<span class="header-section-number">장9</span> 데이터 모델링<a class="anchor" aria-label="anchor" href="#chapter-9-modeling-data"><i class="fas fa-link"></i></a>
</h1>
<p>이 장에서는 OSEMN 모델의 네 번째 단계인 데이터 모델링을 수행할 것입니다.
일반적으로 모델은 여러분이 가진 데이터에 대한 추상적이거나 상위 수준의 설명(description)입니다.
모델링은 개별 데이터 포인트에서 한 걸음 물러나 큰 그림을 본다는 점에서 시각화를 만드는 것과 약간 비슷합니다.</p>
<p>시각화는 모양, 위치, 색상으로 특징지어집니다. 우리는 그것들을 직접 눈으로 보고 해석할 수 있습니다.
반면 모델은 내부적으로 숫자로 특징지어지며, 이는 컴퓨터가 새로운 데이터 포인트에 대한 예측을 수행하는 등의 작업에 모델을 사용할 수 있음을 의미합니다.
(모델이 어떻게 작동하고 성능이 어떠한지 이해하기 위해 여전히 모델을 시각화할 수 있습니다.)</p>
<p>이 장에서 저는 데이터를 모델링하는 데 흔히 사용되는 세 가지 유형의 알고리즘을 살펴볼 것입니다.</p>
<ul>
<li>차원 축소(Dimensionality reduction)</li>
<li>회귀(Regression)</li>
<li>분류(Classification)</li>
</ul>
<p>이 알고리즘들은 통계학과 머신러닝 분야에서 온 것이므로, 용어를 조금 바꾸어 사용하겠습니다.
우리가 <em>데이터셋</em>이라고도 불리는 CSV 파일을 가지고 있다고 가정해 봅시다.
헤더를 제외한 각 행은 하나의 <em>데이터 포인트</em>로 간주됩니다.
각 데이터 포인트는 하나 이상의 <em>특성(features)</em> 또는 측정된 속성들을 가집니다.
때로는 데이터 포인트가 <em>레이블(label)</em>을 가지기도 하는데, 이는 일반적으로 판단 결과나 성과를 의미합니다.
아래에서 와인 데이터셋을 소개할 때 이 개념이 더 구체화될 것입니다.</p>
<p>첫 번째 유형의 알고리즘(차원 축소)은 대부분 비지도 학습(unsupervised)입니다. 즉, 데이터셋의 특성만을 기반으로 모델을 생성합니다.
마지막 두 유형의 알고리즘(회귀 및 분류)은 정의상 지도 학습(supervised) 알고리즘이며, 이는 모델에 레이블 정보도 포함시킨다는 의미입니다.</p>

<div class="rmdcaution">
이 장은 결코 머신러닝 입문서가 아닙니다.
따라서 많은 세부 사항을 생략하고 넘어갈 수밖에 없습니다.
일반적인 권고 사항은, 여러분의 데이터에 알고리즘을 적용하기 전에 해당 알고리즘에 익숙해지는 시간을 갖는 것입니다.
이 장의 끝에서 머신러닝에 관한 몇 권의 책을 추천해 드립니다.
</div>
<div id="개요-6" class="section level2" number="9.1">
<h2>
<span class="header-section-number">9.1</span> 개요<a class="anchor" aria-label="anchor" href="#%EA%B0%9C%EC%9A%94-6"><i class="fas fa-link"></i></a>
</h2>
<p>이 장에서 여러분은 다음 방법을 배우게 됩니다.</p>
<ul>
<li>
<code>tapkee</code><span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Sergey Lisitsyn, Christian Widmer, and Fernando J. Iglesias Garcia, &lt;span&gt;“&lt;span class=&quot;nocase&quot;&gt;tapkee&lt;/span&gt; – an Efficient Dimension Reduction Library,”&lt;/span&gt; 2013, &lt;a href=&quot;http://tapkee.lisitsyn.me&quot;&gt;http://tapkee.lisitsyn.me&lt;/a&gt;.&lt;/p&gt;"><sup>107</sup></a></span>를 사용하여 데이터셋의 차원 축소하기</li>
<li>
<code>vw</code><span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;John Langford, &lt;span&gt;“&lt;span class=&quot;nocase&quot;&gt;vw&lt;/span&gt; – Fast Machine Learning Library for Online Learning,”&lt;/span&gt; 2021, &lt;a href=&quot;https://vowpalwabbit.org&quot;&gt;https://vowpalwabbit.org&lt;/a&gt;.&lt;/p&gt;"><sup>108</sup></a></span>를 사용하여 화이트 와인의 품질 예측하기</li>
<li>
<code>skll</code><span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Educational Testing Service, &lt;span&gt;“&lt;span class=&quot;nocase&quot;&gt;skll&lt;/span&gt; – &lt;span&gt;SciKit-Learn&lt;/span&gt; Laboratory,”&lt;/span&gt; 2021, &lt;a href=&quot;https://skll.readthedocs.org&quot;&gt;https://skll.readthedocs.org&lt;/a&gt;.&lt;/p&gt;"><sup>109</sup></a></span>을 사용하여 와인을 레드 또는 화이트로 분류하기</li>
</ul>
<p>이 장은 다음 파일로 시작합니다.</p>
<pre>cd /data/ch09
l</pre>
<p>이 파일들을 가져오는 방법은 <a href="chapter-2-getting-started.html#chapter-2-getting-started">2장</a>에 설명되어 있습니다.
그 외의 파일들은 커맨드 라인 도구를 사용하여 다운로드하거나 생성한 것들입니다.</p>
</div>
<div id="와인-좀-더-주세요" class="section level2" number="9.2">
<h2>
<span class="header-section-number">9.2</span> 와인 좀 더 주세요!<a class="anchor" aria-label="anchor" href="#%EC%99%80%EC%9D%B8-%EC%A2%80-%EB%8D%94-%EC%A3%BC%EC%84%B8%EC%9A%94"><i class="fas fa-link"></i></a>
</h2>
<p>이 장 전반에 걸쳐 ’비뉴 베르드(vinho verde)’라고 불리는 포르투갈 와인의 레드 및 화이트 품종에 대한 와인 감별사들의 기록 데이터셋을 사용할 것입니다.
각 데이터 포인트는 하나의 와인을 나타냅니다. 각 와인은 11가지 물리화학적 특성에 대해 등급이 매겨져 있습니다: (1) 고정 산도(fixed acidity), (2) 휘발성 산도(volatile acidity), (3) 구연산(citric acid), (4) 잔류 당분(residual sugar), (5) 염화물(chlorides), (6) 유리 이산화황(free sulfur dioxide), (7) 총 이산화황(total sulfur dioxide), (8) 밀도(density), (9) pH, (10) 황산염(sulphates), (11) 알코올(alcohol).
또한 0(매우 나쁨)에서 10(매우 우수) 사이의 종합 품질 점수가 있는데, 이는 와인 전문가들에 의한 최소 세 번의 평가 결과에 대한 중앙값입니다. 이 데이터셋에 대한 더 자세한 정보는 <a href="http://archive.ics.uci.edu/ml/datasets/Wine+Quality">UCI 머신러닝 저장소</a>에서 확인할 수 있습니다.</p>
<p>데이터셋은 화이트 와인용과 레드 와인용 두 개의 파일로 나뉘어 있습니다.
가장 먼저 할 일은 <code>curl</code>을 사용하여 두 파일을 가져오는 것입니다 (물론 시간이 아까우니 <code>parallel</code>을 사용합니다).</p>
<pre>parallel "curl -sL http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-{}.csv &gt; wine-{}.csv" ::: red white#!enter=FALSE
C-C#!literal=FALSE</pre>
<p>세 개의 콜론(<code>:::</code>)은 <code>parallel</code>에 데이터를 전달하는 또 다른 방법입니다.</p>
<pre>cp /data/.cache/wine-*.csv .</pre>
<p>두 파일을 검사하고 줄 수를 세어봅시다.</p>
<pre>&lt; wine-red.csv nl |
fold |
trim
&lt; wine-white.csv nl | fold | trim
wc -l wine-{red,white}.csv</pre>
<p><span class="callout">➊</span> 명확성을 위해 <code>nl</code>을 사용하여 줄 번호를 추가했습니다.
<br><span class="callout">➋</span> 헤더 전체를 보기 위해 <code>fold</code>를 사용했습니다.</p>
<p>언뜻 보기에 이 데이터는 꽤 깨끗해 보입니다.
그래도 대부분의 커맨드 라인 도구가 기대하는 형식에 더 잘 부합하도록 정제(scrub)해 보겠습니다.
구체적으로 다음 작업을 할 것입니다.</p>
<ul>
<li>헤더를 소문자로 변환합니다.</li>
<li>세미콜론(<code>;</code>)을 쉼표(<code>,</code>)로 바꿉니다.</li>
<li>공백을 언더스코어(<code>_</code>)로 바꿉니다.</li>
<li>불필요한 따옴표를 제거합니다.</li>
</ul>
<p><code>tr</code> 도구를 사용하면 이 모든 작업을 처리할 수 있습니다.
이번에는 옛 추억을 되살려 <code>for</code> 루프를 사용하여 두 파일을 처리해 보겠습니다.</p>
<pre>for COLOR in red white; do
&lt; wine-$COLOR.csv tr '[A-Z]; ' '[a-z],_' | tr -d \" &gt; wine-${COLOR}-clean.csv
done</pre>
<p>또한 두 파일을 결합하여 단일 데이터셋을 만들어 봅시다.
<code>csvstack</code><span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Christopher Groskopf, &lt;span&gt;“&lt;span class=&quot;nocase&quot;&gt;csvstack&lt;/span&gt; – Stack up the Rows from Multiple &lt;span&gt;CSV&lt;/span&gt; Files,”&lt;/span&gt; 2020, &lt;a href=&quot;https://csvkit.rtfd.org&quot;&gt;https://csvkit.rtfd.org&lt;/a&gt;.&lt;/p&gt;"><sup>110</sup></a></span>을 사용하여 <em>type</em>이라는 열을 추가할 것입니다. 이 열은 첫 번째 파일의 행에는 “red”, 두 번째 파일의 행에는 “white”라는 값을 갖게 됩니다.</p>
<pre>csvstack -g red,white -n type wine-{red,white}-clean.csv |
xsv select 2-,1 &gt; wine.csv</pre>
<p><span class="callout">➊</span> 새 열 <em>type</em>은 <code>csvstack</code>에 의해 맨 앞에 배치됩니다.
<br><span class="callout">➋</span> 일부 알고리즘은 레이블이 마지막 열에 있다고 가정하므로, <code>xsv</code>를 사용하여 <em>type</em> 열을 맨 뒤로 옮깁니다.</p>
<p>대부분의 머신러닝 알고리즘은 결측값(missing values)을 처리하지 못하므로, 이 데이터셋에 결측값이 있는지 확인하는 것이 좋습니다.</p>
<pre>csvstat wine.csv --nulls</pre>
<p>훌륭합니다!
만약 결측값이 있었다면 해당 특성의 평균값이나 가장 빈번한 값 등으로 채워 넣을 수 있었을 것입니다.
대안적으로, 결측값이 하나라도 있는 데이터 포인트를 아예 제거하는 덜 정교한 접근 방식도 있습니다.
그냥 호기심에, 레드 와인과 화이트 와인의 품질 분포가 어떻게 다른지 살펴봅시다.</p>
<pre>rush run -t 'ggplot(df, aes(x = quality, fill = type)) + geom_density(adjust = 3, alpha = 0.5)' wine.csv &gt; wine-quality.png
display wine-quality.png</pre>
<p>밀도 그래프를 통해 화이트 와인의 품질이 더 높은 값 쪽으로 분포되어 있음을 알 수 있습니다.
이것이 화이트 와인이 전반적으로 레드 와인보다 더 좋다는 뜻일까요, 아니면 화이트 와인 전문가들이 레드 와인 전문가들보다 더 쉽게 높은 점수를 준다는 뜻일까요?
그것은 데이터가 우리에게 말해주지 않는 부분입니다.
혹시 알코올 농도와 품질 사이에 관계가 있을까요?
<code>rush</code>를 사용하여 알아봅시다.</p>
<pre>rush plot --x alcohol --y quality --color type --geom smooth wine.csv &gt; wine-alcohol-vs-quality.png
display wine-alcohol-vs-quality.png</pre>
<p>유레카! 흠흠, 이제 모델링을 계속해 볼까요?</p>
</div>
<div id="tapkee를-사용한-차원-축소" class="section level2" number="9.3">
<h2>
<span class="header-section-number">9.3</span> Tapkee를 사용한 차원 축소<a class="anchor" aria-label="anchor" href="#tapkee%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-%EC%B0%A8%EC%9B%90-%EC%B6%95%EC%86%8C"><i class="fas fa-link"></i></a>
</h2>
<p>차원 축소의 목표는 고차원의 데이터 포인트를 저차원의 매핑으로 옮기는 것입니다.
핵심 과제는 유사한 데이터 포인트들이 저차원 매핑에서도 서로 가깝게 유지되도록 하는 것입니다.
이전 섹션에서 보았듯이, 우리의 와인 데이터셋은 13개의 특성을 포함하고 있습니다.
시각화하기에 가장 직관적인 두 개의 차원으로 축소를 진행해 보겠습니다.</p>
<p>차원 축소는 종종 탐색적 데이터 분석(EDA)의 일부로 간주됩니다.
플로팅하기에 특성이 너무 많을 때 유용합니다.
산점도 행렬(scatter-plot matrix)을 만들 수도 있지만, 이는 한 번에 두 개의 특성만 보여줄 수 있습니다.
또한 다른 머신러닝 알고리즘을 위한 전처리 단계로도 유용합니다.</p>
<p>대부분의 차원 축소 알고리즘은 비지도 학습입니다.
이는 데이터 포인트의 레이블을 사용하지 않고 저차원 매핑을 구축한다는 의미입니다.</p>
<p>이 섹션에서는 PCA(주성분 분석, Principal Components Analysis<span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;K. Pearson, &lt;span&gt;“On Lines and Planes of Closest Fit to Systems of Points in Space,”&lt;/span&gt; &lt;em&gt;Philosophical Magazine&lt;/em&gt; 2, no. 11 (1901): 559–72.&lt;/p&gt;"><sup>111</sup></a></span>)와 t-SNE(t-분포 확률적 임베딩, t-distributed Stochastic Neighbor Embedding<span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Laurens van der Maaten and Geoffrey Everest Hinton, &lt;span&gt;“Visualizing Data Using t-&lt;span&gt;SNE&lt;/span&gt;,”&lt;/span&gt; &lt;em&gt;Journal of Machine Learning Research&lt;/em&gt; 9 (2008): 2579–2605.&lt;/p&gt;"><sup>112</sup></a></span>)라는 두 가지 기술을 살펴보겠습니다.</p>
<div id="tapkee-소개" class="section level3" number="9.3.1">
<h3>
<span class="header-section-number">9.3.1</span> Tapkee 소개<a class="anchor" aria-label="anchor" href="#tapkee-%EC%86%8C%EA%B0%9C"><i class="fas fa-link"></i></a>
</h3>
<p>Tapkee는 차원 축소를 위한 C++ 템플릿 라이브러리입니다<span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Sergey Lisitsyn, Christian Widmer, and Fernando J. Iglesias Garcia, &lt;span&gt;“Tapkee: An Efficient Dimension Reduction Library,”&lt;/span&gt; &lt;em&gt;Journal of Machine Learning Research&lt;/em&gt; 14 (2013): 2355–59.&lt;/p&gt;"><sup>113</sup></a></span>.
이 라이브러리에는 다음을 포함한 많은 차원 축소 알고리즘이 구현되어 있습니다.</p>
<ul>
<li>국소 선형 임베딩(Locally Linear Embedding)</li>
<li>Isomap</li>
<li>다차원 척도법(Multidimensional Scaling)</li>
<li>PCA</li>
<li>t-SNE</li>
</ul>
<p>이 알고리즘들에 대한 자세한 정보는 <a href="http://tapkee.lisitsyn.me/">Tapkee 웹사이트</a>에서 찾을 수 있습니다.
Tapkee는 주로 다른 애플리케이션에 포함될 수 있는 라이브러리지만, <code>tapkee</code>라는 커맨드 라인 도구도 제공합니다.
이를 사용하여 와인 데이터셋의 차원 축소를 수행해 보겠습니다.</p>
</div>
<div id="선형-및-비선형-매핑" class="section level3" number="9.3.2">
<h3>
<span class="header-section-number">9.3.2</span> 선형 및 비선형 매핑<a class="anchor" aria-label="anchor" href="#%EC%84%A0%ED%98%95-%EB%B0%8F-%EB%B9%84%EC%84%A0%ED%98%95-%EB%A7%A4%ED%95%91"><i class="fas fa-link"></i></a>
</h3>
<p>먼저, 각 특성이 동일하게 중요하게 취급되도록 표준화(standardization)를 사용하여 특성의 스케일을 조정하겠습니다.
일반적으로 머신러닝 알고리즘을 적용할 때 스케일 조정을 하면 더 나은 결과를 얻을 수 있습니다.</p>
<p>스케일 조정을 위해 <code>rush</code>와 <code>tidyverse</code> 패키지를 사용합니다.</p>
<pre>rush run --tidyverse --output wine-scaled.csv \
'select(df, -type) %&gt;%
scale() %&gt;%
as_tibble() %&gt;%
mutate(type = df$type)' wine.csv
csvlook wine-scaled.csv</pre>
<p><span class="callout">➊</span> <code><a href="https://rdrr.io/r/base/scale.html">scale()</a></code> 함수는 수치형 열에만 작동하므로 <em><code>type</code></em> 열을 임시로 제거해야 합니다.
<br><span class="callout">➋</span> <code><a href="https://rdrr.io/r/base/scale.html">scale()</a></code> 함수는 데이터 프레임을 받지만 행렬을 반환합니다.
<br><span class="callout">➌</span> <code>as_tibble()</code> 함수는 행렬을 다시 데이터 프레임으로 변환합니다.
<br><span class="callout">➍</span> 마지막으로 <em><code>type</code></em> 열을 다시 추가합니다.</p>
<p>이제 두 가지 차원 축소 기술을 적용하고 <code>Rio-scatter</code>를 사용하여 매핑을 시각화해 보겠습니다.</p>
<pre>xsv select '!type' wine-scaled.csv |
header -d |
tapkee --method pca |
tee wine-pca.txt | trim</pre>
<p><span class="callout">➊</span> <em><code>type</code></em> 열을 제외합니다.
<br><span class="callout">➋</span> 헤더를 제거합니다.
<br><span class="callout">➌</span> PCA를 적용합니다.</p>
<pre>&lt; wine-pca.txt header -a pc1,pc2 |
paste -d, - &lt;(xsv select type wine-scaled.csv) |
tee wine-pca.csv | csvlook</pre>
<p><span class="callout">➊</span> <em><code>pc1</code></em>과 <em><code>pc2</code></em> 열이 있는 헤더를 다시 추가합니다.
<br><span class="callout">➋</span> <em><code>type</code></em> 열을 다시 추가합니다.</p>
<p>이제 산점도를 생성할 수 있습니다.</p>
<pre>rush plot --x pc1 --y pc2 --color type --shape type wine-pca.csv &gt; wine-pca.png
display wine-pca.png</pre>
<p>동일한 방식으로 t-SNE를 수행해 보겠습니다.</p>
<pre>xsv select '!type' wine-scaled.csv |
header -d |
tapkee --method t-sne |
header -a x,y |
paste -d, - &lt;(xsv select type wine-scaled.csv) |
rush plot --x x --y y --color type --shape type &gt; wine-tsne.png</pre>
<p><span class="callout">➊</span> <em><code>type</code></em> 열을 제외합니다.
<br><span class="callout">➋</span> 헤더를 제거합니다.
<br><span class="callout">➌</span> t-SNE를 적용합니다.
<br><span class="callout">➍</span> <em><code>x</code></em>와 <em><code>y</code></em> 열이 있는 헤더를 다시 추가합니다.
<br><span class="callout">➎</span> <em><code>type</code></em> 열을 다시 추가합니다.
<br><span class="callout">➏</span> 산점도를 생성합니다.</p>
<pre>display wine-tsne.png</pre>
<p>t-SNE가 PCA보다 물리화학적 특성을 기반으로 레드 와인과 화이트 와인을 더 잘 분리하는 것을 볼 수 있습니다.
이 산점도들은 데이터셋이 특정한 구조를 가지고 있음을 확인시켜 줍니다. 즉, 특성과 레이블 사이에 관계가 있습니다.
이를 확인했으니 이제 안심하고 지도 학습(supervised machine learning)을 적용해 보겠습니다.
먼저 회귀 작업을 시작하고 이어서 분류 작업을 진행하겠습니다.</p>
</div>
</div>
<div id="vowpal-wabbit을-사용한-회귀" class="section level2" number="9.4">
<h2>
<span class="header-section-number">9.4</span> Vowpal Wabbit을 사용한 회귀<a class="anchor" aria-label="anchor" href="#vowpal-wabbit%EC%9D%84-%EC%82%AC%EC%9A%A9%ED%95%9C-%ED%9A%8C%EA%B7%80"><i class="fas fa-link"></i></a>
</h2>
<p>이 섹션에서는 와인의 물리화학적 특성을 기반으로 화이트 와인의 품질을 예측하는 모델을 만들 것입니다.
품질은 0에서 10 사이의 숫자이므로, 이를 회귀(regression) 작업으로 간주할 수 있습니다.</p>
<p>이를 위해 Vowpal Wabbit, 줄여서 <code>vw</code>를 사용할 것입니다.</p>
<div id="데이터-준비하기" class="section level3" number="9.4.1">
<h3>
<span class="header-section-number">9.4.1</span> 데이터 준비하기<a class="anchor" aria-label="anchor" href="#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A4%80%EB%B9%84%ED%95%98%EA%B8%B0"><i class="fas fa-link"></i></a>
</h3>
<p>CSV 파일로 작업하는 대신, <code>vw</code>는 자체적인 데이터 형식을 사용합니다.
<code>csv2vw</code><span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Jeroen Janssens, &lt;span&gt;“&lt;span class=&quot;nocase&quot;&gt;csv2vw&lt;/span&gt; – Convert &lt;span&gt;CSV&lt;/span&gt; to Vowpal Wabbit Format,”&lt;/span&gt; 2021, &lt;a href=&quot;https://github.com/jeroenjanssens/dsutils&quot;&gt;https://github.com/jeroenjanssens/dsutils&lt;/a&gt;.&lt;/p&gt;"><sup>114</sup></a></span> 도구는 이름에서 알 수 있듯이 CSV를 이 형식으로 변환할 수 있습니다.
<code>--label</code> 옵션은 어떤 열이 레이블을 포함하고 있는지 지정하는 데 사용됩니다.
결과를 살펴봅시다.</p>
<pre>csv2vw wine-white-clean.csv --label quality | trim</pre>
<p>이 형식에서 각 줄은 하나의 데이터 포인트입니다.
줄은 레이블로 시작하고, 그 뒤에 파이프 기호(<code>|</code>)가 오며, 그 다음에는 공백으로 구분된 특성 이름/값 쌍들이 옵니다.
CSV 형식과 비교했을 때 이 형식이 지나치게 장황해 보일 수도 있지만, 가중치(weights), 태그(tags), 네임스페이스(namespaces), 그리고 희소 특성 표현(sparse feature representation)과 같은 더 많은 유연함을 제공합니다.
와인 데이터셋에서는 이러한 유연성이 필요하지 않지만, 더 복잡한 문제에 <code>vw</code>를 적용할 때는 유용할 수 있습니다.
이 <a href="https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Input-format">기사</a>에서 <code>vw</code> 형식에 대해 더 자세히 설명하고 있습니다.</p>
<p>회귀 모델을 만들거나 <em>훈련(train)</em>하고 나면, 이를 사용하여 아직 보지 못한 새로운 데이터 포인트에 대해 예측을 수행할 수 있습니다.
즉, 모델이 이전에 본 적 없는 와인을 제공하면 그 품질을 예측하거나 <em>테스트(test)</em>할 수 있습니다.
이러한 예측의 정확도를 적절하게 평가하려면, 훈련에 사용하지 않을 데이터를 일부 따로 떼어놓아야 합니다.
보통 전체 데이터셋의 80%를 훈련용으로 사용하고 나머지 20%를 테스트용으로 사용합니다.</p>
<p>먼저 <code>split</code><span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Torbjorn Granlund and Richard M. Stallman, &lt;span&gt;“&lt;span class=&quot;nocase&quot;&gt;split&lt;/span&gt; – Split a File into Pieces,”&lt;/span&gt; 2019, &lt;a href=&quot;https://www.gnu.org/software/coreutils&quot;&gt;https://www.gnu.org/software/coreutils&lt;/a&gt;.&lt;/p&gt;"><sup>115</sup></a></span>을 사용하여 전체 데이터셋을 동일한 다섯 부분으로 나누어 이 작업을 수행할 수 있습니다.
<code>wc</code>를 사용하여 각 부분의 데이터 포인트 수를 확인합니다.</p>
<pre>csv2vw wine-white-clean.csv --label quality |
shuf |
split -d -n r/5 - wine-part-
wc -l wine-part-*</pre>
<p><span class="callout">➊</span> <code>shuf</code><span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Paul Eggert, &lt;span&gt;“&lt;span class=&quot;nocase&quot;&gt;shuf&lt;/span&gt; – Generate Random Permutations,”&lt;/span&gt; 2019, &lt;a href=&quot;https://www.gnu.org/software/coreutils&quot;&gt;https://www.gnu.org/software/coreutils&lt;/a&gt;.&lt;/p&gt;"><sup>116</sup></a></span> 도구는 데이터셋을 무작위로 섞어서 훈련 세트와 테스트 세트가 유사한 품질 분포를 갖도록 보장합니다.</p>
<p>이제 첫 번째 부분(즉, 20%)을 테스트 세트인 <em>wine-test.vw</em>로 사용하고, 나머지 네 부분(즉, 80%)을 합쳐서 훈련 세트인 <em>wine-train.vw</em>를 만듭니다.</p>
<pre>mv wine-part-00 wine-test.vw
cat wine-part-* &gt; wine-train.vw
rm wine-part-*
wc -l wine-*.vw</pre>
<p>이제 <code>vw</code>를 사용하여 모델을 훈련할 준비가 되었습니다.</p>
</div>
<div id="모델-훈련하기" class="section level3" number="9.4.2">
<h3>
<span class="header-section-number">9.4.2</span> 모델 훈련하기<a class="anchor" aria-label="anchor" href="#%EB%AA%A8%EB%8D%B8-%ED%9B%88%EB%A0%A8%ED%95%98%EA%B8%B0"><i class="fas fa-link"></i></a>
</h3>
<p><code>vw</code> 도구는 매우 많은(거의 400개에 달하는!) 옵션을 제공합니다.
다행히 효과적으로 사용하기 위해 그 모든 옵션이 필요한 것은 아닙니다.
여기서 사용하는 옵션들을 설명하기 위해 각 옵션을 별도의 줄에 적어보겠습니다.</p>
<pre>vw \
--data wine-train.vw \
--final_regressor wine.model \
--passes 10 \
--cache_file wine.cache \
--nn 3 \
--quadratic :: \
--l2 0.000005 \
--bit_precision 25</pre>
<p><span class="callout">➊</span> <em>wine-train.vw</em> 파일이 모델 훈련에 사용됩니다.
<br><span class="callout">➋</span> 모델 또는 <em>회귀분석기(regressor)</em>는 <em>wine.model</em> 파일에 저장됩니다.
<br><span class="callout">➌</span> 훈련 패스(passes)의 횟수입니다.
<br><span class="callout">➍</span> 여러 번의 패스를 수행할 때 캐싱이 필요합니다.
<br><span class="callout">➎</span> 3개의 은닉 유닛(hidden units)을 가진 신경망을 사용합니다.
<br><span class="callout">➏</span> 모든 입력 특성을 기반으로 이차(quadratic) 특성을 생성하여 사용합니다. 중복되는 특성은 <code>vw</code>에 의해 제거됩니다.
<br><span class="callout">➐</span> L2 규제(regularization)를 사용합니다.
<br><span class="callout">➑</span> 특성을 저장하는 데 25비트를 사용합니다.</p>
<p>이제 회귀 모델을 훈련시켰으니, 이를 사용하여 예측을 해보겠습니다.</p>
</div>
<div id="모델-테스트하기" class="section level3" number="9.4.3">
<h3>
<span class="header-section-number">9.4.3</span> 모델 테스트하기<a class="anchor" aria-label="anchor" href="#%EB%AA%A8%EB%8D%B8-%ED%85%8C%EC%8A%A4%ED%8A%B8%ED%95%98%EA%B8%B0"><i class="fas fa-link"></i></a>
</h3>
<p>모델은 <em>wine.model</em> 파일에 저장되어 있습니다.
이 모델을 사용하여 예측을 수행하기 위해, 이번에는 다른 옵션들을 사용하여 <code>vw</code>를 다시 실행합니다.</p>
<pre>vw \
--data wine-test.vw \
--initial_regressor wine.model \
--testonly \
--predictions predictions \
--quiet
bat predictions | trim</pre>
<p><span class="callout">➊</span> <em>wine-test.vw</em> 파일이 모델 테스트에 사용됩니다.
<br><span class="callout">➋</span> <em>wine.model</em> 파일에 저장된 모델을 사용합니다.
<br><span class="callout">➌</span> 레이블 정보를 무시하고 테스트만 수행합니다.
<br><span class="callout">➍</span> 예측 결과는 <em>predictions</em>라는 파일에 저장됩니다.
<br><span class="callout">➎</span> 진단 메시지와 진행 상황 업데이트를 출력하지 않습니다.</p>
<p><code>paste</code>를 사용하여 <em>predictions</em> 파일에 담긴 예측값과 <em>wine-test.vw</em> 파일에 담긴 실제값(관측값, observed values)을 결합해 봅시다.
<code>awk</code>를 사용하여 예측값과 관측값을 비교하고 평균 절대 오차(MAE, Mean Absolute Error)를 계산할 수 있습니다.
MAE는 화이트 와인의 품질을 예측할 때 <code>vw</code>가 평균적으로 얼마나 차이가 나는지 알려줍니다.</p>
<pre>paste -d, predictions &lt;(cut -d '|' -f 1 wine-test.vw) |
tee results.csv |
awk -F, '{E+=sqrt(($1-$2)^2)} END {print "MAE: " E/NR}' |
cowsay</pre>
<p>예측값은 평균적으로 약 0.6점 정도 차이가 납니다.
<code>rush plot</code>을 사용하여 관측값과 예측값 사이의 관계를 시각화해 봅시다.</p>
<pre>&lt; results.csv header -a "predicted,observed" |
rush plot --x observed --y predicted --geom jitter &gt; wine-regression.png
display wine-regression.png</pre>
<p>모델 훈련에 사용된 옵션들이 조금 복잡해 보일 수 있습니다.
모든 기본값(default values)을 사용했을 때 <code>vw</code>의 성능이 어떠한지 확인해 봅시다.</p>
<pre>vw -d wine-train.vw -f wine2.model --quiet
vw -data wine-test.vw -i wine2.model -t -p predictions --quiet
paste -d, predictions &lt;(cut -d '|' -f 1 wine-test.vw) |
awk -F, '{E+=sqrt(($1-$2)^2)} END {print "MAE: " E/NR}'</pre>
<p><span class="callout">➊</span> 회귀 모델 훈련
<br><span class="callout">➋</span> 회귀 모델 테스트
<br><span class="callout">➌</span> 평균 절대 오차(MAE) 계산</p>
<p>기본값을 사용했을 때 MAE가 0.04 더 높게 나타났습니다. 이는 예측 결과가 약간 더 나빠졌음을 의미합니다.</p>
<p>이 섹션에서는 <code>vw</code>가 할 수 있는 일의 극히 일부만을 다루었습니다.
이 도구에 그렇게 많은 옵션이 있는 데는 이유가 있습니다.
회귀 외에도 이진 분류(binary classification), 다중 클래스 분류(multi-class classification), 강화 학습(reinforcement learning), 그리고 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA) 등을 지원합니다.
<a href="https://vowpalwabbit.org/">웹사이트</a>에서 더 많은 튜토리얼과 기사를 찾아볼 수 있습니다.</p>
</div>
</div>
<div id="scikit-learn-laboratory를-사용한-분류" class="section level2" number="9.5">
<h2>
<span class="header-section-number">9.5</span> SciKit-Learn Laboratory를 사용한 분류<a class="anchor" aria-label="anchor" href="#scikit-learn-laboratory%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-%EB%B6%84%EB%A5%98"><i class="fas fa-link"></i></a>
</h2>
<!-- TODO: Explain SKLL better -->
<p>이 섹션에서는 와인이 레드인지 화이트인지를 예측하는 분류 모델(분류기, classifier)을 훈련시킬 것입니다.
이 작업에도 <code>vw</code>를 사용할 수 있지만, 다른 도구인 SciKit-Learn Laboratory(SKLL)를 소개해 드리고 싶습니다.
이름에서 알 수 있듯이, 이 도구는 파이썬에서 널리 쓰이는 머신러닝 패키지인 SciKit-Learn을 기반으로 구축되었습니다.
SKLL 자체도 파이썬 패키지이며, 커맨드 라인에서 SciKit-Learn을 사용할 수 있게 해주는 <code>run_experiment</code> 도구를 제공합니다.
저는 패키지 이름과 일치하여 기억하기 쉬운 <code>skll</code>이라는 별칭(alias)을 사용하여 <code>run_experiment</code>를 호출하겠습니다.</p>
<pre>alias skll=run_experiment
skll</pre>
<div id="데이터-준비하기-1" class="section level3" number="9.5.1">
<h3>
<span class="header-section-number">9.5.1</span> 데이터 준비하기<a class="anchor" aria-label="anchor" href="#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A4%80%EB%B9%84%ED%95%98%EA%B8%B0-1"><i class="fas fa-link"></i></a>
</h3>
<p><code>skll</code>은 훈련용 데이터셋과 테스트용 데이터셋이 서로 다른 디렉터리에 있되 같은 파일 이름을 가질 것을 기대합니다.
예측치(predictions)가 반드시 원래 데이터셋과 같은 순서로 나오지 않으므로, 예측치를 올바른 데이터 포인트와 일치시킬 수 있도록 고유 식별자를 포함하는 <em><code>id</code></em> 열을 추가하겠습니다.
균형 잡힌(balanced) 데이터셋을 만들어 봅시다.</p>
<pre>NUM_RED="$(&lt; wine-red-clean.csv wc -l)"
csvstack -n type -g red,white \
wine-red-clean.csv \
&lt;(&lt; wine-white-clean.csv body shuf | head -n $NUM_RED) |
body shuf |
nl -s, -w1 -v0 |
sed '1s/0,/id,/' |
tee wine-balanced.csv | csvlook</pre>
<p><span class="callout">➊</span> 레드 와인의 수를 <em><code>NUM_RED</code></em> 변수에 저장합니다.
<br><span class="callout">➋</span> 모든 레드 와인과 무작위로 샘플링된 동일한 수의 화이트 와인을 결합합니다.
<br><span class="callout">➌</span> <code>nl</code>을 사용하여 각 줄 앞에 “줄 번호”를 추가합니다.
<br><span class="callout">➍</span> 적절한 열 이름이 되도록 첫 번째 줄의 “0”을 “id”로 바꿉니다.</p>
<p>이제 이 균형 잡힌 데이터셋을 훈련 세트와 테스트 세트로 나눕니다.</p>
<pre>mkdir -p {train,test}
HEADER="$(&lt; wine-balanced.csv header)"
&lt; wine-balanced.csv header -d | shuf | split -d -n r/5 - wine-part-
wc -l wine-part-*
cat wine-part-00 | header -a $HEADER &gt; test/features.csv &amp;&amp; rm wine-part-00
cat wine-part-* | header -a $HEADER &gt; train/features.csv &amp;&amp; rm wine-part-*
wc -l t*/features.csv</pre>
<p>이제 균형 잡힌 훈련 세트와 테스트 세트가 준비되었으니, 분류기를 만드는 단계를 진행할 수 있습니다.</p>
</div>
<div id="실험-실행하기" class="section level3" number="9.5.2">
<h3>
<span class="header-section-number">9.5.2</span> 실험 실행하기<a class="anchor" aria-label="anchor" href="#%EC%8B%A4%ED%97%98-%EC%8B%A4%ED%96%89%ED%95%98%EA%B8%B0"><i class="fas fa-link"></i></a>
</h3>
<p><code>skll</code>에서 분류기를 훈련시키려면 설정 파일(configuration file)에 실험을 정의해야 합니다.
이 정의에는 데이터셋을 어디서 찾을지, 어떤 분류기를 사용할지 등을 명시하는 여러 섹션이 포함됩니다.
여기서 사용할 설정 파일 <em>classify.cfg</em>는 다음과 같습니다.</p>
<pre>bat classify.cfg</pre>
<p><code>skll</code>을 사용하여 실험을 실행합니다.</p>
<pre>skll -l classify.cfg 2&gt;/dev/null</pre>
<p><code>-l</code> 옵션은 로컬 모드에서 실행할 것을 지정합니다.
<code>skll</code>은 클러스터에서 실험을 실행하는 기능도 제공합니다.
실험이 완료되는 데 걸리는 시간은 선택한 알고리즘의 복잡도와 데이터의 크기에 따라 달라집니다.</p>
</div>
<div id="결과-분석하기" class="section level3" number="9.5.3">
<h3>
<span class="header-section-number">9.5.3</span> 결과 분석하기<a class="anchor" aria-label="anchor" href="#%EA%B2%B0%EA%B3%BC-%EB%B6%84%EC%84%9D%ED%95%98%EA%B8%B0"><i class="fas fa-link"></i></a>
</h3>
<p>모든 분류기의 훈련과 테스트가 완료되면, 결과는 <em>output</em> 디렉터리에서 확인할 수 있습니다.</p>
<pre>ls -1 output</pre>
<p><code>skll</code>은 각 분류기에 대해 네 개의 파일을 생성합니다: 로그 파일 한 개, 결과 파일 두 개, 그리고 예측값 파일 한 개입니다.
다음 SQL 쿼리를 사용하여 알고리즘 이름과 등급별 정확도를 추출해 보겠습니다.</p>
<pre>&lt; output/wine_summary.tsv csvsql --query "SELECT learner_name, accuracy FROM stdin ORDER BY accuracy DESC" | csvlook -I</pre>
<p>여기서 가장 중요한 열은 정확도를 나타내는 <em><code>accuracy</code></em>로, 이는 올바르게 분류된 데이터 포인트의 비율을 의미합니다.
이를 통해 실제로 모든 알고리즘이 매우 좋은 성능을 보이고 있음을 알 수 있습니다.
RandomForestClassifier가 가장 좋은 성능을 보였고, 그 뒤를 KNeighborsClassifier가 근소한 차이로 따르고 있습니다.</p>
<p>각 JSON 파일에는 혼동 행렬(confusion matrix)이 포함되어 있어 각 분류기의 성능에 대한 추가적인 통찰을 제공합니다.
혼동 행렬은 열이 실제 레이블(레드와 화이트)을 나타내고 행이 예측된 레이블을 나타내는 표입니다.
대각선 상의 숫자가 높을수록 정확한 예측이 많음을 의미합니다.
<code>jq</code>를 사용하여 각 분류기의 이름과 해당 혼동 행렬을 인쇄할 수 있습니다.</p>
<pre>jq -r '.[] | "\(.learner_name):\n\(.result_table)\n"' output/*.json</pre>
<p>혼동 행렬은 특히 분류해야 할 클래스가 세 개 이상일 때 어떤 종류의 오분류가 발생하는지 확인하거나, 각 클래스별로 오분류 비용이 다를 때 특히 도움이 됩니다.</p>
<p>사용 관점에서 볼 때, <code>vw</code>와 <code>skll</code>이 서로 다른 두 가지 접근 방식을 취한다는 점이 흥미롭습니다.
<code>vw</code>는 커맨드 라인 옵션을 사용하는 반면, <code>skll</code>은 별도의 설정 파일이 필요합니다.
두 방식 모두 장단점이 있습니다.
커맨드 라인 옵션은 더 즉흥적인 사용을 가능하게 하지만, 설정 파일은 아마도 재현(reproduce)하기가 더 쉽습니다.
또한 이미 보았듯이, 수많은 옵션과 함께 <code>vw</code>를 호출하는 방식은 스크립트나 <em>Makefile</em>에 쉽게 넣을 수 있습니다.
반대로, <code>skll</code>이 설정 파일 없이 옵션을 받도록 만드는 것은 비교적 덜 명확합니다.</p>
</div>
</div>
<div id="요약-8" class="section level2" number="9.6">
<h2>
<span class="header-section-number">9.6</span> 요약<a class="anchor" aria-label="anchor" href="#%EC%9A%94%EC%95%BD-8"><i class="fas fa-link"></i></a>
</h2>
<p>이 장에서는 데이터 모델링에 대해 살펴보았습니다.
예제를 통해 비지도 학습인 차원 축소, 그리고 지도 학습인 회귀와 분류라는 세 가지 서로 다른 머신러닝 작업에 대해 깊이 파고들어 보았습니다.
아쉽게도 본격적인 머신러닝 튜토리얼은 이 책의 범위를 벗어납니다.
머신러닝에 대해 더 배우고 싶은 분들을 위해 다음 섹션에서 몇 권의 책을 추천해 드립니다.
이것으로 이 책에서 다루는 데이터 과학 OSEMN 모델의 네 번째이자 마지막 단계를 마쳤습니다.
다음 장은 마지막 중간 막(intermezzo) 장으로, 커맨드 라인을 다른 곳에서 활용하는 방법을 다룰 것입니다.</p>
</div>
<div id="더-읽어보기-4" class="section level2" number="9.7">
<h2>
<span class="header-section-number">9.7</span> 더 읽어보기<a class="anchor" aria-label="anchor" href="#%EB%8D%94-%EC%9D%BD%EC%96%B4%EB%B3%B4%EA%B8%B0-4"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>Sebastian Raschka와 Vahid Mirjalili의 저서 <em>Python Machine Learning</em>은 머신러닝에 대한 포괄적인 개요와 파이썬을 사용한 적용 방법을 제공합니다.</li>
<li>Jared Lander의 <em>R for Everyone</em>의 후반부 장들에서 R을 사용하여 다양한 머신러닝 작업을 수행하는 방법을 설명합니다.</li>
<li>머신러닝에 대해 더 깊이 이해하고 싶다면, Christopher Bishop의 <em>Pattern Recognition and Machine Learning</em>과 David MacKay의 <em>Information Theory, Inference, and Learning Algorithms</em>를 강력히 추천합니다.</li>
<li>t-SNE 알고리즘에 대해 더 자세히 알고 싶다면, Laurens van der Maaten과 Geoffrey Hinton의 원저 논문인 <em>Visualizing Data Using T-SNE</em>를 추천합니다.</li>
</ul>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="chapter-8-parallel-pipelines.html"><span class="header-section-number">8</span> 병렬 파이프라인</a></div>
<div class="next"><a href="chapter-10-polyglot-data-science.html"><span class="header-section-number">10</span> 다국어 데이터 과학</a></div>
</div></main><div class="col-md-3 col-lg-3 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>이 페이지의 내용</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#chapter-9-modeling-data"><span class="header-section-number">9</span> 데이터 모델링</a></li>
<li><a class="nav-link" href="#%EA%B0%9C%EC%9A%94-6"><span class="header-section-number">9.1</span> 개요</a></li>
<li><a class="nav-link" href="#%EC%99%80%EC%9D%B8-%EC%A2%80-%EB%8D%94-%EC%A3%BC%EC%84%B8%EC%9A%94"><span class="header-section-number">9.2</span> 와인 좀 더 주세요!</a></li>
<li>
<a class="nav-link" href="#tapkee%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-%EC%B0%A8%EC%9B%90-%EC%B6%95%EC%86%8C"><span class="header-section-number">9.3</span> Tapkee를 사용한 차원 축소</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#tapkee-%EC%86%8C%EA%B0%9C"><span class="header-section-number">9.3.1</span> Tapkee 소개</a></li>
<li><a class="nav-link" href="#%EC%84%A0%ED%98%95-%EB%B0%8F-%EB%B9%84%EC%84%A0%ED%98%95-%EB%A7%A4%ED%95%91"><span class="header-section-number">9.3.2</span> 선형 및 비선형 매핑</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#vowpal-wabbit%EC%9D%84-%EC%82%AC%EC%9A%A9%ED%95%9C-%ED%9A%8C%EA%B7%80"><span class="header-section-number">9.4</span> Vowpal Wabbit을 사용한 회귀</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A4%80%EB%B9%84%ED%95%98%EA%B8%B0"><span class="header-section-number">9.4.1</span> 데이터 준비하기</a></li>
<li><a class="nav-link" href="#%EB%AA%A8%EB%8D%B8-%ED%9B%88%EB%A0%A8%ED%95%98%EA%B8%B0"><span class="header-section-number">9.4.2</span> 모델 훈련하기</a></li>
<li><a class="nav-link" href="#%EB%AA%A8%EB%8D%B8-%ED%85%8C%EC%8A%A4%ED%8A%B8%ED%95%98%EA%B8%B0"><span class="header-section-number">9.4.3</span> 모델 테스트하기</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#scikit-learn-laboratory%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-%EB%B6%84%EB%A5%98"><span class="header-section-number">9.5</span> SciKit-Learn Laboratory를 사용한 분류</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A4%80%EB%B9%84%ED%95%98%EA%B8%B0-1"><span class="header-section-number">9.5.1</span> 데이터 준비하기</a></li>
<li><a class="nav-link" href="#%EC%8B%A4%ED%97%98-%EC%8B%A4%ED%96%89%ED%95%98%EA%B8%B0"><span class="header-section-number">9.5.2</span> 실험 실행하기</a></li>
<li><a class="nav-link" href="#%EA%B2%B0%EA%B3%BC-%EB%B6%84%EC%84%9D%ED%95%98%EA%B8%B0"><span class="header-section-number">9.5.3</span> 결과 분석하기</a></li>
</ul>
</li>
<li><a class="nav-link" href="#%EC%9A%94%EC%95%BD-8"><span class="header-section-number">9.6</span> 요약</a></li>
<li><a class="nav-link" href="#%EB%8D%94-%EC%9D%BD%EC%96%B4%EB%B3%B4%EA%B8%B0-4"><span class="header-section-number">9.7</span> 더 읽어보기</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/partrita/data-science-at-the-command-line/blob/master/book/2e/09.Rmd">소스 보기 <i class=""></i></a></li>
          <li><a id="book-edit" href="https://github.com/partrita/data-science-at-the-command-line/edit/master/book/2e/09.Rmd">이 페이지 편집 <i class=""></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container-fluid">
    <div class="row">
      <div class="d-none d-lg-block col-lg-2 sidebar"></div>
      <div class="col-sm-12 col-md-9 col-lg-7 mt-3" style="max-width: 45rem;">
        <p><strong>커맨드 라인에서 시작하는 데이터 과학, 2판</strong> 저자 <a href="https://twitter.com/jeroenhjanssens" class="text-light">Jeroen Janssens</a>. 최종 업데이트: January 29, 2026. 이 책은 <a class="text-light" href="https://bookdown.org">bookdown</a> R 패키지로 제작되었습니다.</p>
      </div>
      <div class="col-md-3 col-lg-3 d-none d-md-block sidebar"></div>
    </div>
  </div>
</footer>
</body>
</html>
